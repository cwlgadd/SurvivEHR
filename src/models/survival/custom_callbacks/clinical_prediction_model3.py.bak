# Create custom callbacks for our pytorch-lightning model

import numpy as np
from pytorch_lightning import Callback
import torch
from sklearn.manifold import TSNE
import umap
import wandb
import matplotlib.pyplot as plt
from CPRD.src.models.base_callback import BaseCallback
# from CPRD.data.foundational_loader import convert_batch_to_none_causal
from pycox.evaluation import EvalSurv
import pandas as pd
import seaborn as sns
import logging
import copy


class PerformanceMetrics(Callback):
    """
    Record metrics for survival model.
    """

    def __init__(self, 
                 outcome_tokens,
                 outcome_token_to_desurv_output_index,
                 log_individual=True,
                 log_combined=False,
                 log_ctd=True, 
                 log_ibs=True, 
                 log_inbll=True,
                ):
        """

        KWARGS:
            log_individual:   Whether or not to log for the individual CIFs/CDFs. 
                                For few-shot single-risk, this is the only appropriate choice. For few-shot competing-risk this may highlight which 
                                outcomes the model is better able to predict.
                                For fine-tune single risk, there is only one outcome CIF/CDF and so this is not appropriate.
                              Note:  
                                If there are low prevalence outcome tokens then there are likeliy to be batches with no individuals  
                                experiencing that outcome, leading to warnings. In this case, the batch's metric for that individual 
                                CIF will not be logged which may bias the aggregated reported values.
            log_marginal:     Whether or not to log the marginal CIFs/CDFs. 
                                This is only an appropriate choice for the Competing Risk case. In the SR setting, this will raise an
                                exception.
                                
        """
        
        Callback.__init__(self)
        self.outcome_tokens = outcome_tokens
        self.outcome_token_to_desurv_output_index = outcome_token_to_desurv_output_index
        self.log_individual = log_individual
        self.log_marginal = log_combined
        self.log_ctd = log_ctd
        self.log_ibs = log_ibs
        self.log_inbll = log_inbll

    def plot_outcome_curve(self, cdf, lbls, _trainer, log_name="outcome_split_curve", ylabel=None):
        
        plt.close()
        cdf_unce = cdf[lbls==1, :]
        cdf_cens = cdf[lbls==0, :]
        
        wandb_images = []
        fig, ax = plt.subplots(1, 1)
        for i in range(cdf_unce.shape[0]):
            plt.plot(np.linspace(0,1,1000), cdf_unce[i,:], c="r", label="event" if i == 0 else None, alpha=1)
        for i in range(cdf_cens.shape[0]):
            plt.plot(np.linspace(0,1,1000), cdf_cens[i,:], c="k", label="censored" if i == 0 else None, alpha=0.1)
        
        plt.legend(loc=2)
        plt.xlabel("t (scaled time)")
        if ylabel is not None:
            plt.ylabel(ylabel)

        _trainer.logger.experiment.log({
                log_name + "-Survival": wandb.Image(fig)
            })

    def get_metrics(self, cdf, lbls, target_ages, _trainer, _pl_module, log_name):
        
        if np.sum(lbls) == 0:
            logging.warning(f"Only censored events in batch. Evaluating metrics will be unstable.")

        metric_dict = {}
        try:
            # Evaluate concordance. Scale using the head layers internal scaling.
            surv = pd.DataFrame(np.transpose((1 - cdf)), index=_pl_module.model.surv_layer.t_eval)
            ev = EvalSurv(surv, target_ages, lbls, censor_surv='km')
    
            # Calculate and log desired metrics
            time_grid = np.linspace(start=0, stop=_pl_module.model.surv_layer.t_eval.max() , num=300)
            if self.log_ctd:
                ctd = ev.concordance_td()                           # Time-dependent Concordance Index
                metric_dict = {**metric_dict, log_name+"ctd": ctd}
            if self.log_ibs:
                ibs = ev.integrated_brier_score(time_grid)          # Integrated Brier Score
                metric_dict = {**metric_dict, log_name+"ibs": ibs}
            if self.log_inbll:
                inbll = ev.integrated_nbll(time_grid)               # Integrated Negative Binomial LogLikelihood
                metric_dict = {**metric_dict, log_name+"inbll": inbll}

            self.log_dict(metric_dict)
        except:
            logging.warning("Unable to calculate metrics, this batch will be skipped - this will bias metrics.")
        
    def run_callback(self,
                     _trainer,
                     _pl_module,
                     batch,
                     log_name:               str='Metrics',
                     plot_outcome_curves = False,
                    ):

        # Make prediction of each survival curve
        all_outputs, _, _ = _pl_module(batch, return_loss=False, return_generation=True)
        pred_surv_CDFs = all_outputs["surv"]["surv_CDF"]      
        pred_surv_pi = all_outputs["surv"]["surv_pi"]
        
        target_tokens = batch['target_token'].cpu().numpy()
        target_ages = batch['target_age_delta'].cpu().numpy()
        target_values = batch['target_value'].cpu().numpy()

        # Merge (additively) each outcome risk curve into a single CDF, and update label for if outcome occurred or not
        #    In the fine-tuning SR case, the outcomes are combined into a single event. In this case `log_individual` is  
        #    repeatedly using the same shared event's CDF, but plotting/evaluating against just one of the outcome tokens 
        #    which combine to create that single event.
        cdf = np.zeros_like(pred_surv_CDFs[0])
        lbls = np.zeros_like(target_tokens)
        surv_indices_included = []               
        for _outcome_token in self.outcome_tokens:
            # print(f"{_outcome_token} of {self.outcome_tokens} included from {len(pred_surv_CDFs)} surv CDFs")
            
            _outcome_labels = (target_tokens == _outcome_token)             # 1 if == _outcome_token else 0
            lbls += _outcome_labels

            _outcome_surv_index = self.outcome_token_to_desurv_output_index[_outcome_token]
            _outcome_cdf = pred_surv_CDFs[_outcome_surv_index]

            # When different outcomes map to the same CIF/CDF curve, then we do not duplicate in `log_marginal`
            if _outcome_surv_index not in surv_indices_included:
                cdf += _outcome_cdf
                surv_indices_included.append(_outcome_surv_index)
            
            # Make log records for individual outcomes
            #######
            if self.log_individual:
                # Plot the outcome curve
                if plot_outcome_curves:
                    ylabel = r"$F_{" + str(_outcome_token) + r"}(t)$"
                    self.plot_outcome_curve(_outcome_cdf, 
                                            _outcome_labels, 
                                            _trainer, 
                                            log_name=log_name+f"_{_outcome_token}", ylabel=ylabel)
                # Log metrics
                self.get_metrics(_outcome_cdf, 
                                 _outcome_labels,
                                 target_ages, 
                                 _trainer, 
                                 _pl_module, 
                                 log_name+f"_{_outcome_token}")

        # Plot the outcome curves
        if self.log_marginal:
            if plot_outcome_curves:
                ylabel = r"$\sum_{k\in{" + f"{','.join([str(i) for i in self.outcome_tokens])}" + r"}} F_k(t)$"
                self.plot_outcome_curve(cdf, lbls, _trainer, log_name=log_name, ylabel=ylabel)
            # Log metrics
            #  Note: in the single risk case these metrics wont make sense. This is because the cdf is provided as the sum of the indendent CDFs, and 
            #        there is no way to rescale them. We could take the average, but this will still be wrong.
            self.get_metrics(cdf, lbls, target_ages, _trainer, _pl_module, log_name=log_name)

        # # Merge (additively) each outcome risk curve into a single CDF, and update label for if outcome occurred or not
        # #
        # cdf = np.zeros_like(pred_surv_CDFs[0])
        # lbls = np.zeros_like(target_tokens)
        # for _outcome_token in self.outcome_tokens:
        #     print(f"{_outcome_token} of {self.outcome_tokens} included from {len(pred_surv_CDFs)} surv CDFs")
            
        #     _outcome_labels = (target_tokens == _outcome_token)
        #     lbls += _outcome_labels

        #     _outcome_surv_index = self.outcome_token_to_desurv_output_index[_outcome_token]
        #     _outcome_cdf = pred_surv_CDFs[_outcome_surv_index] 
        #     cdf += _outcome_cdf
            
        #     # Make log records for individual outcomes
        #     #######
        #     if self.log_individual:
        #         # Plot the outcome curve
        #         if plot_outcome_curves:
        #             ylabel = r"$F_{" + str(_outcome_token) + r"}(t)$"
        #             self.plot_outcome_curve(_outcome_cdf, _outcome_labels, _trainer, log_name=log_name+f"_{_outcome_token}", ylabel=ylabel)
        #         # Log metrics
        #         self.get_metrics(_outcome_cdf, _outcome_labels, target_ages, _trainer, _pl_module, log_name+f"_{_outcome_token}")

        # # Plot the outcome curves
        # if self.log_marginal:
        #     assert pred_surv_pi is not None, "Marginalising out the event type can only be done in the Competing-Risk case."
        #     if plot_outcome_curves:
        #         ylabel = r"$\sum_{k\in{" + f"{','.join([str(i) for i in self.outcome_tokens])}" + r"}} F_k(t)$"
        #         self.plot_outcome_curve(cdf, lbls, _trainer, log_name=log_name, ylabel=ylabel)
        #     # Log metrics
        #     #  Note: in the single risk case these metrics wont make sense. This is because the cdf is provided as the sum of the indendent CDFs, and 
        #     #        there is no way to rescale them. We could take the average, but this will still be wrong.
        #     self.get_metrics(cdf, lbls, target_ages, _trainer, _pl_module, log_name=log_name)
            
    def on_validation_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):
        # Run callback
        self.run_callback(_trainer=trainer, 
                          _pl_module = pl_module,
                          batch=batch,
                          log_name = "Val:OutcomePerformanceMetrics", 
                          plot_outcome_curves = True
                          )

    def on_test_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):
        # Run callback
        self.run_callback(_trainer=trainer, 
                          _pl_module = pl_module,
                          batch=batch,
                          log_name = "Test:OutcomePerformanceMetrics", 
                          plot_outcome_curves = True
                          )
