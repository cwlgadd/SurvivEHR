{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJpXpmjEYC_T"
   },
   "source": [
    "# Data workflow - DEPRECATED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning \n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sqlite3\n",
    "from dataclasses import dataclass\n",
    "import logging\n",
    "\n",
    "import CPRD\n",
    "from CPRD.data.foundational_loader import FoundationalDataModule\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "# device = \"cpu\"    # just for debug errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set GPT config to be equivalent\n",
    "@dataclass\n",
    "class DemoConfig:\n",
    "    tokenizer: str = \"tabular\"\n",
    "    unk_freq_threshold: float = 0.0\n",
    "    block_size: int = 64             # what is the maximum context length for predictions?\n",
    "    # n_layer: int = 6\n",
    "    # n_head: int = 6\n",
    "    # n_embd: int = 384\n",
    "    # pos_encoding: str = \"index-embedding\"                 # Manually adding later\n",
    "    # bias: bool = True\n",
    "    # attention_type: str = \"global\"    \n",
    "    # dropout: float = 0.0\n",
    "\n",
    "config = DemoConfig()\n",
    "\n",
    "@dataclass\n",
    "class OptConfig:\n",
    "    batch_size: int = 64\n",
    "    # eval_interval: int = 1\n",
    "    # learning_rate: float = 3e-4\n",
    "    # epochs: int = 10\n",
    "    \n",
    "opt = OptConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is first extracted from CPRD using [DExtER](https://link.springer.com/article/10.1007/s10654-020-00677-6) and is available within the optimal project master dataset. Some outlier filtering has already been done by DExtER in this extraction, and the ICD diagnostic codes in CPRD have been summarised and processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data can be found here:\n",
      "total 70K\n",
      "drwx--S--- 7 nobody nobody  16K Nov 29 10:15 \u001b[0m\u001b[01;34m.\u001b[0m\n",
      "drwx--S--- 8 nobody nobody 4.0K Nov 29 10:16 \u001b[01;34m..\u001b[0m\n",
      "drwx--S--- 2 nobody nobody 4.0K Nov 14 10:44 \u001b[01;34mbaseline\u001b[0m\n",
      "-rwx------ 1 nobody nobody 4.0K Oct 23 08:26 \u001b[01;32m._.DS_Store\u001b[0m\n",
      "-rwx------ 1 nobody nobody 6.1K Nov  4 13:01 \u001b[01;32m.DS_Store\u001b[0m\n",
      "drwx--S--- 3 nobody nobody 4.0K Nov 29 10:15 \u001b[01;34mHES\u001b[0m\n",
      "drwx--S--- 2 nobody nobody 4.0K Feb  8 14:22 \u001b[01;34mmetadata\u001b[0m\n",
      "drwx--S--- 4 nobody nobody 4.0K Jan 10 11:04 \u001b[01;34mtimeseries\u001b[0m\n",
      "drwx--S--- 3 nobody nobody 4.0K Feb 21 13:30 \u001b[01;34mzip\u001b[0m\n",
      "\n",
      "... with baseline covariates stored here\n",
      "total 30G\n",
      "drwx--S--- 2 nobody nobody 4.0K Nov 14 10:44 \u001b[0m\u001b[01;34m.\u001b[0m\n",
      "drwx--S--- 7 nobody nobody  16K Nov 29 10:15 \u001b[01;34m..\u001b[0m\n",
      "-rwx------ 1 nobody nobody  15G Nov 10 15:43 \u001b[01;32mmasterDataOptimal_v3.csv\u001b[0m\n",
      "-rwx------ 1 nobody nobody  15G Nov 14 10:27 \u001b[01;32mmasterDataOptimalWithIMD_v3.csv\u001b[0m\n",
      "\n",
      "... and time series events stored here\n",
      "total 98K\n",
      "drwx--S--- 4 nobody nobody 4.0K Jan 10 11:04 \u001b[0m\u001b[01;34m.\u001b[0m\n",
      "drwx--S--- 7 nobody nobody  16K Nov 29 10:15 \u001b[01;34m..\u001b[0m\n",
      "-rwx------ 1 nobody nobody 4.0K Nov  4 12:57 \u001b[01;32m._.DS_Store\u001b[0m\n",
      "-rwx------ 1 nobody nobody 6.1K Nov  4 19:11 \u001b[01;32m.DS_Store\u001b[0m\n",
      "drwx--S--- 2 nobody nobody  16K Nov 13 10:27 \u001b[01;34mmeasurement_and_tests\u001b[0m\n",
      "drwx--S--- 2 nobody nobody 4.0K Nov 13 10:27 \u001b[01;34mmedication\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(f\"The data can be found here:\")\n",
    "!ls --color -lah /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data\n",
    "\n",
    "print(f\"\\n... with baseline covariates stored here\")\n",
    "!ls --color -lah /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/baseline\n",
    "\n",
    "print(f\"\\n... and time series events stored here\")\n",
    "!ls --color -lah /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/timeseries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compressing DExtER output \n",
    "\n",
    "These are pre-processed further in R to obtain three .csv files which are in turn compiled into an SQLite database within the `CPRD.data` module. \n",
    "\n",
    "* TODO: Replace R filtering with in-built python script\n",
    "* TODO: It is future work to combine these steps into DExtER, this will massively reduce the memory requirements.\n",
    "    * DExtER's output has the benefit of keeping records separate. E.g. if we only care about a subset of measurements then we only need to look at one file. However, we cam equally be achieved by a DB query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scripts for compressing these outputs can be found here\n",
      "total 67K\n",
      "drwx--S---  3 gaddcz nobody 4.0K Sep 28 11:12 \u001b[0m\u001b[01;34m.\u001b[0m\n",
      "drwx--S---. 5 gaddcz nobody 4.0K Oct 24 11:21 \u001b[01;34m..\u001b[0m\n",
      "drwx--S---  2 gaddcz nobody  16K Oct 18 15:48 \u001b[01;34mprocessed\u001b[0m\n",
      "-rwx------  1 gaddcz nobody 7.5K Sep 28 15:22 \u001b[01;32mprocessing_diagnoses.R\u001b[0m\n",
      "-rwx------. 1 gaddcz nobody  14K Oct 18 16:10 \u001b[01;32mprocessing_measurements_and_tests.R\u001b[0m\n",
      "-rwx------  1 gaddcz nobody 2.2K Sep 19 11:00 \u001b[01;32mprocessing_static.R\u001b[0m\n",
      "\n",
      "... and the saved files can be found here\n",
      "total 2.0G\n",
      "drwx--S--- 2 gaddcz nobody  16K Oct 18 15:48 \u001b[0m\u001b[01;34m.\u001b[0m\n",
      "drwx--S--- 3 gaddcz nobody 4.0K Sep 28 11:12 \u001b[01;34m..\u001b[0m\n",
      "-rwx------ 1 gaddcz nobody 983M Oct 18 15:48 \u001b[01;32mcprd.db\u001b[0m\n",
      "-rwx------ 1 gaddcz nobody 128M Oct  2 15:06 \u001b[01;32mdiagnosis_history.csv\u001b[0m\n",
      "-rwx------ 1 gaddcz nobody 874M Sep 28 15:38 \u001b[01;32mmeasurements.csv\u001b[0m\n",
      "-rwx------ 1 gaddcz nobody  42M Sep 20 12:06 \u001b[01;32mstatic.csv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(f\"The scripts for compressing these outputs can be found here\")\n",
    "!ls --color -lah /rds/projects/s/subramaa-mum-predict/CharlesGadd_Oxford/FoundationModel/preprocessing\n",
    "\n",
    "print(f\"\\n... and the saved files can be found here\")\n",
    "!ls --color -lah /rds/projects/s/subramaa-mum-predict/CharlesGadd_Oxford/FoundationModel/preprocessing/processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL tables\n",
    "\n",
    "We can convert each of the processed files into separate SQL tables. \n",
    "\n",
    "* TODO: Data should never have really left an SQL form here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static table\n",
    "<div align=\"justify\">\n",
    "The static table has one row per data owner. \n",
    "\n",
    "* PRACTICE_PATIENT_ID: The data owner identifier,\n",
    "* [ETHNICITY, SEX, YEAR_OF_BIRTH]: The static variables which remain constant throughout a lifetime.\n",
    "* INDEX_AGE:\n",
    "* START_AGE:\n",
    "*  END_AGE: \n",
    "</div>\n",
    "\n",
    "<div align=\"center\">\n",
    "  TODO: \n",
    "</div>\n",
    "\n",
    "* TODO: Repace missing with nans or some other unique value so these can be masked later\n",
    "* TODO: Add filtering to only include events after index age\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`Static table` docstring:\n",
      "\n",
      "    \n",
      "    Produced anonymized table:\n",
      "    ┌──────────────────────┬─────┬───────────┬───────────────┬─────────────┬─────────────┬───────────────┐\n",
      "    │ PRACTICE_PATIENT_ID  ┆ SEX ┆ ETHNICITY ┆ YEAR_OF_BIRTH ┆ INDEX_AGE   ┆ START_AGE   ┆ END_AGE       │\n",
      "    │ ---                  ┆ --- ┆ ---       ┆ ---           ┆ ---         ┆ ---         ┆ ---           │\n",
      "    │ str                  ┆ str ┆ str       ┆ str           ┆ i64 (days)  ┆ i64 (days)  ┆ i64 (days)    │\n",
      "    ╞══════════════════════╪═════╪═══════════╪═══════════════╪═════════════╪═════════════╪═══════════════╡\n",
      "    │ <anonymous 1>        ┆ M   ┆ WHITE     ┆ yyyy--mm-dd   ┆ dd          ┆ dd          ┆ dd            │\n",
      "    │ <anonymous 2>        ┆ F   ┆ MISSING   ┆ yyyy--mm-dd   ┆ dd          ┆ dd          ┆ dd            │\n",
      "    │ …                    ┆ …   ┆ …         ┆ …             ┆             ┆             ┆               │\n",
      "    │ <anonymous N>        ┆ M   ┆ WHITE     ┆ yyyy--mm-dd   ┆ dd          ┆ dd          ┆ dd            │\n",
      "    └──────────────────────┴─────┴───────────┴───────────────┴─────────────┴─────────────┴───────────────┘\n",
      "        \n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(f\"`Static table` docstring:\\n{CPRD.data.database.build_static_db.build_static_table.__doc__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnosis table\n",
    "<div align=\"justify\">\n",
    "The diagnosis table has one row per diagnosis. \n",
    "\n",
    "* PRACTICE_PATIENT_ID: The data owner identifier,\n",
    "* EVENT: The categorical event\n",
    "* AGE_AT_EVENT: Number if days between subject birth and the day of event\n",
    "* VALUE: to be removed\n",
    "</div>\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "  TODO: \n",
    "</div>\n",
    "\n",
    "* We could record based on ICD code subgroups. For example each of these conditions are categorical, but they could be further sub categories. For example, diabetes can be one category and types further divide this.\n",
    "* remove EVENT_TYPE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`Diagnosis table` docstring:\n",
      "\n",
      "    Build measurements and tests table in database\n",
      "\n",
      "    Produced anonymized table:\n",
      "    ┌──────────────────────┬───────┬──────────────┬──────────────┬────────────────────────────┐\n",
      "    │ PRACTICE_PATIENT_ID  ┆ VALUE ┆ EVENT        ┆ AGE_AT_EVENT ┆ EVENT_TYPE                 │\n",
      "    │ ---                  ┆ ---   ┆ ---          ┆ ---          ┆ ---                        │\n",
      "    │ str                  ┆ f64   ┆ str          ┆ i64 (days)   ┆ str                        │\n",
      "    ╞══════════════════════╪═══════╪══════════════╪══════════════╪════════════════════════════╡\n",
      "    │ <anonymous 1>        ┆ null  ┆ HF           ┆ 11632        ┆ categorical                │\n",
      "    │ <anonymous 2>        ┆ null  ┆ HF           ┆ 25635        ┆ categorical                │\n",
      "    │ …                    ┆ …     ┆ …            ┆ …            ┆ …                          │\n",
      "    │ <anonymous N>        ┆ null  ┆ FIBROMYALGIA ┆ 8546         ┆ categorical                │\n",
      "    └──────────────────────┴───────┴──────────────┴──────────────┴────────────────────────────┘\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(f\"`Diagnosis table` docstring:\\n{CPRD.data.database.build_diagnosis_db.build_diagnosis_table.__doc__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measurements and tests table\n",
    "<div align=\"justify\">\n",
    "The measurements table has one row per diagnosis. \n",
    "\n",
    "* PRACTICE_PATIENT_ID: The data owner identifier,\n",
    "* EVENT: The categorical event\n",
    "* AGE_AT_EVENT: Number if days between subject birth and the day of event\n",
    "* VALUE: The measurement/test record\n",
    "* EVENT_TYPE: whether the value is categorical (e.g. EVENT==\"smoking\", value=\"ex-smoker\"), or continuous (e.g. EVENT==\"bmi\", value=23.3)\n",
    "</div>\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "  TODO: \n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`Measurements and tests table` docstring:\n",
      " \n",
      "    Build measurements and tests table in database\n",
      "\n",
      "    Produced anonymized table:\n",
      "    ┌──────────────────────┬───────┬──────────────────┬──────────────┬───────────────────────┐\n",
      "    │ PRACTICE_PATIENT_ID  ┆ VALUE ┆ EVENT            ┆ AGE_AT_EVENT ┆ EVENT_TYPE            │\n",
      "    │ ---                  ┆ ---   ┆ ---              ┆ ---          ┆ ---                   │\n",
      "    │ str                  ┆ f64   ┆ str              ┆ i64 (days)   ┆ str                   │\n",
      "    ╞══════════════════════╪═══════╪══════════════════╪══════════════╪═══════════════════════╡\n",
      "    │ <anonymous 1>        ┆ 23.3  ┆ bmi              ┆ 10254        ┆ univariate_regression │\n",
      "    │ <anonymous 1>        ┆ 24.1  ┆ bmi              ┆ 11829        ┆ univariate_regression │\n",
      "    │ …                    ┆ …     ┆ …                ┆ …            ┆ …                     │\n",
      "    │ <anonymous N>        ┆ 0.17  ┆ eosinophil_count ┆ 12016        ┆ univariate_regression │\n",
      "    └──────────────────────┴───────┴──────────────────┴──────────────┴───────────────────────┘\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(f\"`Measurements and tests table` docstring:\\n{CPRD.data.database.build_measurements_and_tests_db.build_measurements_table.__doc__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering\n",
    "\n",
    "We can query the SQL tables separately to find the set of patients that fit the study criteria. \n",
    "\n",
    "First we can connect directly to the database using sqlite3\n",
    "\n",
    "<div align=\"center\">\n",
    "  TODO: \n",
    "</div>\n",
    "\n",
    "* The db shouldn't really be interfaced with like this, this is mostly for demonstration and a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DB = \"/rds/projects/s/subramaa-mum-predict/CharlesGadd_Oxford/FoundationModel/preprocessing/processed/cprd.db\"\n",
    "conn = sqlite3.connect(PATH_TO_DB)\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing a look-up directly on the tables shows what diagnoses, measurements and tests are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have access to measurements and tests:\n",
      " * bmi\n",
      " * hydroxyvitamin2\n",
      " * hydroxyvitamin3\n",
      " * aspartate_transam\n",
      " * serum_level\n",
      " * creatinine_ratio\n",
      " * basophil_count\n",
      " * blood_calcium\n",
      " * blood_urea\n",
      " * brain_natriuretic_peptide_level\n",
      " * calcium_adjusted_level\n",
      " * calculated_LDL_cholesterol_level\n",
      " * combined_total_vitamin_D2_and_D3_level\n",
      " * corrected_serum_calcium_level\n",
      " * diastolic_blood_pressure\n",
      " * eosinophil_count\n",
      "\n",
      "Additionally, we have access to diagnoses:\n",
      " * HF\n",
      " * AF\n",
      " * ISCHAEMICSTROKE\n",
      " * STROKEUNSPECIFIED\n",
      " * STROKE_HAEMRGIC\n",
      " * HYPERTENSION\n",
      " * MINFARCTION\n",
      " * IHD_NOMI\n",
      " * PAD_STRICT\n",
      " * VALVULARDISEASES\n",
      " * AORTICANEURYSM\n",
      " * TYPE1DM\n",
      " * TYPE2DIABETES\n",
      " * CKDSTAGE3TO5\n",
      " * DEPRESSION\n",
      " * ANXIETY\n",
      " * BIPOLAR\n",
      " * EATINGDISORDERS\n",
      " * SCHIZOPHRENIAMM\n",
      " * AUTISM\n",
      " * ALCOHOLMISUSE\n",
      " * SUBSTANCEMISUSE\n",
      " * CHRONIC_LIVER_DISEASE_ALCOHOL\n",
      " * NAFLD\n",
      " * OTHER_CHRONIC_LIVER_DISEASE_OPTIMAL\n",
      " * ULCERATIVE_COLITIS\n",
      " * CROHNS_DISEASE\n",
      " * ALL_DEMENTIA\n",
      " * PARKINSONS\n",
      " * EPILEPSY\n",
      " * ALLCA_NOBCC_VFINAL\n",
      " * LYMPHOMA_PREVALENCE\n",
      " * LEUKAEMIA_PREVALENCE\n",
      " * PLASMACELL_NEOPLASM\n",
      " * ASTHMA_PUSHASTHMA\n",
      " * COPD\n",
      " * OSA\n",
      " * BRONCHIECTASIS\n",
      " * CYSTICFIBROSIS\n",
      " * ATOPICECZEMA\n",
      " * ALLERGICRHINITISCONJ\n",
      " * HIVAIDS\n",
      " * OSTEOPOROSIS\n",
      " * OSTEOARTHRITIS\n",
      " * RHEUMATOIDARTHRITIS\n",
      " * GOUT\n",
      " * SYSTEMIC_LUPUS_ERYTHEMATOSUS\n",
      " * SJOGRENSSYNDROME\n",
      " * SYSTEMIC_SCLEROSIS\n",
      " * PMRANDGCA\n",
      " * CHRONICFATIGUESYNDROMEMM\n",
      " * HYPOTHYROIDISM_DRAFT_V1\n",
      " * HYPERTHYROIDISM\n",
      " * ADDISON_DISEASE\n",
      " * MS\n",
      " * VISUAL_IMPAIRMENT\n",
      " * MENIERESDISEASE\n",
      " * PERIPHERAL_NEUROPATHY\n",
      " * DOWNSSYNDROME\n",
      " * PERNICIOUSANAEMIA\n",
      " * PSORIASIS\n",
      " * PSORIATICARTHRITIS2021\n",
      " * ILD_SH\n",
      " * PTSDDIAGNOSIS\n",
      " * PREVALENT_IBS\n",
      " * ANY_DEAFNESS_HEARING_LOSS\n",
      " * LEARNINGDISABILITY\n",
      " * ENDOMETRIOSIS_ADENOMYOSIS_V2\n",
      " * POLYCYSTIC_OVARIAN_SYNDROME_PCOS\n",
      " * SICKLE_CELL_DISEASE\n",
      " * HAEMOCHROMATOSIS\n",
      " * FIBROMYALGIA\n"
     ]
    }
   ],
   "source": [
    "# Check what measurements are available\n",
    "cursor.execute(\"SELECT DISTINCT EVENT FROM measurement_table\")\n",
    "measurements = [_item[0] for _item in cursor.fetchall()]\n",
    "print(\"We have access to measurements and tests:\\n * \" + '\\n * '.join(measurements))\n",
    "\n",
    "# Check what diagnoses are available\n",
    "cursor.execute(\"SELECT DISTINCT EVENT FROM diagnosis_table\")\n",
    "diagnoses = [_item[0] for _item in cursor.fetchall()]\n",
    "print(\"\\nAdditionally, we have access to diagnoses:\\n * \" + '\\n * '.join(diagnoses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given some subset of these we can query the database to find those subjects who have at least one entry of a list of measurements, tests or diagnoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p20508_942381720508', 'p20524_2116546320524', 'p20433_213995120433', 'p20655_1198195820655', 'p20433_213498220433', 'p20491_964962720491', 'p20524_2116378620524', 'p20679_996260920679', 'p20692_5495714120692', 'p20741_1100618520741']\n"
     ]
    }
   ],
   "source": [
    "identifiers1 = CPRD.data.database.queries.query_measurement([\"bmi\", \"diastolic_blood_pressure\"], cursor)        \n",
    "identifiers2 = CPRD.data.database.queries.query_diagnosis([\"DEPRESSION\", \"TYPE1DM\", \"TYPE2DIABETES\"], cursor)    #  \"DEPRESSION\"  ,  \"ANXIETY\"\n",
    "all_identifiers = list(set(identifiers1).intersection(identifiers2))    # Turn smaller list into the set\n",
    "\n",
    "print(all_identifiers[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using N=10000 random samples, from the available 117102 with satisfied our criteria\n"
     ]
    }
   ],
   "source": [
    "# For now, lets take only a random subset of 10,000\n",
    "N = np.min((len(all_identifiers), 10000))\n",
    "print(f\"Using N={N} random samples, from the available {len(all_identifiers)} with satisfied our criteria\")\n",
    "\n",
    "identifiers = random.choices(all_identifiers, k=N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make PyTorch dataloader\n",
    "\n",
    "We can now initialise our DL friendly datasets. This is done within the DataModule initialisation. \n",
    "\n",
    "Initialisation takes one required argument, which is the set of patient `identifiers` we wish to build into our dataset. The remaining arguments are optional and explained in the docstring\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "  TODO: \n",
    "</div>\n",
    "\n",
    "* Add option of saving and loading initialised train/test/val splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`FoundationalDataModule` docstring:\n",
      "\n",
      "    PyTorch-Lightning datamodule for foundational models\n",
      "    \n",
      "    ARGS:\n",
      "        practice_patient_id (list[str])\n",
      "            List of practice patient identifiers which satisfy study criteria.\n",
      "            \n",
      "    KWARGS:\n",
      "        batch_size (int): \n",
      "        \n",
      "        unk_freq_threshold (float). \n",
      "            Value between 0 and 1, controlling at what level of frequency rare tokens (equiv. conditions/measurements \n",
      "            with this tokenizer) are mapped to the UNK token. Used to reduce vocabulary size\n",
      "            \n",
      "        min_workers (int):\n",
      "\n",
      "        load_event_stream (optional, str):\n",
      "        \n",
      "        save_event_stream (optional, str):\n",
      "        \n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(f\"`FoundationalDataModule` docstring:\\n{FoundationalDataModule.__doc__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Building polars dataset\n",
      "INFO:root:Using measurements\n",
      "INFO:root:Using diagnoses\n",
      "INFO:root:Dropping samples with no dynamic events\n",
      "INFO:root:Using tabular tokenizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8633 training samples\n",
      "480 validation samples\n",
      "480 test samples\n",
      "89 dictionary elements\n"
     ]
    }
   ],
   "source": [
    "dm = FoundationalDataModule(identifiers=identifiers,\n",
    "                            tokenizer=config.tokenizer,\n",
    "                            batch_size=opt.batch_size,\n",
    "                            max_seq_length=config.block_size,\n",
    "                            unk_freq_threshold=config.unk_freq_threshold,\n",
    "                            include_measurements=True,\n",
    "                            include_diagnoses=True,\n",
    "                            preprocess_measurements=False\n",
    "                            )\n",
    "\n",
    "print(f\"{len(dm.train_set)} training samples\")\n",
    "print(f\"{len(dm.val_set)} validation samples\")\n",
    "print(f\"{len(dm.test_set)} test samples\")\n",
    "print(f\"{dm.tokenizer.vocab_size} dictionary elements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer \n",
    "\n",
    "This creates a tokenizer that is shared across each train/test/val split.\n",
    "\n",
    "The tokenizer can either be \"tabular\", in which values tied to events are treated separately, and loaded in batches under the key `\"values\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Building polars dataset\n",
      "INFO:root:Using measurements\n",
      "INFO:root:Using diagnoses\n",
      "INFO:root:Dropping samples with no dynamic events\n",
      "INFO:root:Using tabular tokenizer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['PAD',\n",
       " 'UNK',\n",
       " 'diastolic_blood_pressure',\n",
       " 'bmi',\n",
       " 'eosinophil_count',\n",
       " 'basophil_count',\n",
       " 'corrected_serum_calcium_level',\n",
       " 'DEPRESSION',\n",
       " 'serum_level',\n",
       " 'calculated_LDL_cholesterol_level']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[     nan,      nan,  80.0000,  ...,  29.5000,  78.0000,  28.7000],\n",
       "        [100.0000,      nan,  80.0000,  ...,  70.0000,  72.0000,  26.6000],\n",
       "        [ 79.0000,  78.0000,  41.6000,  ...,   0.2900,  73.0000,  36.5000],\n",
       "        ...,\n",
       "        [     nan,      nan,      nan,  ...,      nan,      nan,      nan],\n",
       "        [ 80.0000,  84.0000,  82.0000,  ...,      nan,      nan,      nan],\n",
       "        [ 36.0000,  87.0000,  37.6000,  ...,      nan,      nan,      nan]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dm = FoundationalDataModule(identifiers=identifiers,\n",
    "                            tokenizer=\"tabular\",\n",
    "                            batch_size=opt.batch_size,\n",
    "                            max_seq_length=config.block_size,\n",
    "                            unk_freq_threshold=config.unk_freq_threshold,\n",
    "                            include_measurements=True,\n",
    "                            include_diagnoses=True,\n",
    "                            preprocess_measurements=False\n",
    "                            )\n",
    "\n",
    "display(list(dm.tokenizer._stoi.keys())[:10])\n",
    "\n",
    "for batch in dm.train_dataloader():\n",
    "    break\n",
    "display(batch[\"values\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer also be `\"non-tabular\"`, in which values are concatenated onto events.\n",
    "\n",
    "For example, as in https://proceedings.neurips.cc/paper_files/paper/2023/file/3eb7ca52e8207697361b2c0fb3926511-Paper-Conference.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Building polars dataset\n",
      "INFO:root:Using measurements\n",
      "INFO:root:Using diagnoses\n",
      "INFO:root:Dropping samples with no dynamic events\n",
      "INFO:root:Using non-tabular tokenizer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['PAD', 'UNK', '0', '1', '2', '3', '4', '5', '6', '7']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
       "        [nan, nan, nan,  ..., nan, nan, nan],\n",
       "        [nan, nan, nan,  ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan,  ..., nan, nan, nan],\n",
       "        [nan, nan, nan,  ..., nan, nan, nan],\n",
       "        [nan, nan, nan,  ..., nan, nan, nan]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dm = FoundationalDataModule(identifiers=identifiers,\n",
    "                            tokenizer=\"non-tabular\",\n",
    "                            batch_size=opt.batch_size,\n",
    "                            max_seq_length=config.block_size,\n",
    "                            unk_freq_threshold=config.unk_freq_threshold,\n",
    "                            include_measurements=True,\n",
    "                            include_diagnoses=True,\n",
    "                            preprocess_measurements=False\n",
    "                            )\n",
    "\n",
    "display(list(dm.tokenizer._stoi.keys())[:10])\n",
    "\n",
    "for batch in dm.train_dataloader():\n",
    "    break\n",
    "display(batch[\"values\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardisation and pre-processing\n",
    "\n",
    "We can also perform standardisation and pre-processing on creating the dataloader. Note, DExtER already performs outlier removal, but I found it beneficial to repeat this.\n",
    "\n",
    "Normalisation values are stored in a dictionary so the inverse transformation can be performed on any model outputs.\n",
    "\n",
    "This also works for the `\"non-tabular\"` tokenizer, but we do not truncate significant figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Building polars dataset\n",
      "INFO:root:Using measurements\n",
      "INFO:root:Using test/measurement standardisation method: normalise\n",
      "INFO:root:Removing measurement and test outliers. Using three deviations from mean as cutoff\n",
      "INFO:root:Using diagnoses\n",
      "INFO:root:Dropping samples with no dynamic events\n",
      "INFO:root:Using tabular tokenizer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'blood_calcium': (2.3055555555555554, 0.09426010460561489),\n",
       " 'hydroxyvitamin3': (50.486464088397774, 28.253421025622043),\n",
       " 'serum_level': (27.36, 20.39314546302366),\n",
       " 'blood_urea': (6.433777506112473, 3.5687089758899444),\n",
       " 'basophil_count': (0.06952197006276979, 0.0995862931307564),\n",
       " 'combined_total_vitamin_D2_and_D3_level': (55.69924812030075,\n",
       "  28.274832916537175),\n",
       " 'bmi': (29.81121446763248, 7.217455651019206),\n",
       " 'calculated_LDL_cholesterol_level': (2.6379761713089747, 1.044809612939309),\n",
       " 'aspartate_transam': (26.781417112299465, 19.880841072154606),\n",
       " 'corrected_serum_calcium_level': (2.3245091185410347, 0.12969810940207963),\n",
       " 'brain_natriuretic_peptide_level': (100.22884615384615, 155.17429964150236),\n",
       " 'creatinine_ratio': (4.052689655172414, 7.437664867936673),\n",
       " 'diastolic_blood_pressure': (78.92946571661037, 11.728599302849629),\n",
       " 'eosinophil_count': (0.22289105762382594, 0.18930721183486415),\n",
       " 'hydroxyvitamin2': (3.4080597014925367, 2.913374198070332),\n",
       " 'calcium_adjusted_level': (2.3229432213209713, 0.12874156300085612)}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dm = FoundationalDataModule(identifiers=identifiers,\n",
    "                            tokenizer=\"tabular\",\n",
    "                            batch_size=opt.batch_size,\n",
    "                            max_seq_length=config.block_size,\n",
    "                            unk_freq_threshold=config.unk_freq_threshold,\n",
    "                            include_measurements=True,\n",
    "                            include_diagnoses=True,\n",
    "                            preprocess_measurements=True\n",
    "                            )\n",
    "\n",
    "display(dm.standardisation_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The inner workings for the Datamodule: The Dataset\n",
    "\n",
    "Each dataset within the DataModule is a ``cprd.data.dataset.dataset_polars.EventStreamDataset`` object. Within this class we process the SQL tables from a form in which each patient (static) or record (dianosis/measurement/test) has its own row, into a ragged form where each patient has their own row in every case. This does not change the form of the static table. However, the form of the diagnoses and measurements/tests changes - and during the same update we combine these frames to one frame which contains dynamic information.\n",
    "\n",
    "This is all wrapped in the method ``fit()``, but we can see the separate steps below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`_load_dynamic` docstring:\n",
      "    \n",
      "        Load and merge dynamic tables from SQL\n",
      "        \n",
      "        ARGS:\n",
      "            practice_patient_id (list[str])\n",
      "                List of practice patient identifiers which satisfy study criteria.\n",
      "            \n",
      "        KWARGS:\n",
      "            include_measurements: bool = True,\n",
      "                Flag of whether to include measurements and tests in polars dataframe\n",
      "            include_diagnoses: bool = True\n",
      "                Flag of whether to include diagnoses in polars dataframe\n",
      "        \n",
      "        RETURNS:\n",
      "            Polars lazy frame, of the form:\n",
      "            ┌──────────────────────┬───────────────────────┬───────────────────────────────────┬─────────────────────────┬───────────────────────────────────┐\n",
      "            │ PRACTICE_PATIENT_ID  ┆ VALUE                 ┆ EVENT                             ┆ AGE_AT_EVENT            ┆ EVENT_TYPE                        │\n",
      "            │ ---                  ┆ ---                   ┆ ---                               ┆ ---                     ┆ ---                               │\n",
      "            │ str                  ┆ list[f64]             ┆ list[str]                         ┆ list[i64]   (in days)   ┆ list[str]                         │\n",
      "            ╞══════════════════════╪═══════════════════════╪═══════════════════════════════════╪═════════════════════════╪═══════════════════════════════════╡\n",
      "            │ <anonymous 1>        ┆ [null, 21.92]         ┆ [\"diagnosis name\", \"record name\"  ┆ [age 1, age 2]          ┆ [\"categorical\", \"univariate_regre │\n",
      "            │ <anonymous 2>        ┆ [27.1, 75.0, … 91.0]  ┆ [\"record name\", ...]              ┆ [age 1, age 2, … ]      ┆ [\"univariate_regression\", \"univa… │\n",
      "            │ …                    ┆ …                     ┆ …                                 ┆ …                       ┆ …                                 │\n",
      "            │ <anonymous N2>       ┆ [70.0, 0.1, … 80.0]   ┆ [\"record name\", ...]              ┆ [age 1, age 2, … ]      ┆ [\"univariate_regression\", \"univa… │\n",
      "            └──────────────────────┴───────────────────────┴───────────────────────────────────┴─────────────────────────┴───────────────────────────────────┘\n",
      "            (synthetic entries)\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "print(f\"`_load_dynamic` docstring:\\n{CPRD.data.dataset.dataset_polars.EventStreamDataset._load_dynamic.__doc__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is then combined into one DL friendly representation, which our DataModule uses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`_build_DL_representation` docstring:\n",
      "\n",
      "        Build the DL-friendly representation in polars given the list of `practice_patient_id`s which fit study criteria\n",
      "                \n",
      "        ARGS:\n",
      "            practice_patient_ids (list[str])\n",
      "                List of practice patient identifiers which satisfy study criteria.\n",
      "            \n",
      "        KWARGS:\n",
      "        \n",
      "        \n",
      "        RETURNS:\n",
      "            Polars lazy frame, of the (anonymized) form:\n",
      "            ┌──────────────────────┬─────┬─────────────┬───────────────┬──────────────────────┬─────────────────────────┬─────────────────────┬────────────────────────┐\n",
      "            │ PRACTICE_PATIENT_ID  ┆ SEX ┆ ETHNICITY   ┆ YEAR_OF_BIRTH ┆ VALUE                ┆ EVENT                   ┆ AGE_AT_EVENT        ┆ EVENT_TYPE             │\n",
      "            │ ---                  ┆ --- ┆ ---         ┆ ---           ┆ ---                  ┆ ---                     ┆ ---                 ┆ ---                    │\n",
      "            │ str                  ┆ str ┆ str         ┆ str           ┆ list[f64]            ┆ list[str]               ┆ list[i64]           ┆ list[str]              │\n",
      "            ╞══════════════════════╪═════╪═════════════╪═══════════════╪══════════════════════╪═════════════════════════╪═════════════════════╪════════════════════════╡\n",
      "            │ <anonymous 1>        ┆ M   ┆ MISSING     ┆ yyy-mm-dd     ┆ [null, 21.92]        ┆ [\"diagnosis name\", ...] ┆ [age 1, age 2]      ┆ [\"multi_label_cl...\", ]│\n",
      "            │ <anonymous 2>        ┆ F   ┆ WHITE       ┆ yyy-mm-dd     ┆ [27.1, 75.0, … 91.0] ┆ [\"record name\", ...]    ┆ [age 1, age 2, … ]  ┆ [\"univariate_reg...\", ]│\n",
      "            │ …                    ┆ …                 ┆ …             ┆ …                    ┆ …                       ┆ …                   ┆ …                      │\n",
      "            │ <anonymous N>        ┆ F   ┆ SOUTH_ASIAN ┆ yyy-mm-dd     ┆ [70.0, 0.1, … 80.0]  ┆ [\"record name\", ...]    ┆ [age 1, age 2, … ]  ┆ [\"univariate_reg...\", ]│\n",
      "            └──────────────────────┴─────┴─────────────┴───────────────┴──────────────────────┴─────────────────────────┴─────────────────────┴────────────────────────┘\n",
      "            with index cols: (age at index, age at start, age at end)\n",
      "\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "print(f\"`_build_DL_representation` docstring:\\n{CPRD.data.dataset.dataset_polars.EventStreamDataset._build_DL_representation.__doc__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``__getitem__`` within the dataset class then retrieves rows of this table and performs the relevant processing such as tokenization. Speed ups could be obtained by pre-processing this tokenization.\n",
    "\n",
    "As these are ragged lists for memory efficiency, we also provide a ``collate_fn`` in the DataModule. This performs padding - and by performing padding this way we can avoid excessive padding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data iterators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using \\_\\_getitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A single element of the training dataset contains:\n",
      "  * identifier\n",
      "  * tokens\n",
      "  * ages\n",
      "  * values\n",
      "\n",
      "identifier: p20684_2602734120684\n",
      "\n",
      "tokens: tensor([13, 10,  2,  7,  3,  4,  3,  2,  2])\n",
      "... decoding to `ASTHMA_PUSHASTHMA ANXIETY diastolic_blood_pressure DEPRESSION bmi eosinophil_count bmi diastolic_blood_pressure diastolic_blood_pressure`\n",
      "\n",
      "ages: tensor([ 107, 5247, 5415, 5696, 5821, 6143, 7184, 7184, 8465])\n",
      "\n",
      "values: tensor([    nan,     nan, -0.3350,     nan, -1.5533, -0.6492, -0.9714, -0.7613,\n",
      "         0.0913])\n"
     ]
    }
   ],
   "source": [
    "# print(dm.train_set[0])\n",
    "print(\"A single element of the training dataset contains:\\n  * \" + '\\n  * '.join(dm.train_set[0].keys()))\n",
    "\n",
    "sample = np.random.randint(len(dm.train_set))\n",
    "\n",
    "for k, v in dm.train_set[sample].items():\n",
    "    print(f\"\\n{k}: {v}\")\n",
    "    if k == \"tokens\":\n",
    "        print(f\"... decoding to `{dm.decode(v.tolist())}`\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collating into batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A sample from the dataloader batch gives:\n",
      "  * tokens\n",
      "  * ages\n",
      "  * values\n",
      "  * attention_mask\n",
      "\n",
      "For example, a single sample from a collated batch gives (viewing only first 10 elements of each padded sequence):\n",
      "\n",
      "* The time of event (in days since birth) of events:\n",
      "\t tensor([4141, 8894, 8894, 8902, 8902, 8917, 8917, 9110, 9266, 9397])\n",
      "\n",
      "* The tokenized and padded (within batch) block of a patient's sequence of events:\n",
      "\t tensor([19,  5,  4,  5,  4,  5,  4,  2,  2,  3])\n",
      "... which can be decoded. E.g. the above tokens decode to:\n",
      "\t PREVALENT_IBS basophil_count eosinophil_count basophil_count eosinophil_count basophil_count eosinophil_count diastolic_blood_pressure diastolic_blood_pressure bmi`\n",
      "\n",
      "* When we use a 'tabular' tokenizer, the corresponding values are given as:\n",
      "\t tensor([    nan, -0.5977, -0.3322, -0.4973, -0.3322, -0.5977, -0.3322,  0.6028,\n",
      "        -0.5908, -0.1581])\n",
      "\n",
      "* The attention mask tells us which values are padded entries, so we can then mask them alongside the self-attention masking\n",
      "\t This is of shape (B=64 * L=64)\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 0]])\n"
     ]
    }
   ],
   "source": [
    "for idx, batch in enumerate(dm.train_dataloader()):\n",
    "    break\n",
    "print(\"A sample from the dataloader batch gives:\\n  * \" + '\\n  * '.join(batch.keys()))\n",
    "\n",
    "k = 10\n",
    "print(f\"\\nFor example, a single sample from a collated batch gives (viewing only first {k} elements of each padded sequence):\")\n",
    "\n",
    "# ages\n",
    "print(f\"\\n* The time of event (in days since birth) of events:\\n\\t {batch['ages'][0, :k]}\")\n",
    "\n",
    "# tokens\n",
    "print(f\"\\n* The tokenized and padded (within batch) block of a patient's sequence of events:\\n\\t {batch['tokens'][0,:k]}\")\n",
    "print(f\"... which can be decoded. E.g. the above tokens decode to:\\n\\t {dm.decode(batch['tokens'][0,:k].tolist())}`\")\n",
    "\n",
    "# values\n",
    "print(f\"\\n* When we use a 'tabular' tokenizer, the corresponding values are given as:\\n\\t {batch['values'][0,:k]}\")\n",
    "\n",
    "# attention mask\n",
    "#   B = batch size, L = Longest sequence length within batch\n",
    "print(f\"\\n* The attention mask tells us which values are padded entries, so we can then mask them alongside the self-attention masking\" + \n",
    "      f\"\\n\\t This is of shape (B={batch['attention_mask'].shape[0]} * L={batch['attention_mask'].shape[1]})\" +\n",
    "      f\"\\n{batch['attention_mask']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular vs. non-tabular tokenization\n",
    "\n",
    "We can also use a non-tabular tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Building polars dataset\n",
      "INFO:root:Using measurements\n",
      "INFO:root:Using test/measurement standardisation method: normalise\n",
      "INFO:root:Removing measurement and test outliers. Using three deviations from mean as cutoff\n",
      "INFO:root:Using diagnoses\n",
      "INFO:root:Dropping samples with no dynamic events\n",
      "INFO:root:Using non-tabular tokenizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8603 training samples\n",
      "478 validation samples\n",
      "478 test samples\n",
      "101 dictionary elements\n",
      "\n",
      "Dictionaries\n",
      "{0: 'PAD', 1: 'UNK', 2: '0', 3: '1', 4: '2', 5: '3', 6: '4', 7: '5', 8: '6', 9: '7', 10: '8', 11: '9', 12: '.', 13: 'diastolic_blood_pressure', 14: 'bmi', 15: 'eosinophil_count', 16: 'basophil_count', 17: 'corrected_serum_calcium_level', 18: 'DEPRESSION', 19: 'serum_level', 20: 'calculated_LDL_cholesterol_level', 21: 'ANXIETY', 22: 'HYPERTENSION', 23: 'TYPE2DIABETES', 24: 'OSTEOARTHRITIS', 25: 'ASTHMA_PUSHASTHMA', 26: 'ATOPICECZEMA', 27: 'ALLERGICRHINITISCONJ', 28: 'aspartate_transam', 29: 'ANY_DEAFNESS_HEARING_LOSS', 30: 'PREVALENT_IBS', 31: 'IHD_NOMI', 32: 'ALCOHOLMISUSE', 33: 'ALLCA_NOBCC_VFINAL', 34: 'CKDSTAGE3TO5', 35: 'blood_urea', 36: 'PERIPHERAL_NEUROPATHY', 37: 'HYPOTHYROIDISM_DRAFT_V1', 38: 'calcium_adjusted_level', 39: 'COPD', 40: 'combined_total_vitamin_D2_and_D3_level', 41: 'AF', 42: 'PSORIASIS', 43: 'HF', 44: 'GOUT', 45: 'OSTEOPOROSIS', 46: 'SUBSTANCEMISUSE', 47: 'MINFARCTION', 48: 'STROKEUNSPECIFIED', 49: 'ALL_DEMENTIA', 50: 'VALVULARDISEASES', 51: 'hydroxyvitamin3', 52: 'PAD_STRICT', 53: 'hydroxyvitamin2', 54: 'OTHER_CHRONIC_LIVER_DISEASE_OPTIMAL', 55: 'FIBROMYALGIA', 56: 'OSA', 57: 'TYPE1DM', 58: 'EPILEPSY', 59: 'NAFLD', 60: 'POLYCYSTIC_OVARIAN_SYNDROME_PCOS', 61: 'EATINGDISORDERS', 62: 'ENDOMETRIOSIS_ADENOMYOSIS_V2', 63: 'RHEUMATOIDARTHRITIS', 64: 'PMRANDGCA', 65: 'creatinine_ratio', 66: 'HYPERTHYROIDISM', 67: 'PTSDDIAGNOSIS', 68: 'VISUAL_IMPAIRMENT', 69: 'BIPOLAR', 70: 'ISCHAEMICSTROKE', 71: 'SCHIZOPHRENIAMM', 72: 'BRONCHIECTASIS', 73: 'CHRONICFATIGUESYNDROMEMM', 74: 'CHRONIC_LIVER_DISEASE_ALCOHOL', 75: 'AORTICANEURYSM', 76: 'PERNICIOUSANAEMIA', 77: 'MENIERESDISEASE', 78: 'ULCERATIVE_COLITIS', 79: 'PARKINSONS', 80: 'LEARNINGDISABILITY', 81: 'PSORIATICARTHRITIS2021', 82: 'brain_natriuretic_peptide_level', 83: 'CROHNS_DISEASE', 84: 'ILD_SH', 85: 'LYMPHOMA_PREVALENCE', 86: 'STROKE_HAEMRGIC', 87: 'AUTISM', 88: 'MS', 89: 'SJOGRENSSYNDROME', 90: 'LEUKAEMIA_PREVALENCE', 91: 'HIVAIDS', 92: 'blood_calcium', 93: 'SYSTEMIC_LUPUS_ERYTHEMATOSUS', 94: 'HAEMOCHROMATOSIS', 95: 'PLASMACELL_NEOPLASM', 96: 'SYSTEMIC_SCLEROSIS', 97: 'ADDISON_DISEASE', 98: 'DOWNSSYNDROME', 99: 'CYSTICFIBROSIS', 100: 'SICKLE_CELL_DISEASE'}\n"
     ]
    }
   ],
   "source": [
    "dm = FoundationalDataModule(identifiers=identifiers,\n",
    "                            tokenizer=\"non-tabular\",\n",
    "                            batch_size=opt.batch_size,\n",
    "                            max_seq_length=config.block_size,\n",
    "                            unk_freq_threshold=config.unk_freq_threshold)\n",
    "\n",
    "print(f\"{len(dm.train_set)} training samples\")\n",
    "print(f\"{len(dm.val_set)} validation samples\")\n",
    "print(f\"{len(dm.test_set)} test samples\")\n",
    "print(f\"{dm.tokenizer.vocab_size} dictionary elements\")\n",
    "\n",
    "print(\"\\nDictionaries\")\n",
    "print(dm.tokenizer._itos)\n",
    "# print(dm.tokenizer._stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A sample from the dataloader batch gives:\n",
      "  * tokens\n",
      "  * ages\n",
      "  * values\n",
      "  * attention_mask\n",
      "\n",
      "Repeating the above example, a single sample from a collated batch gives (viewing only first 10 elements of each sequence):\n",
      "\n",
      "* The time of event (in days since birth) of events:\n",
      "\t tensor([7004, 7009, 7009, 7009, 7009, 7009, 7009, 7009, 7009, 7009])\n",
      "\n",
      "* The tokenized and padded (within batch) block of a patient's sequence of events:\n",
      "\t tensor([ 6, 15,  2, 12,  4, 11,  8,  4,  2,  6])\n",
      "... which can be decoded. E.g. the above tokens decode to:\n",
      "\t 4 eosinophil_count 0 . 2 9 6 2 0 4`\n",
      "\n",
      "* When we use a tabular tokenizer, the corresponding values are given as:\n",
      "\t tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "\n",
      "* The attention mask tells us which values are padded entries, so we can then mask them alongside the self-attention masking\n",
      "\t This is of shape (B=64 * L=64)\n",
      "tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "for idx, batch in enumerate(dm.train_dataloader()):\n",
    "    break\n",
    "print(\"A sample from the dataloader batch gives:\\n  * \" + '\\n  * '.join(batch.keys()))\n",
    "\n",
    "k = 10\n",
    "print(f\"\\nRepeating the above example, a single sample from a collated batch gives (viewing only first {k} elements of each sequence):\")\n",
    "\n",
    "# ages\n",
    "print(f\"\\n* The time of event (in days since birth) of events:\\n\\t {batch['ages'][0, :k]}\")\n",
    "\n",
    "# tokens\n",
    "print(f\"\\n* The tokenized and padded (within batch) block of a patient's sequence of events:\\n\\t {batch['tokens'][0,:k]}\")\n",
    "print(f\"... which can be decoded. E.g. the above tokens decode to:\\n\\t {dm.decode(batch['tokens'][0,:k].tolist())}`\")\n",
    "\n",
    "# values\n",
    "print(f\"\\n* When we use a {config.tokenizer} tokenizer, the corresponding values are given as:\\n\\t {batch['values'][0,:k]}\")\n",
    "\n",
    "# attention mask\n",
    "#   B = batch size, L = Longest sequence length within batch\n",
    "print(f\"\\n* The attention mask tells us which values are padded entries, so we can then mask them alongside the self-attention masking\" + \n",
    "      f\"\\n\\t This is of shape (B={batch['attention_mask'].shape[0]} * L={batch['attention_mask'].shape[1]})\" +\n",
    "      f\"\\n{batch['attention_mask']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "sys_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
