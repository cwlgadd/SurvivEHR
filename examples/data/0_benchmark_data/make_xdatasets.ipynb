{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4921342e-8ff8-45d4-b1f6-9fe480c2ee1d",
   "metadata": {},
   "source": [
    "# CPRD Notebook:\n",
    "## Evaluation of fine-tuning the pre-trained SurvivEHR-CR model on a supervised cohort study.\n",
    "\n",
    "Cohort study: predicting Cardiovascular Disease in a Type 2 Diabetes Mellitus population.\n",
    "\n",
    "This notebook quantifies the performance obtained when fine-tuning the pre-trained model to a sub-population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a671c59b-4428-4e63-a138-7244418a87c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added path '/rds/homes/g/gaddcz/Projects/CPRD/virtual-envTorch2.0-icelake/lib/python3.10/site-packages' at start of search paths.\n",
      "/rds/homes/g/gaddcz/Projects/CPRD/examples/data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "node_type = os.getenv('BB_CPU')\n",
    "venv_dir = f'/rds/homes/g/gaddcz/Projects/CPRD/virtual-envTorch2.0-{node_type}'\n",
    "venv_site_pkgs = Path(venv_dir) / 'lib' / f'python{sys.version_info.major}.{sys.version_info.minor}' / 'site-packages'\n",
    "if venv_site_pkgs.exists():\n",
    "    sys.path.insert(0, str(venv_site_pkgs))\n",
    "    print(f\"Added path '{venv_site_pkgs}' at start of search paths.\")\n",
    "else:\n",
    "    print(f\"Path '{venv_site_pkgs}' not found. Check that it exists and/or that it exists for node-type '{node_type}'.\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch.multiprocessing\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "print(os.getcwd())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d866c7f0-eaa8-4129-b3b1-d4d6b504d0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda.\n",
      "env: SLURM_NTASKS_PER_NODE=28\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "from CPRD.examples.modelling.SurvivEHR.run_experiment import run\n",
    "from FastEHR.dataloader.foundational_loader import FoundationalDataModule\n",
    "import pickle \n",
    "\n",
    "from sklearn import set_config\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "from sksurv.datasets import load_gbsg2\n",
    "from sksurv.preprocessing import OneHotEncoder\n",
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "from CPRD.src.modules.head_layers.survival.desurv import ODESurvSingle\n",
    "from pycox.evaluation import EvalSurv\n",
    "\n",
    "import time\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "import polars as pl\n",
    "pl.Config.set_tbl_rows(10000)\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 10000\n",
    "\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = \"cpu\"    # if more informative debugging statements are needed\n",
    "print(f\"Using device: {device}.\")\n",
    "\n",
    " # TODO: define an env variable to fix for a local hpc environment issue, this shouldn't be needed\n",
    "%env SLURM_NTASKS_PER_NODE=28   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfbfbc95-0553-4f07-aca4-c25ef905acf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(datamodule, target_tokens, split='train', n=None):\n",
    "\n",
    "    X = pd.DataFrame(columns=[f'static_{_idx}' for _idx in range(16)] + [f'{datamodule.train_set.tokenizer._itos[_idx]}' for _idx in range(2,vocab_size)])\n",
    "    Y = []\n",
    "\n",
    "    match split:\n",
    "        case 'train':\n",
    "            dataloader = datamodule.train_dataloader()\n",
    "        case 'val':\n",
    "            dataloader = datamodule.val_dataloader()\n",
    "        case 'test':\n",
    "            dataloader = datamodule.test_dataloader()\n",
    "        case _:\n",
    "            raise NotImplementedError\n",
    "    \n",
    "    for b_idx, batch in tqdm(enumerate(dataloader), total=n, desc=f\"Creating {split} cross-sectional dataset to be used for benchmarking.\"):\n",
    "\n",
    "        # Input\n",
    "        ########\n",
    "        # Static variables are already processed into categories where required\n",
    "        static = batch[\"static_covariates\"].numpy()\n",
    "    \n",
    "        # Get a binary vector of vocab_size elements, which indicate if patient has any history of a condition (at any time, as long as it fits study criteria)\n",
    "        # Note, 0 and 1 are PAD and UNK tokens which arent required\n",
    "        input_tokens = batch[\"tokens\"]\n",
    "        token_binary = np.zeros((static.shape[0], vocab_size-2))\n",
    "        for s_idx in range(static.shape[0]):\n",
    "            for tkn_idx in range(2, vocab_size):\n",
    "                if tkn_idx in input_tokens[s_idx, :]:\n",
    "                    token_binary[s_idx, tkn_idx-2] = 1\n",
    "    \n",
    "        batch_input = np.hstack((static, token_binary))\n",
    "        batch_df = pd.DataFrame(batch_input, columns=X.columns)\n",
    "        X = pd.concat([X, batch_df])\n",
    "        \n",
    "        # Target\n",
    "        ########\n",
    "        targets = batch[\"target_token\"].numpy()\n",
    "        for s_idx in range(static.shape[0]):\n",
    "            # default to 0\n",
    "            target = 0\n",
    "\n",
    "            # replace with target if its in the outcome set\n",
    "            for idx_outcome, outcome in enumerate(target_tokens):\n",
    "                if targets[s_idx] == outcome:\n",
    "                    target = idx_outcome + 1\n",
    "            \n",
    "            Y.append((target, batch[\"target_age_delta\"][s_idx] ))\n",
    "    \n",
    "        # if n is not None and b_idx >= n:\n",
    "        #     break\n",
    "\n",
    "    y = np.array(Y, dtype=[('Status', 'int'), ('Survival_in_days', '<f8')])\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bad2373-14f7-4fd0-b02e-072a429a990a",
   "metadata": {},
   "source": [
    "## Load or create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e47f9da-7369-4fe2-af9b-0f06c728a425",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sizes = [int(np.exp(_log_n)) for _log_n in np.linspace(np.log(3000), np.log(500000), 10)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e00688-1426-4046-910c-45bde94db899",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:====================================================================================================\n",
      "INFO:root:# Loading DataModule for dataset /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_MultiMorbidity50+/. This will be loaded in supervised form.\n",
      "INFO:root:====================================================================================================\n",
      "INFO:root:Creating supervised collator for DataModule\n",
      "INFO:root:Scaling supervised target ages by a factor of 1.0 times the context scale.\n",
      "INFO:root:Using meta information from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/meta_information_QuantJenny.pickle\n",
      "INFO:root:Using train file-row count dictionary from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_MultiMorbidity50+/file_row_count_dict_train.pickle\n",
      "INFO:root:Using test file-row count dictionary from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_MultiMorbidity50+/file_row_count_dict_test.pickle\n",
      "INFO:root:Using val file-row count dictionary from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_MultiMorbidity50+/file_row_count_dict_val.pickle\n",
      "INFO:root:Tokenzier created based on 7,555,415,275 tokens\n",
      "INFO:root:Using tabular tokenizer, created from meta information and containing 265 tokens\n",
      "INFO:root:Set seed to 42\n",
      "INFO:root:Loaded /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_MultiMorbidity50+/split=train/ dataset, with 1,650,998 samples\n",
      "INFO:root:Set seed to 42\n",
      "INFO:root:Loaded /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_MultiMorbidity50+/split=test/ dataset, with 107,557 samples\n",
      "INFO:root:Set seed to 42\n",
      "INFO:root:Loaded /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_MultiMorbidity50+/split=val/ dataset, with 91,487 samples\n",
      "Creating train cross-sectional dataset to be used for benchmarking.:  18%|█▊        | 4738/25797 [59:30<6:54:49,  1.18s/it]"
     ]
    }
   ],
   "source": [
    "experiments = [ \"mm\"]   # \"Hypertension\", \"\" \"MM\"  \"cvd\"\n",
    "sample_sizes = [None]\n",
    "# sample_sizes = [int(np.exp(_log_n)) for _log_n in np.linspace(np.log(3000), np.log(500000), 10)] #+ [None]\n",
    "# sample_sizes = sample_sizes[-1:]\n",
    "\n",
    "\n",
    "for experiment in experiments:\n",
    "    for sample_size in sample_sizes:\n",
    "\n",
    "            # set the seeds - if we are using all data there is no need to bootstrap\n",
    "            if sample_size is None:\n",
    "                seeds = [42]\n",
    "            else:\n",
    "                seeds = [1,2,3,4,5]\n",
    "                \n",
    "            for seed in seeds:\n",
    "                \n",
    "                # load the configuration file, override any settings \n",
    "                with initialize(version_base=None, config_path=\"../modelling/SurvivEHR/confs\", job_name=\"testing_notebook\"):\n",
    "                    cfg = compose(config_name=\"config_CompetingRisk37M\") \n",
    "                    cfg.transformer.block_size=1000000     # Ensure all records get included\n",
    "            \n",
    "                match experiment.lower():\n",
    "                    case \"cvd\":\n",
    "                        cfg.data.path_to_ds=\"/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/\"\n",
    "                    case \"hypertension\":\n",
    "                        cfg.data.path_to_ds=\"/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_Hypertension/\"\n",
    "                    case \"mm\":\n",
    "                        # cfg.data.path_to_ds=\"/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_MultiMorbidity2/\"\n",
    "                        cfg.data.path_to_ds=\"/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_MultiMorbidity50+/\"\n",
    "                        \n",
    "            \n",
    "                supervised = True \n",
    "                logging.info(\"=\"*100)\n",
    "                logging.info(f\"# Loading DataModule for dataset {cfg.data.path_to_ds}. This will be loaded in {'supervised' if supervised else 'causal'} form.\")\n",
    "                logging.info(\"=\"*100)\n",
    "                dm = FoundationalDataModule(path_to_db=cfg.data.path_to_db,\n",
    "                                            path_to_ds=cfg.data.path_to_ds,\n",
    "                                            load=True,\n",
    "                                            tokenizer=\"tabular\",\n",
    "                                            batch_size=cfg.data.batch_size,\n",
    "                                            max_seq_length=cfg.transformer.block_size,\n",
    "                                            global_diagnoses=cfg.data.global_diagnoses,\n",
    "                                            freq_threshold=cfg.data.unk_freq_threshold,\n",
    "                                            min_workers=cfg.data.min_workers,\n",
    "                                            overwrite_meta_information=cfg.data.meta_information_path,\n",
    "                                            supervised=supervised,\n",
    "                                            subsample_training=sample_size,\n",
    "                                            seed=seed,\n",
    "                                           )\n",
    "                if sample_size is not None:\n",
    "                    print(dm.train_set.subsample_indicies)\n",
    "                # Get required information from initialised dataloader\n",
    "                # ... vocab size\n",
    "                vocab_size = dm.train_set.tokenizer.vocab_size\n",
    "                # ... Extract the measurements, using the fact that the diagnoses are all up upper case. This is needed for automatically setting the configuration below\n",
    "                #     encode into the list of univariate measurements to model with Normal distribution\n",
    "                # measurements_for_univariate_regression = [record for record in dm.tokenizer._event_counts[\"EVENT\"] if record.upper() != record]\n",
    "                # cfg.head.tokens_for_univariate_regression = dm.encode(measurements_for_univariate_regression) #\n",
    "                measurements_for_univariate_regression = dm.train_set.meta_information[\"measurement_tables\"][dm.train_set.meta_information[\"measurement_tables\"][\"count_obs\"] > 0][\"event\"].to_list()\n",
    "                cfg.head.tokens_for_univariate_regression = dm.encode(measurements_for_univariate_regression)\n",
    "                logging.debug(OmegaConf.to_yaml(cfg))\n",
    "        \n",
    "                match experiment.lower():\n",
    "                    case \"cvd\":\n",
    "                        conditions = [\"IHDINCLUDINGMI_OPTIMALV2\", \"ISCHAEMICSTROKE_V2\", \"MINFARCTION\", \"STROKEUNSPECIFIED_V2\", \"STROKE_HAEMRGIC\"]\n",
    "                        cfg.experiment.fine_tune_outcomes=conditions\n",
    "                    case \"hypertension\":\n",
    "                        conditions = [\"HYPERTENSION\"]\n",
    "                        cfg.experiment.fine_tune_outcomes=conditions\n",
    "                    case \"mm\":\n",
    "                        conditions = (\n",
    "                            dm.tokenizer._event_counts.filter((pl.col(\"COUNT\") > 0) &\n",
    "                                (pl.col(\"EVENT\").str.contains(r'^[A-Z0-9_]+$')))\n",
    "                              .select(\"EVENT\")\n",
    "                              .to_series()\n",
    "                              .to_list()\n",
    "                        )\n",
    "                        cfg.experiment.fine_tune_outcomes=conditions\n",
    "                \n",
    "                target_tokens = dm.encode(conditions)\n",
    "            \n",
    "                # print(OmegaConf.to_yaml(cfg))\n",
    "        \n",
    "                if sample_size is not None:\n",
    "                    save_path = cfg.data.path_to_ds + f\"benchmark_data/N={sample_size}_seed{seed}.pickle\" \n",
    "                else:\n",
    "                    save_path = cfg.data.path_to_ds + \"benchmark_data/all.pickle\"\n",
    "                \n",
    "                try:\n",
    "                    # Load the pickled file for testing\n",
    "                    print(f\"Trying to load {save_path}\")\n",
    "                    \n",
    "                    with open(save_path, \"rb\") as handle:\n",
    "                        data = pickle.load(handle)\n",
    "                \n",
    "                except:\n",
    "                    print(f\"Loading failed, creating dataset\")\n",
    "                    \n",
    "                    # Training set\n",
    "                    n_train =  len(dm.train_dataloader()) \n",
    "                    X_train, y_train = make_dataset(dm, target_tokens, split='train', n=n_train)    \n",
    "        \n",
    "                    data = {\n",
    "                        \"X_train\": X_train,\n",
    "                        \"y_train\": y_train,\n",
    "                    }\n",
    "        \n",
    "                     # Test and validation sets - only for the full dataset version, as there is no point repeating this operation\n",
    "                    if sample_size is None:\n",
    "                        \n",
    "                        n_val = len(dm.val_dataloader())  \n",
    "                        X_val, y_val = make_dataset(dm, target_tokens, split='val', n=n_val)\n",
    "                        \n",
    "                        n_test = len(dm.test_dataloader())  \n",
    "                        X_test, y_test = make_dataset(dm, target_tokens, split='test', n=n_test)\n",
    "                        \n",
    "                        print(X_test)\n",
    "                        print(X_test.head())\n",
    "                        \n",
    "                        data = {**data,\n",
    "                                \"X_val\": X_val,\n",
    "                                \"y_val\": y_val,\n",
    "                                \"X_test\": X_test,\n",
    "                                \"y_test\": y_test\n",
    "                                }\n",
    "                    \n",
    "                    with open(save_path, 'wb') as handle:\n",
    "                        pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                        print(f\"Saving to {save_path}\")\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765e2647-4695-4c8c-b598-fe1882730f6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1849ed90-83c7-4451-8746-7288f6f4ac4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a12730-fb1d-46cd-81b0-38c5d2a021af",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.decode([33])\n",
    "dm.tokenizer._stoi\n",
    "dm.tokenizer._event_counts\n",
    "\n",
    "\n",
    "print(events_more_than_zero)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c147964-b3fa-407c-a988-75873d872eb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b4d73f-d28d-4f6e-952a-7fb76cc1bf78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5ae0af-10c4-4c0c-9d21-e4c3cf3d07b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e606ced-beae-4b0f-87f7-462adfc4e671",
   "metadata": {},
   "outputs": [],
   "source": [
    "supervised = True if cfg.experiment.fine_tune_outcomes is not None else False\n",
    "logging.info(\"=\"*100)\n",
    "logging.info(f\"# Loading DataModule for dataset {cfg.data.path_to_ds}. This will be loaded in {'supervised' if supervised else 'causal'} form.\")\n",
    "logging.info(\"=\"*100)\n",
    "dm = FoundationalDataModule(path_to_db=cfg.data.path_to_db,\n",
    "                            path_to_ds=cfg.data.path_to_ds,\n",
    "                            load=True,\n",
    "                            tokenizer=\"tabular\",\n",
    "                            batch_size=cfg.data.batch_size,\n",
    "                            max_seq_length=cfg.transformer.block_size,\n",
    "                            global_diagnoses=cfg.data.global_diagnoses,\n",
    "                            freq_threshold=cfg.data.unk_freq_threshold,\n",
    "                            min_workers=cfg.data.min_workers,\n",
    "                            overwrite_meta_information=cfg.data.meta_information_path,\n",
    "                            supervised=supervised\n",
    "                           )\n",
    "# Get required information from initialised dataloader\n",
    "# ... vocab size\n",
    "vocab_size = dm.train_set.tokenizer.vocab_size\n",
    "# ... Extract the measurements, using the fact that the diagnoses are all up upper case. This is needed for automatically setting the configuration below\n",
    "#     encode into the list of univariate measurements to model with Normal distribution\n",
    "# measurements_for_univariate_regression = [record for record in dm.tokenizer._event_counts[\"EVENT\"] if record.upper() != record]\n",
    "# cfg.head.tokens_for_univariate_regression = dm.encode(measurements_for_univariate_regression) #\n",
    "measurements_for_univariate_regression = dm.train_set.meta_information[\"measurement_tables\"][dm.train_set.meta_information[\"measurement_tables\"][\"count_obs\"] > 0][\"event\"].to_list()\n",
    "cfg.head.tokens_for_univariate_regression = dm.encode(measurements_for_univariate_regression)\n",
    "logging.debug(OmegaConf.to_yaml(cfg))\n",
    "\n",
    "# target_tokens = dm.encode(['IHDINCLUDINGMI_OPTIMALV2', 'ISCHAEMICSTROKE_V2', 'MINFARCTION', 'STROKEUNSPECIFIED_V2', 'STROKE_HAEMRGIC'])\n",
    "target_tokens = dm.encode(['HYPERTENSION'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f61fa0f-4217-40d7-8996-73301eaf736a",
   "metadata": {},
   "source": [
    "# Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589fa98a-4455-42ec-8871-ef22fef8adc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for batch in dm.train_dataloader():\n",
    "#     break\n",
    "# print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032cb2f5-10f5-4b90-a8df-d4ea7d05a513",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# X_train, y_train = make_static_dataset(dm.train_set, n=1e6)\n",
    "# X_test, y_test = make_static_dataset(dm.test_set, n=None)\n",
    "# # print(Y)\n",
    "# # print(X.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4b2ba2-22c3-48fa-8df3-ffd201700636",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "load = False\n",
    "\n",
    "if load is False:\n",
    "    \n",
    "    n_train =  len(dm.train_dataloader()) \n",
    "    X_train, y_train = make_xsectional_dataset2(dm, target_tokens, split='train', n=n_train)    \n",
    "\n",
    "    n_val = len(dm.val_dataloader())  \n",
    "    X_val, y_val = make_xsectional_dataset2(dm, target_tokens, split='val', n=n_val)#\n",
    "\n",
    "    n_test = len(dm.test_dataloader())  \n",
    "    X_test, y_test = make_xsectional_dataset2(dm, target_tokens, split='test', n=n_test)\n",
    "\n",
    "    print(X_test)\n",
    "    print(X_test.head())\n",
    "    \n",
    "    import pickle \n",
    "    \n",
    "    data = {\n",
    "        \"X_train\": X_train,\n",
    "        \"y_train\": y_train,\n",
    "        \"X_val\": X_val,\n",
    "        \"y_val\": y_val,\n",
    "        \"X_test\": X_test,\n",
    "        \"y_test\": y_test\n",
    "    }\n",
    "    with open('/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_Hypertension/xsectional_data_CR.pickle', 'wb') as handle:\n",
    "        pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    with open('/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_Hypertension/xsectional_data_CR.pickle', \"rb\") as handle:\n",
    "        data = pickle.load(handle)\n",
    "\n",
    "    X_train = data[\"X_train\"]\n",
    "    y_train = data[\"y_train\"]\n",
    "    X_val = data[\"X_val\"][:10000]\n",
    "    y_val = data[\"y_val\"][:10000]\n",
    "    X_test = data[\"X_test\"][:10000]\n",
    "    y_test = data[\"y_test\"][:10000]\n",
    "\n",
    "    # print(X_train.shape)\n",
    "    # print(y_train)\n",
    "    print(X_val.shape)\n",
    "    print(y_val)\n",
    "    # print(X_test.shape)\n",
    "    # print(y_test.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c7d206-b50a-426f-8683-36bb276e394b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique([_i[0] for _i in y_test], return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4373fe01-886a-42ae-a505-1a21019c8a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c07623f-a10e-4a75-8086-d237766b40a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target_ages = np.asarray([i[1] for i in y_test])\n",
    "lbls = np.asarray([1 if i[0] == True else 0 for i in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5f222d-0065-4794-a98a-2cc6807a37cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dm.train_set.tokenizer._stoi\n",
    "print(target_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5cc5ca-6493-42f0-b231-3d6021ca447f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_time_scale = 365*5                               \n",
    "# the time grid which we generate over\n",
    "t_eval = np.linspace(0, _time_scale, _time_scale + 1) \n",
    "print(t_eval[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9633888-6abc-4b14-bbe4-2d024c3097d5",
   "metadata": {},
   "source": [
    "# Random Survival Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fe9707-3774-4472-b12a-b1aa06507126",
   "metadata": {},
   "outputs": [],
   "source": [
    "rsf = RandomSurvivalForest(\n",
    "    n_estimators=100, n_jobs=-1, random_state=1337,\n",
    "    bootstrap=True, max_samples=1000, low_memory=False,\n",
    "    # min_samples_split=50,\n",
    "    # min_samples_leaf=15, \n",
    ")\n",
    "# rsf.unique_times_ = t_eval\n",
    "est = rsf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d20f78-cde5-4ca4-a5e9-2d744c0fe61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# est.unique_times_ = t_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00ee6b2-59b6-4484-a6be-7cc8550755e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rsf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d35471-825a-4f1e-ad27-45711ec1572d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.metrics import integrated_brier_score\n",
    "\n",
    "survs = est.predict_survival_function(X_test)\n",
    "# times = np.arange(1, 365*5)[::20]\n",
    "# preds = np.asarray([[fn(t) for t in t_eval] for fn in survs])\n",
    "# score = integrated_brier_score(y_train, y_test, preds, times)\n",
    "\n",
    "# print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2916fa08-c7da-4137-b17d-7d30da9b4444",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(survs.shape)\n",
    "preds = np.asarray([[fn(t) for t in t_eval] for fn in survs])\n",
    "\n",
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617c6f81-8c6f-4293-bccd-f6ebd6ff756d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3e0b84-339c-4bf7-a399-4a2d4879ab28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate concordance. Scale using the head layers internal scaling.\n",
    "# surv = pd.DataFrame(np.transpose((1 - cdf)), index=_pl_module.model.surv_layer.t_eval)\n",
    "\n",
    "surv = pd.DataFrame(np.transpose(preds), index=t_eval)\n",
    "ev = EvalSurv(surv, target_ages, lbls, censor_surv='km')\n",
    "\n",
    "time_grid = np.linspace(start=0, stop=t_eval[-1] , num=300)\n",
    "print(ev.concordance_td())\n",
    "print(ev.integrated_brier_score(time_grid))\n",
    "print(ev.integrated_nbll(time_grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54bf04f-c64c-4f91-9c89-783e361d8f8e",
   "metadata": {},
   "source": [
    "# DeSurv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca10359-06da-4f94-af70-e4411aa85b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "desurv_model = ODESurvSingle(cov_dim=X_train.shape[1],\n",
    "                             hidden_dim=[],\n",
    "                             n=15)\n",
    "\n",
    "batch_size = data_loader.batch_size\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch_idx, (x, t, k) in enumerate(data_loader):\n",
    "        argsort_t = torch.argsort(t)\n",
    "        x_ = x[argsort_t,:].to(self.odenet.device)\n",
    "        t_ = t[argsort_t].to(self.odenet.device)\n",
    "        k_ = k[argsort_t].to(self.odenet.device)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.forward(x_,t_,k_)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    if epoch % logging_freq == 0:\n",
    "        print(f\"\\tEpoch: {epoch:2}. Total loss: {train_loss:11.2f}\")\n",
    "        if data_loader_val is not None:\n",
    "            val_loss = 0\n",
    "            for batch_idx, (x, t, k) in enumerate(data_loader_val):\n",
    "                argsort_t = torch.argsort(t)\n",
    "                x_ = x[argsort_t,:].to(self.odenet.device)\n",
    "                t_ = t[argsort_t].to(self.odenet.device)\n",
    "                k_ = k[argsort_t].to(self.odenet.device)\n",
    "\n",
    "                loss = self.forward(x_,t_,k_)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                wait = 0\n",
    "                print(f\"best_epoch: {epoch}\")\n",
    "                torch.save(self.state_dict(), \"low_\")\n",
    "            else:\n",
    "                wait += 1\n",
    "\n",
    "            if wait > max_wait:\n",
    "                state_dict = torch.load(\"low_\")\n",
    "                self.load_state_dict(state_dict)\n",
    "                return\n",
    "\n",
    "            print(f\"\\tEpoch: {epoch:2}. Total val loss: {val_loss:11.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cf17ea-f1b0-4f76-be25-b49f46fb2c55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "sys_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
