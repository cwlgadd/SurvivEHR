{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f58899f-4329-4fff-b4cb-cf80a4e7a4e4",
   "metadata": {},
   "source": [
    "# Creating the parquet dataset from SQLite tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9c70662-7dcf-4a1f-a343-8afd16a58be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added path '/rds/homes/g/gaddcz/Projects/CPRD/virtual-envTorch2.0-icelake/lib/python3.10/site-packages' at start of search paths.\n",
      "/rds/homes/g/gaddcz/Projects/CPRD/examples/data/3_build_fine_tuning_datasets/Study3_MultiMorbidity\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "node_type = os.getenv('BB_CPU')\n",
    "venv_dir = f'/rds/homes/g/gaddcz/Projects/CPRD/virtual-envTorch2.0-{node_type}'\n",
    "venv_site_pkgs = Path(venv_dir) / 'lib' / f'python{sys.version_info.major}.{sys.version_info.minor}' / 'site-packages'\n",
    "if venv_site_pkgs.exists():\n",
    "    sys.path.insert(0, str(venv_site_pkgs))\n",
    "    print(f\"Added path '{venv_site_pkgs}' at start of search paths.\")\n",
    "else:\n",
    "    print(f\"Path '{venv_site_pkgs}' not found. Check that it exists and/or that it exists for node-type '{node_type}'.\")\n",
    "\n",
    "!pwd\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f229a9ea-a77b-45ea-b5db-da34e291ccf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "from FastEHR.dataloader.foundational_loader import FoundationalDataModule\n",
    "\n",
    "import logging\n",
    "import time\n",
    "\n",
    "from CPRD.examples.data.study_criteria import multimorbidity_inclusion_method\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = \"cpu\"    # if more informative debugging statements are needed\n",
    "print(f\"Using device: {device}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff00f74b-f1a5-43d3-8e28-6c302d43b88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_decoder: true\n",
      "data:\n",
      "  batch_size: 64\n",
      "  unk_freq_threshold: 0.0\n",
      "  min_workers: 12\n",
      "  global_diagnoses: false\n",
      "  repeating_events: true\n",
      "  path_to_db: /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/cprd.db\n",
      "  path_to_ds: /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_MultiMorbidity/\n",
      "  meta_information_path: /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/meta_information_QuantJenny.pickle\n",
      "  subsample_training: null\n",
      "experiment:\n",
      "  type: pre-train\n",
      "  project_name: SurvivEHR\n",
      "  run_id: ${head.SurvLayer}PreTrain_small_${experiment.seed}\n",
      "  fine_tune_id: null\n",
      "  notes: null\n",
      "  tags: null\n",
      "  train: true\n",
      "  test: true\n",
      "  verbose: true\n",
      "  seed: 1337\n",
      "  log: true\n",
      "  log_dir: /rds/projects/s/subramaa-mum-predict/CharlesGadd_Oxford/FoundationModelOutput/\n",
      "  ckpt_dir: /rds/projects/s/subramaa-mum-predict/CharlesGadd_Oxford/FoundationModelOutput/checkpoints/\n",
      "  fine_tune_outcomes: null\n",
      "optim:\n",
      "  num_epochs: 1\n",
      "  learning_rate: 0.0003\n",
      "  scheduler_warmup: true\n",
      "  scheduler: decaycawarmrestarts\n",
      "  scheduler_periods: 10000\n",
      "  learning_rate_decay: 0.8\n",
      "  val_check_interval: 2500\n",
      "  early_stop: true\n",
      "  early_stop_patience: 30\n",
      "  log_every_n_steps: 20\n",
      "  limit_val_batches: 0.025\n",
      "  limit_test_batches: null\n",
      "  accumulate_grad_batches: 1\n",
      "transformer:\n",
      "  block_type: Neo\n",
      "  block_size: 256\n",
      "  n_layer: 6\n",
      "  n_head: 6\n",
      "  n_embd: 384\n",
      "  layer_norm_bias: false\n",
      "  attention_type: global\n",
      "  bias: true\n",
      "  dropout: 0.0\n",
      "  attention_dropout: 0.0\n",
      "  resid_dropout: 0.0\n",
      "  private_heads: 0\n",
      "  use_fine_tune_adapter: true\n",
      "  adapter_dim: 8\n",
      "head:\n",
      "  SurvLayer: cr\n",
      "  surv_weight: 1\n",
      "  tokens_for_univariate_regression: None\n",
      "  value_weight: 0.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load the configuration file, override any settings \n",
    "with initialize(version_base=None, config_path=\"../../../modelling/SurvivEHR/confs\", job_name=\"dataset_creation_notebook\"):\n",
    "    cfg = compose(config_name=\"config_CompetingRisk11M\", overrides=[])\n",
    "\n",
    "# Create new dataset \n",
    "cfg.data.path_to_ds = \"/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_MultiMorbidity/\"\n",
    "print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162323d0-9e9e-420e-80f4-394cc7979925",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Creating unsupervised collator for DataModule\n",
      "INFO:root:Building Polars datasets and saving to /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_MultiMorbidity/\n",
      "INFO:root:Using train/test/val splits from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/practice_id_splits.pickle\n",
      "INFO:root:Processing test split...\n",
      "Thread generating parquet for 74 practices: 100%|██████████| 74/74 [1:42:01<00:00, 82.72s/it]   \n",
      "INFO:root:Created dataset at /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_MultiMorbidity/split=test with 1988 number of samples\n",
      "Getting file row counts. This allows the creation of an index to file map, increasing read efficiency: 860it [00:13, 62.91it/s] \n",
      "INFO:root:\t Obtained with a total of 206187 samples\n",
      "INFO:root:Processing train split...\n",
      "Thread generating parquet for 1330 practices:  12%|█▏        | 159/1330 [2:26:45<15:53:04, 48.83s/it] "
     ]
    }
   ],
   "source": [
    "# Build \n",
    "dm = FoundationalDataModule(path_to_db=cfg.data.path_to_db,\n",
    "                            path_to_ds=cfg.data.path_to_ds,\n",
    "                            load=False,\n",
    "                            include_diagnoses=True,\n",
    "                            include_measurements=True,\n",
    "                            drop_missing_data=False,\n",
    "                            drop_empty_dynamic=True,\n",
    "                            tokenizer=\"tabular\",\n",
    "                            overwrite_practice_ids = \"/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/practice_id_splits.pickle\",\n",
    "                            overwrite_meta_information=cfg.data.meta_information_path,\n",
    "                            study_inclusion_method=multimorbidity_inclusion_method(),  # min_events=50\n",
    "                            num_threads=1   # Note: Most of the overhead is in the Polars processing, so unless you have lots of CPus this may not speed up dataset creation\n",
    "                           )\n",
    "\n",
    "vocab_size = dm.train_set.tokenizer.vocab_size\n",
    "\n",
    "print(f\"{len(dm.train_set)} training patients\")\n",
    "print(f\"{len(dm.val_set)} validation patients\")\n",
    "print(f\"{len(dm.test_set)} test patients\")\n",
    "print(f\"{vocab_size} vocab elements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d679de4-c337-4c3d-80d1-0c918331ecae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.train_set.view_sample(1, max_dynamic_events=None, report_time=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4f2824-a48d-4067-b03b-0212f4bd7676",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.meta_information[\"diagnosis_table\"].event.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4d2a6c-39e8-4f1e-bcb2-057fa0cabad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "pl.Config.set_tbl_rows(300)\n",
    "pl.Config.set_fmt_str_lengths(100)\n",
    "\n",
    "print(dm.train_set.tokenizer._event_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0ae8d6-7899-4f89-ab4a-ff753c1fa9ba",
   "metadata": {},
   "source": [
    "# Test data loading times (so we can optimise cpu usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0164d4-9829-451e-a169-55569f4aaa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "dataset1 = pq.ParquetDataset(path_to_db + \"polars/split=train/\", \n",
    "                            filters=[('PRACTICE_ID','=','p20763')]\n",
    "                            )\n",
    "\n",
    "import time\n",
    "\n",
    "start = time.time()   # starting time\n",
    "df  = dataset1.read().to_pandas()\n",
    "df = df[df[\"row_nr\"] == 100]\n",
    "print(df)\n",
    "print(time.time() - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdc9021-702c-4b5f-a77b-6a49eac7e535",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "times = []\n",
    "start = time.time()   # starting time\n",
    "for row_idx, row in enumerate(tqdm(dm.train_set)):\n",
    "    # print(f\"Sample loaded in {time.time()-start} seconds\")\n",
    "    times.append(time.time()-start)\n",
    "    start = time.time()\n",
    "    if row_idx > 1e4:\n",
    "        break\n",
    "print(np.mean(times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a9ee3b-bb5f-4272-94b9-40833d71384a",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "start = time.time()   # starting time\n",
    "for batch_idx, batch in enumerate(tqdm(dm.train_dataloader())):\n",
    "    # print(f\"batch loaded in {time.time()-start} seconds\")    \n",
    "    times.append(time.time()-start)\n",
    "    start = time.time()\n",
    "    if batch_idx > 1e4:\n",
    "        break\n",
    "print(np.mean(times))\n",
    "\n",
    "# for key in batch.keys():\n",
    "#     print(f\"{key}\".ljust(20) + f\"{batch[key].shape}\")\n",
    "\n",
    "# tokens = batch[\"tokens\"][0].tolist()    \n",
    "# sentence = dm.decode(tokens).split(\" \")\n",
    "# for token, value in zip(sentence, batch[\"values\"][0].tolist()):\n",
    "#     print(f\"{token}:\".ljust(40) + f\"{value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46573a7f-6131-4f27-9877-feb27cc60789",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.train_set.view_sample(1236, max_dynamic_events=12, report_time=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abb3df6-37e5-4e8e-9a4e-3b4fb709a586",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "sys_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
