{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJpXpmjEYC_T"
   },
   "source": [
    "## GPT - demo on subset of CPRD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sqlite3\n",
    "from dataclasses import dataclass\n",
    "import logging\n",
    "from CPRD.data.foundational_loader import FoundationalDataModule\n",
    "from CPRD.src.models.gpt_pico.transformer import GPTLanguageModel\n",
    "from CPRD.src.models.gpt_simple.task_heads import GPTModelForCausalLM\n",
    "\n",
    "# TODO:\n",
    "# replace boilerplate with pytorch lightning\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "# device = \"cpu\"    # just for debug errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set GPT config to be equivalent\n",
    "@dataclass\n",
    "class DemoConfig:\n",
    "    block_size: int = 256             # what is the maximum context length for predictions?\n",
    "    n_layer: int = 6\n",
    "    n_head: int = 6\n",
    "    n_embd: int = 384\n",
    "    pos_encoding: str = None          # Manually adding later\n",
    "    bias: bool = True\n",
    "    attention_type: str = \"global\"    \n",
    "    dropout: float = 0.0\n",
    "    unk_freq_threshold: float = 0.0\n",
    "    tabular = False\n",
    "\n",
    "config = DemoConfig()\n",
    "\n",
    "@dataclass\n",
    "class OptConfig:\n",
    "    batch_size: int = 64\n",
    "    eval_interval: int = 1\n",
    "    learning_rate: float = 3e-4\n",
    "    epochs: int = 10\n",
    "    \n",
    "opt = OptConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrate on a reduced cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CPRD.data.database import queries\n",
    "\n",
    "PATH_TO_DB = \"/rds/projects/s/subramaa-mum-predict/CharlesGadd_Oxford/FoundationModel/preprocessing/processed/cprd.db\"\n",
    "conn = sqlite3.connect(PATH_TO_DB)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Get a list of patients which fit a reduced set of criterion\n",
    "identifiers1 = queries.query_measurement([\"bmi\", \"diastolic_blood_pressure\"], cursor)        \n",
    "identifiers2 = queries.query_diagnosis([\"DEPRESSION\", \"TYPE1DM\", \"TYPE2DIABETES\"], cursor)    #  \"DEPRESSION\"  ,  \"ANXIETY\"\n",
    "all_identifiers = list(set(identifiers1).intersection(identifiers2))    # Turn smaller list into the set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using N=10000 random samples, from the available 117102\n"
     ]
    }
   ],
   "source": [
    "# Lets take only the first N for faster development\n",
    "N = np.min((len(all_identifiers), 10000))\n",
    "print(f\"Using N={N} random samples, from the available {len(all_identifiers)}\")\n",
    "\n",
    "identifiers = random.choices(all_identifiers, k=N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Building DL-friendly representation\n",
      "INFO:root:Dropping samples with no dynamic events\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8615 training samples\n",
      "479 validation samples\n",
      "479 test samples\n",
      "100 vocab elements\n"
     ]
    }
   ],
   "source": [
    "dm = FoundationalDataModule(identifiers=identifiers,\n",
    "                            tabular=config.tabular,\n",
    "                            batch_size=opt.batch_size,\n",
    "                            max_seq_length=config.block_size,\n",
    "                            unk_freq_threshold=config.unk_freq_threshold)\n",
    "vocab_size = dm.train_set.tokenizer.vocab_size\n",
    "\n",
    "print(f\"{len(dm.train_set)} training samples\")\n",
    "print(f\"{len(dm.val_set)} validation samples\")\n",
    "print(f\"{len(dm.test_set)} test samples\")\n",
    "print(f\"{vocab_size} vocab elements\")\n",
    "# print(dm.train_set.tokenizer._itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualise a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A single element of the dataset contains:\n",
      "  * identifier\n",
      "  * in_tokens\n",
      "  * in_ages\n",
      "  * in_values\n",
      "  * target_tokens\n",
      "  * target_ages\n",
      "  * target_values\n",
      "\n",
      "identifier: p20389_944530620389\n",
      "\n",
      "in_tokens: tensor([18, 14,  4,  3, 12,  4, 13,  9,  2, 12,  2, 13,  8, 11, 12,  2, 13,  7,\n",
      "         9, 12,  2, 26, 13,  8,  9, 12,  2, 13,  9,  9, 12])\n",
      "\n",
      "in_ages: tensor([10046, 11609, 11609, 11609, 11609, 11609, 11609, 11609, 11609, 11609,\n",
      "        11609, 11738, 11738, 11738, 11738, 11738, 11748, 11748, 11748, 11748,\n",
      "        11748, 11826, 12161, 12161, 12161, 12161, 12161, 12392, 12392, 12392,\n",
      "        12392])\n",
      "\n",
      "in_values: tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan])\n",
      "\n",
      "target_tokens: tensor([14,  4,  3, 12,  4, 13,  9,  2, 12,  2, 13,  8, 11, 12,  2, 13,  7,  9,\n",
      "        12,  2, 26, 13,  8,  9, 12,  2, 13,  9,  9, 12,  2])\n",
      "\n",
      "target_ages: tensor([11609, 11609, 11609, 11609, 11609, 11609, 11609, 11609, 11609, 11609,\n",
      "        11738, 11738, 11738, 11738, 11738, 11748, 11748, 11748, 11748, 11748,\n",
      "        11826, 12161, 12161, 12161, 12161, 12161, 12392, 12392, 12392, 12392,\n",
      "        12392])\n",
      "\n",
      "target_values: tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan])\n"
     ]
    }
   ],
   "source": [
    "# print(dm.train_set[0])\n",
    "print(\"A single element of the dataset contains:\\n  * \" + '\\n  * '.join(dm.train_set[0].keys()))\n",
    "\n",
    "for k, v in dm.train_set[0].items():\n",
    "    print(f\"\\n{k}: {v}\")\n",
    "    if k == \"tokens\":\n",
    "        print(f\"... decoding to `{dm.decode(v.tolist())}`\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualise a batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create models and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using Positional Embedding. This module uses the index position of an event within the block of events.\n",
      "INFO:root:Using Positional Encoding. This module uses the index position of an event within the block of events.\n",
      "INFO:root:Using Temporal Positional Encoding. This module uses the patient's age at an event within their time series.\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "\n",
    "# Baseline model to test my changes against\n",
    "#   Note: this benchmark model uses index position along the batch\n",
    "models.append(GPTLanguageModel(config, vocab_size).to(device))\n",
    "\n",
    "# My development model\n",
    "# Handle positional vs. temporal encoding/embedding\n",
    "# Cases: \n",
    "#     index-embedding:       use index position along the batch\n",
    "#     index-encoding:        use index position along the batch\n",
    "#     temporal-encoding:     use age along a patient's timeline\n",
    "pos_encodings = [\"index-embedding\", \"index-encoding\", \"temporal-encoding\"]\n",
    "for pe in pos_encodings:\n",
    "    config = DemoConfig()\n",
    "    config.pos_encoding = pe\n",
    "    models.append(GPTModelForCausalLM(config, vocab_size).to(device))\n",
    "\n",
    "m_names = [\"kaparthy benchmark\"] + pos_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_curves_train = [[] for _ in models]\n",
    "loss_curves_val = [[] for _ in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hoelkOrFY8bN",
    "outputId": "961304cd-e379-40d4-dd56-8de0b91d2861"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.815844 M parameters\n",
      "Epoch 0:\tTrain loss 1.19. Val loss 1.12\n",
      "Epoch 1:\tTrain loss 0.92. Val loss 0.83\n",
      "Epoch 2:\tTrain loss 0.71. Val loss 0.73\n",
      "Epoch 3:\tTrain loss 0.66. Val loss 0.69\n",
      "Epoch 4:\tTrain loss 0.63. Val loss 0.68\n",
      "Epoch 5:\tTrain loss 0.62. Val loss 0.67\n",
      "Epoch 6:\tTrain loss 0.61. Val loss 0.66\n",
      "Epoch 7:\tTrain loss 0.61. Val loss 0.66\n",
      "Epoch 8:\tTrain loss 0.60. Val loss 0.66\n",
      "Epoch 9:\tTrain loss 0.60. Val loss 0.65\n",
      "DEPRESSION diastolic_blood_pressure 7 0 . 0 bmi 2 0 . 5 diastolic_blood_pressure 6 0 . 0 POLYCYSTIC_OVARIAN_SYNDROME_PCOS diastolic_blood_pressure 7 0 . 0 ALLERGICRHINITISCONJ diastolic_blood_pressure 7 0 . 0 eosinophil_count 0 .\n",
      "10.777444 M parameters\n",
      "Epoch 0:\tTrain loss 1.18. Val loss 0.91\n",
      "Epoch 1:\tTrain loss 0.74. Val loss 0.73\n",
      "Epoch 2:\tTrain loss 0.66. Val loss 0.69\n",
      "Epoch 3:\tTrain loss 0.63. Val loss 0.67\n",
      "Epoch 4:\tTrain loss 0.61. Val loss 0.66\n",
      "Epoch 5:\tTrain loss 0.61. Val loss 0.66\n",
      "Epoch 6:\tTrain loss 0.60. Val loss 0.65\n",
      "Epoch 7:\tTrain loss 0.60. Val loss 0.65\n",
      "Epoch 8:\tTrain loss 0.59. Val loss 0.65\n",
      "Epoch 9:\tTrain loss 0.59. Val loss 0.65\n",
      "DEPRESSION ANXIETY bmi 2 0 . 9 diastolic_blood_pressure 8 0 . 0 diastolic_blood_pressure 7 0 . 0 eosinophil_count 0 . 2 bmi 2 5 . 7 2 diastolic_blood_pressure 8 0 .\n",
      "10.67914 M parameters\n",
      "Epoch 0:\tTrain loss 1.57. Val loss 1.27\n",
      "Epoch 1:\tTrain loss 1.07. Val loss 0.99\n",
      "Epoch 2:\tTrain loss 0.83. Val loss 0.82\n",
      "Epoch 3:\tTrain loss 0.75. Val loss 0.79\n",
      "Epoch 4:\tTrain loss 0.71. Val loss 0.75\n",
      "Epoch 5:\tTrain loss 0.70. Val loss 0.75\n",
      "Epoch 6:\tTrain loss 0.69. Val loss 0.73\n",
      "Epoch 7:\tTrain loss 0.67. Val loss 0.72\n",
      "Epoch 8:\tTrain loss 0.67. Val loss 0.71\n",
      "Epoch 9:\tTrain loss 0.66. Val loss 0.71\n",
      "DEPRESSION SUBSTANCEMISUSE ASTHMA_PUSHASTHMA bmi 3 0 . 0 diastolic_blood_pressure 9 1 . 0 DEPRESSION basophil_count 0 . 0 3 eosinophil_count 0 . 1 4 ANXIETY diastolic_blood_pressure 6 8 . 0 DEPRESSION\n",
      "10.679332 M parameters\n",
      "Epoch 0:\tTrain loss 1.60. Val loss 1.64\n",
      "Epoch 1:\tTrain loss 1.46. Val loss 1.32\n",
      "Epoch 2:\tTrain loss 0.99. Val loss 0.87\n",
      "Epoch 3:\tTrain loss 0.75. Val loss 0.77\n",
      "Epoch 4:\tTrain loss 0.69. Val loss 0.73\n",
      "Epoch 5:\tTrain loss 0.67. Val loss 0.72\n",
      "Epoch 6:\tTrain loss 0.65. Val loss 0.71\n",
      "Epoch 7:\tTrain loss 0.64. Val loss 0.70\n",
      "Epoch 8:\tTrain loss 0.63. Val loss 0.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Using positional temporal-encoding requires ages, but this head has no way of sampling age at next event. Using 50 days as intervals\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:\tTrain loss 0.62. Val loss 0.69\n",
      "DEPRESSION diastolic_blood_pressure 8 blood_urea . 0 basophil_count 0 eosinophil_count 0 PSORIASIS diastolic_blood_pressure 7 POLYCYSTIC_OVARIAN_SYNDROME_PCOS bmi 1 . 0 basophil_count 0 corrected_serum_calcium_level 2 bmi 1 basophil_count 0 diastolic_blood_pressure 8 diastolic_blood_pressure 7 DEPRESSION\n"
     ]
    }
   ],
   "source": [
    "for m_idx, model in enumerate(models):\n",
    "    model = model.to(device)\n",
    "\n",
    "    # print the number of parameters in the model\n",
    "    print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
    "\n",
    "    # create a PyTorch optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=opt.learning_rate)\n",
    "\n",
    "    best_val, best_iter = np.inf, 0\n",
    "    for epoch in range(opt.epochs):\n",
    "        epoch_loss = 0\n",
    "        model.train()\n",
    "        for i, batch in enumerate(dm.train_dataloader()):\n",
    "            # evaluate the loss\n",
    "            logits, loss = model(batch['tokens'].to(device),\n",
    "                                 ages=batch['ages'].to(device),\n",
    "                                 targets=batch['target_tokens'].to(device),\n",
    "                                 attention_mask=batch['attention_mask'].to(device)\n",
    "                                 )\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        epoch_loss /= i\n",
    "        loss_curves_train[m_idx].append(epoch_loss)\n",
    "\n",
    "        # evaluate the loss on val set\n",
    "        with torch.no_grad(): \n",
    "            model.eval()\n",
    "            if epoch % opt.eval_interval == 0 or epoch == opt.epochs - 1:\n",
    "                val_loss = 0\n",
    "                for j, batch in enumerate(dm.val_dataloader()):\n",
    "                    _, loss = model(batch['tokens'].to(device), \n",
    "                                    ages=batch['ages'].to(device), \n",
    "                                    targets=batch['target_tokens'].to(device),\n",
    "                                    attention_mask=batch['attention_mask'].to(device)   \n",
    "                                   )\n",
    "                    val_loss += loss.item()\n",
    "                val_loss /= j\n",
    "                loss_curves_val[m_idx].append(val_loss)\n",
    "                print(f\"Epoch {epoch}:\\tTrain loss {epoch_loss:.2f}. Val loss {val_loss:.2f}\")          \n",
    "                # TODO: Note not fully accurate as last batch is likely not the same size, will be fixed with lightning\n",
    "        \n",
    "            if val_loss >= best_val:\n",
    "                best_iter += 1\n",
    "                if best_iter > 2:\n",
    "                    break\n",
    "            else:\n",
    "                best_val = val_loss\n",
    "                best_iter = 0\n",
    "                \n",
    "    prompt = [\"DEPRESSION\"]\n",
    "    context = torch.from_numpy(np.array(dm.encode(prompt)).reshape((1,-1))).to(device)\n",
    "    fut_tokens, fut_ages = model.generate(context, max_new_tokens=30)\n",
    "    fut_words = dm.decode(fut_tokens[0].tolist())\n",
    "    print(fut_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"k\", \"r\", \"b\", \"y\"]\n",
    "\n",
    "# Plot loss\n",
    "plt.figure()\n",
    "for m_idx, _ in enumerate(models):\n",
    "    # Training\n",
    "    iterations = np.linspace(0, len(loss_curves_train[m_idx]), len(loss_curves_train[m_idx])) * opt.eval_interval\n",
    "    plt.plot(iterations, loss_curves_train[m_idx], label=f\"{m_names[m_idx]}-train\", c=cols[m_idx], linestyle='dashed')\n",
    "    # Validation\n",
    "    iterations = np.linspace(0, len(loss_curves_val[m_idx]), len(loss_curves_val[m_idx])) * opt.eval_interval\n",
    "    plt.plot(iterations, loss_curves_val[m_idx], label=f\"{m_names[m_idx]}-val\", c=cols[m_idx])\n",
    "plt.legend()\n",
    "plt.savefig(\"figs/loss_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability of type II diabetes before and after a type I diagnosis\n",
    "\n",
    "keys: \n",
    "\n",
    "    70: 'TYPE1DM'\n",
    "    31: 'TYPE2DIABETES'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small context comparison, high bmi and blood pressure vs low for diabetes risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.tabular:\n",
    "    low_risk_prompt = [\"bmi\", \"diastolic_blood_pressure\"]\n",
    "    high_risk_prompt = [\"bmi\", \"diastolic_blood_pressure\"]\n",
    "    ages_in_years = [19, 20]\n",
    "else:\n",
    "    low_risk_prompt = [\"bmi\", \"2\", \"2\", \".\", \"5\", \"diastolic_blood_pressure\", \"7\", \"9\", \".\", \"0\"]\n",
    "    high_risk_prompt = [\"bmi\", \"3\", \"7\", \".\", \"5\", \"diastolic_blood_pressure\", \"9\", \"9\", \".\", \"0\"]\n",
    "    ages_in_years = [19, 19, 19, 19, 20, 20, 20, 20, 20]\n",
    "\n",
    "to_days = lambda a_list: torch.FloatTensor([365 * _a for _a in a_list]).reshape((1,-1)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "MODEL_IDX 0\n",
      "==================\n",
      "\n",
      "Control: Low risk: \n",
      "\t (bmi,2,2,.,5,diastolic_blood_pressure,7,9,.,0): \n",
      "probability of type I diabetes 0.0002%\n",
      "probability of type II diabetes 0.0011%\n",
      "\n",
      "Control: High risk: \n",
      "\t (bmi,3,7,.,5,diastolic_blood_pressure,9,9,.,0): \n",
      "probability of type I diabetes 0.0002%\n",
      "probability of type II diabetes 0.0011%\n",
      "\n",
      "Control: Low risk + depression: \n",
      "\t (DEPRESSION,bmi,2,2,.,5,diastolic_blood_pressure,7,9,.,0): \n",
      "probability of type I diabetes 0.0418%\n",
      "probability of type II diabetes 1.8673%\n",
      "\n",
      "Low risk context: Type 1 diagnosis in prompt: \n",
      "\t (TYPE1DM,bmi,2,2,.,5,diastolic_blood_pressure,7,9,.,0): \n",
      "probability of type I diabetes 0.0550%\n",
      "probability of type II diabetes 1.4223%\n",
      "\n",
      "Low risk context: Type 1I diagnosis in prompt: \n",
      "\t (TYPE2DIABETES,bmi,2,2,.,5,diastolic_blood_pressure,7,9,.,0): \n",
      "probability of type I diabetes 0.0864%\n",
      "probability of type II diabetes 0.3458%\n",
      "\n",
      "\n",
      "MODEL_IDX 1\n",
      "==================\n",
      "\n",
      "Control: Low risk: \n",
      "\t (bmi,2,2,.,5,diastolic_blood_pressure,7,9,.,0): \n",
      "probability of type I diabetes 0.0005%\n",
      "probability of type II diabetes 0.0010%\n",
      "\n",
      "Control: High risk: \n",
      "\t (bmi,3,7,.,5,diastolic_blood_pressure,9,9,.,0): \n",
      "probability of type I diabetes 0.0005%\n",
      "probability of type II diabetes 0.0010%\n",
      "\n",
      "Control: Low risk + depression: \n",
      "\t (DEPRESSION,bmi,2,2,.,5,diastolic_blood_pressure,7,9,.,0): \n",
      "probability of type I diabetes 0.0536%\n",
      "probability of type II diabetes 1.0890%\n",
      "\n",
      "Low risk context: Type 1 diagnosis in prompt: \n",
      "\t (TYPE1DM,bmi,2,2,.,5,diastolic_blood_pressure,7,9,.,0): \n",
      "probability of type I diabetes 0.1671%\n",
      "probability of type II diabetes 0.7634%\n",
      "\n",
      "Low risk context: Type 1I diagnosis in prompt: \n",
      "\t (TYPE2DIABETES,bmi,2,2,.,5,diastolic_blood_pressure,7,9,.,0): \n",
      "probability of type I diabetes 0.1804%\n",
      "probability of type II diabetes 0.3325%\n",
      "\n",
      "\n",
      "MODEL_IDX 2\n",
      "==================\n",
      "\n",
      "Control: Low risk: \n",
      "\t (bmi,2,2,.,5,diastolic_blood_pressure,7,9,.,0): \n",
      "probability of type I diabetes 0.0012%\n",
      "probability of type II diabetes 0.0028%\n",
      "\n",
      "Control: High risk: \n",
      "\t (bmi,3,7,.,5,diastolic_blood_pressure,9,9,.,0): \n",
      "probability of type I diabetes 0.0012%\n",
      "probability of type II diabetes 0.0028%\n",
      "\n",
      "Control: Low risk + depression: \n",
      "\t (DEPRESSION,bmi,2,2,.,5,diastolic_blood_pressure,7,9,.,0): \n",
      "probability of type I diabetes 0.1108%\n",
      "probability of type II diabetes 1.0496%\n",
      "\n",
      "Low risk context: Type 1 diagnosis in prompt: \n",
      "\t (TYPE1DM,bmi,2,2,.,5,diastolic_blood_pressure,7,9,.,0): \n",
      "probability of type I diabetes 0.1077%\n",
      "probability of type II diabetes 1.3201%\n",
      "\n",
      "Low risk context: Type 1I diagnosis in prompt: \n",
      "\t (TYPE2DIABETES,bmi,2,2,.,5,diastolic_blood_pressure,7,9,.,0): \n",
      "probability of type I diabetes 0.1121%\n",
      "probability of type II diabetes 1.2850%\n",
      "\n",
      "\n",
      "MODEL_IDX 3\n",
      "==================\n",
      "\n",
      "Control: Low risk: \n",
      "\t (bmi,2,2,.,5,diastolic_blood_pressure,7,9,.,0): \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_1357894/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">649248822.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">29</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_1357894/649248822.py'</span>                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/rds/bear-apps/2022a/EL8-ice/software/PyTorch/1.12.1-foss-2022a-CUDA-11.7.0/lib/python3.10/site-</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1130</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1127 │   │   # this function, and just call forward.</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1128 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">o</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1129 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1130 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1131 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1132 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1133 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/rds/homes/g/gaddcz/Projects/CPRD/src/models/gpt_simple/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">task_heads.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">44</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 41 │   │   │   │   </span>attention_mask: Optional[torch.tensor] = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>,                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 42 │   │   │   │   </span>targets: Optional[torch.tensor] = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>):                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 43 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 44 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>x = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.transformer(tokens=tokens, ages=ages, attention_mask=attention_mask)      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 45 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 46 │   │   </span>logits = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.lm_head(x)                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 47 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/rds/bear-apps/2022a/EL8-ice/software/PyTorch/1.12.1-foss-2022a-CUDA-11.7.0/lib/python3.10/site-</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1130</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1127 │   │   # this function, and just call forward.</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1128 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">o</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1129 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1130 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1131 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1132 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1133 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/rds/homes/g/gaddcz/Projects/CPRD/src/models/gpt_simple/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">transformer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">129</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">126 │   │   # Get positional embeddings/encodings</span>                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">127 │   │   </span>pos_emb = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.wpe(tokens=tokens, ages=ages)       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># positional embeddings of sh</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">128 │   │   # Combine (broadcasts in some choices of encodings)</span>                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>129 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>x = tok_emb + pos_emb                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">130 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">131 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.drop <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">132 │   │   │   </span>x = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.drop(x)                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">RuntimeError: </span>The size of tensor a <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span><span style=\"font-weight: bold\">)</span> must match the size of tensor b <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span><span style=\"font-weight: bold\">)</span> at non-singleton dimension <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_1357894/\u001b[0m\u001b[1;33m649248822.py\u001b[0m:\u001b[94m29\u001b[0m in \u001b[92m<module>\u001b[0m                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_1357894/649248822.py'\u001b[0m                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/rds/bear-apps/2022a/EL8-ice/software/PyTorch/1.12.1-foss-2022a-CUDA-11.7.0/lib/python3.10/site-\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33mpackages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1130\u001b[0m in \u001b[92m_call_impl\u001b[0m                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1127 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# this function, and just call forward.\u001b[0m                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1128 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_pre_hooks \u001b[95mo\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1129 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1130 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*\u001b[96minput\u001b[0m, **kwargs)                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1131 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1132 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1133 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m _global_backward_hooks:                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/rds/homes/g/gaddcz/Projects/CPRD/src/models/gpt_simple/\u001b[0m\u001b[1;33mtask_heads.py\u001b[0m:\u001b[94m44\u001b[0m in \u001b[92mforward\u001b[0m              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 41 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mattention_mask: Optional[torch.tensor] = \u001b[94mNone\u001b[0m,                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 42 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mtargets: Optional[torch.tensor] = \u001b[94mNone\u001b[0m):                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 43 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 44 \u001b[2m│   │   \u001b[0mx = \u001b[96mself\u001b[0m.transformer(tokens=tokens, ages=ages, attention_mask=attention_mask)      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 45 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 46 \u001b[0m\u001b[2m│   │   \u001b[0mlogits = \u001b[96mself\u001b[0m.lm_head(x)                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 47 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/rds/bear-apps/2022a/EL8-ice/software/PyTorch/1.12.1-foss-2022a-CUDA-11.7.0/lib/python3.10/site-\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33mpackages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1130\u001b[0m in \u001b[92m_call_impl\u001b[0m                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1127 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# this function, and just call forward.\u001b[0m                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1128 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_pre_hooks \u001b[95mo\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1129 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1130 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*\u001b[96minput\u001b[0m, **kwargs)                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1131 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1132 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1133 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m _global_backward_hooks:                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/rds/homes/g/gaddcz/Projects/CPRD/src/models/gpt_simple/\u001b[0m\u001b[1;33mtransformer.py\u001b[0m:\u001b[94m129\u001b[0m in \u001b[92mforward\u001b[0m            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m126 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Get positional embeddings/encodings\u001b[0m                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m127 \u001b[0m\u001b[2m│   │   \u001b[0mpos_emb = \u001b[96mself\u001b[0m.wpe(tokens=tokens, ages=ages)       \u001b[2m# positional embeddings of sh\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m128 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Combine (broadcasts in some choices of encodings)\u001b[0m                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m129 \u001b[2m│   │   \u001b[0mx = tok_emb + pos_emb                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m130 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m131 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.drop \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m132 \u001b[0m\u001b[2m│   │   │   \u001b[0mx = \u001b[96mself\u001b[0m.drop(x)                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mRuntimeError: \u001b[0mThe size of tensor a \u001b[1m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1m)\u001b[0m must match the size of tensor b \u001b[1m(\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1m)\u001b[0m at non-singleton dimension \u001b[1;36m1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompts, ages, desc = [], [], []\n",
    "\n",
    "desc.append(\"Control: Low risk\")\n",
    "prompts.append(low_risk_prompt)\n",
    "ages.append(ages_in_years)\n",
    "\n",
    "desc.append(\"Control: High risk\")\n",
    "prompts.append(high_risk_prompt)\n",
    "ages.append(ages_in_years)\n",
    "\n",
    "desc.append(\"Control: Low risk + depression\")\n",
    "prompts.append([\"DEPRESSION\"] + low_risk_prompt)\n",
    "ages.append([17] + ages_in_years)\n",
    "\n",
    "desc.append(\"Low risk context: Type 1 diagnosis in prompt\")\n",
    "prompts.append([\"TYPE1DM\"] + low_risk_prompt)\n",
    "ages.append([17] + ages_in_years)\n",
    "\n",
    "desc.append(\"Low risk context: Type 1I diagnosis in prompt\")\n",
    "prompts.append([\"TYPE2DIABETES\"] + low_risk_prompt)\n",
    "ages.append([17] + ages_in_years)\n",
    "\n",
    "for model_idx in range(len(pos_encodings)+1):\n",
    "    print(f\"\\n\\nMODEL_IDX {model_idx}\\n==================\")\n",
    "    \n",
    "    for p_idx, (prompt, age) in enumerate(zip(prompts, ages)):\n",
    "        print(f\"\\n{desc[p_idx]}: \\n\\t ({','.join(prompt)}): \")\n",
    "        encoded_prompt = torch.from_numpy(np.array(dm.encode(prompt)).reshape((1,-1))).to(device)\n",
    "        lgts, _ = models[model_idx](encoded_prompt,\n",
    "                                    ages=to_days(age))\n",
    "        probs = torch.nn.functional.softmax(lgts, dim=2)\n",
    "        print(f\"probability of type I diabetes {100*float(probs[0, 0, 70].cpu().detach().numpy()):.4f}%\")\n",
    "        print(f\"probability of type II diabetes {100*float(probs[0, 0, 31].cpu().detach().numpy()):.4f}%\")\n",
    "\n",
    "# Note: adding a diagnosis (even if potentially orthogonal) at the beginning of the prompt increases probability of either type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "sys_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
