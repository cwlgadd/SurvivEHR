{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJpXpmjEYC_T"
   },
   "source": [
    "## CPRD GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "from dataclasses import dataclass\n",
    "import logging\n",
    "from CPRD.data.foundational_loader import FoundationalDataModule\n",
    "from CPRD.src.models.gpt_pico.transformer import GPTLanguageModel\n",
    "from CPRD.src.models.gpt_simple.task_heads import GPTModelForCausalLM\n",
    "\n",
    "# TODO:\n",
    "# mask padding tokens\n",
    "# replace boilerplate with pytorch lightning\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "# device = \"cpu\"    # just for debug errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set GPT config to be equivalent\n",
    "@dataclass\n",
    "class DemoConfig:\n",
    "    block_size: int = 256             # what is the maximum context length for predictions?\n",
    "    n_layer: int = 6\n",
    "    n_head: int = 6\n",
    "    n_embd: int = 384\n",
    "    pos_encoding: str = \"index-embedding\"                 # Manually adding later\n",
    "    bias: bool = True\n",
    "    attention_type: str = \"global\"    \n",
    "    dropout: float = 0.0\n",
    "    \n",
    "\n",
    "config = DemoConfig()\n",
    "\n",
    "# optim hyperparameters\n",
    "batch_size = 64\n",
    "eval_interval = 5\n",
    "learning_rate = 3e-4\n",
    "epochs = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrate on a reduced cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p20485_2547470620485', 'p20511_2351894320511', 'p20397_1925954020397', 'p20741_1101765420741', 'p20426_633147620426', 'p20491_964237520491', 'p20467_766385720467', 'p20409_622376720409', 'p20495_1865502120495', 'p20415_268077620415']\n",
      "16327\n"
     ]
    }
   ],
   "source": [
    "from CPRD.data.database import queries\n",
    "\n",
    "PATH_TO_DB = \"/rds/projects/s/subramaa-mum-predict/CharlesGadd_Oxford/FoundationModel/preprocessing/processed/cprd.db\"\n",
    "conn = sqlite3.connect(PATH_TO_DB)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# # Check what measurements are available\n",
    "# cursor.execute(\"SELECT DISTINCT * FROM measurement_table\")\n",
    "# measurements = cursor.fetchall()\n",
    "# print(measurements)\n",
    "\n",
    "# Check what diagnoses are available\n",
    "# cursor.execute(\"SELECT DISTINCT * FROM diagnosis_table\")\n",
    "# diagnoses = cursor.fetchall()\n",
    "# print(diagnoses)\n",
    "\n",
    "# Get a list of patients which fit a reduced set of criterion\n",
    "identifiers1 = queries.query_measurement([\"bmi\", \"hydroxyvitamin2\", \"hydroxyvitamin3\"], cursor)         #  \n",
    "identifiers2 = queries.query_diagnosis([ \"FIBROMYALGIA\", \"HF\"], cursor)    #  \"DEPRESSION\"  ,  \"ANXIETY\"\n",
    "identifiers = list(set(identifiers1).intersection(identifiers2))    # Turn smaller list into the set\n",
    "\n",
    "print(identifiers[:10])\n",
    "print(len(identifiers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building DL-friendly representation\n",
      "Dropping samples with no temporal events\n",
      "14694 training samples\n",
      "817 validation samples\n",
      "816 test samples\n"
     ]
    }
   ],
   "source": [
    "dm = FoundationalDataModule(identifiers=identifiers, batch_size=batch_size, max_seq_length=config.block_size, unk_freq_threshold=0)\n",
    "\n",
    "print(f\"{len(dm.train_set)} training samples\")\n",
    "print(f\"{len(dm.val_set)} validation samples\")\n",
    "print(f\"{len(dm.test_set)} test samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualise a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A sample from the dataloader batch gives:\n",
      "\n",
      "The position index of inputs and targets: \n",
      "inputs: tensor([266, 267, 268, 269, 270, 271, 272, 273, 274, 275])  \n",
      "targets: tensor([267, 268, 269, 270, 271, 272, 273, 274, 275, 276])\n",
      "\n",
      "The time of event (in days since birth) of event of inputs and targets: \n",
      "inputs: tensor([22496, 22496, 22582, 22582, 22582, 22582, 22582, 22666, 22666, 22666])  \n",
      "targets: tensor([22496, 22582, 22582, 22582, 22582, 22582, 22666, 22666, 22666, 22666])\n",
      "\n",
      "The shifted next-step, tokenized and padded (within batch), representation from a block of a patient's sequence for events: \n",
      "inputs: tensor([12,  2, 13,  8,  6, 12,  2, 13,  9,  2]) \n",
      "targets: tensor([ 2, 13,  8,  6, 12,  2, 13,  9,  2, 12])\n",
      "\n",
      "Which can be decoded. E.g. first sample's first 10 block tokens: \n",
      "inputs: . 0 diastolic_blood_pressure 6 4 . 0 diastolic_blood_pressure 7 0  \n",
      "targets: 0 diastolic_blood_pressure 6 4 . 0 diastolic_blood_pressure 7 0 .\n",
      "\n",
      "The attention mask (torch.Size([64, 256])) for padding: \n",
      "tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "for idx, batch in enumerate(dm.train_dataloader()):\n",
    "    break\n",
    "print(\"A sample from the dataloader batch gives:\")\n",
    "print(f\"\\nThe position index of inputs and targets: \\ninputs: {batch['input_pos'][0,:10]}  \\ntargets: {batch['target_pos'][0,:10]}\")\n",
    "print(f\"\\nThe time of event (in days since birth) of event of inputs and targets: \\ninputs: {batch['input_ages'][0,:10]}  \\ntargets: {batch['target_ages'][0,:10]}\")\n",
    "print(f\"\\nThe shifted next-step, tokenized and padded (within batch), representation from a block of a patient's sequence for events: \\ninputs: {batch['input_ids'][0,:10]} \\ntargets: {batch['target_ids'][0,:10]}\")\n",
    "print(f\"\\nWhich can be decoded. E.g. first sample's first 10 block tokens: \\ninputs: {dm.decode(batch['input_ids'][0,:10].tolist())}  \\ntargets: {dm.decode(batch['target_ids'][0,:10].tolist())}\")\n",
    "print(f\"\\nThe attention mask ({batch['attention_mask'].shape}) for padding: \\n{batch['attention_mask']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n",
      "{0: 'PAD', 1: 'UNK', 2: '0', 3: '1', 4: '2', 5: '3', 6: '4', 7: '5', 8: '6', 9: '7', 10: '8', 11: '9', 12: '.', 13: 'diastolic_blood_pressure', 14: 'eosinophil_count', 15: 'bmi', 16: 'basophil_count', 17: 'corrected_serum_calcium_level', 18: 'serum_level', 19: 'calculated_LDL_cholesterol_level', 20: 'HF', 21: 'HYPERTENSION', 22: 'OSTEOARTHRITIS', 23: 'IHD_NOMI', 24: 'aspartate_transam', 25: 'DEPRESSION', 26: 'AF', 27: 'CKDSTAGE3TO5', 28: 'ASTHMA_PUSHASTHMA', 29: 'ANY_DEAFNESS_HEARING_LOSS', 30: 'ANXIETY', 31: 'TYPE2DIABETES', 32: 'blood_urea', 33: 'ATOPICECZEMA', 34: 'MINFARCTION', 35: 'FIBROMYALGIA', 36: 'ALLCA_NOBCC_VFINAL', 37: 'COPD', 38: 'calcium_adjusted_level', 39: 'VALVULARDISEASES', 40: 'ALLERGICRHINITISCONJ', 41: 'GOUT', 42: 'PERIPHERAL_NEUROPATHY', 43: 'HYPOTHYROIDISM_DRAFT_V1', 44: 'OSTEOPOROSIS', 45: 'PREVALENT_IBS', 46: 'combined_total_vitamin_D2_and_D3_level', 47: 'STROKEUNSPECIFIED', 48: 'PAD_STRICT', 49: 'ALL_DEMENTIA', 50: 'ALCOHOLMISUSE', 51: 'PSORIASIS', 52: 'PMRANDGCA', 53: 'hydroxyvitamin3', 54: 'hydroxyvitamin2', 55: 'OSA', 56: 'RHEUMATOIDARTHRITIS', 57: 'ISCHAEMICSTROKE', 58: 'OTHER_CHRONIC_LIVER_DISEASE_OPTIMAL', 59: 'AORTICANEURYSM', 60: 'HYPERTHYROIDISM', 61: 'VISUAL_IMPAIRMENT', 62: 'BRONCHIECTASIS', 63: 'brain_natriuretic_peptide_level', 64: 'SUBSTANCEMISUSE', 65: 'creatinine_ratio', 66: 'EPILEPSY', 67: 'NAFLD', 68: 'ILD_SH', 69: 'CHRONICFATIGUESYNDROMEMM', 70: 'TYPE1DM', 71: 'ENDOMETRIOSIS_ADENOMYOSIS_V2', 72: 'PARKINSONS', 73: 'PERNICIOUSANAEMIA', 74: 'STROKE_HAEMRGIC', 75: 'POLYCYSTIC_OVARIAN_SYNDROME_PCOS', 76: 'MENIERESDISEASE', 77: 'LYMPHOMA_PREVALENCE', 78: 'ULCERATIVE_COLITIS', 79: 'EATINGDISORDERS', 80: 'PTSDDIAGNOSIS', 81: 'BIPOLAR', 82: 'CHRONIC_LIVER_DISEASE_ALCOHOL', 83: 'LEUKAEMIA_PREVALENCE', 84: 'SCHIZOPHRENIAMM', 85: 'PSORIATICARTHRITIS2021', 86: 'SYSTEMIC_LUPUS_ERYTHEMATOSUS', 87: 'CROHNS_DISEASE', 88: 'SJOGRENSSYNDROME', 89: 'LEARNINGDISABILITY', 90: 'PLASMACELL_NEOPLASM', 91: 'HAEMOCHROMATOSIS', 92: 'AUTISM', 93: 'MS', 94: 'ADDISON_DISEASE', 95: 'SYSTEMIC_SCLEROSIS', 96: 'blood_calcium', 97: 'HIVAIDS', 98: 'DOWNSSYNDROME', 99: 'CYSTICFIBROSIS', 100: 'SICKLE_CELL_DISEASE'}\n"
     ]
    }
   ],
   "source": [
    "vocab_size = dm.train_set.tokenizer.vocab_size\n",
    "\n",
    "print(vocab_size)\n",
    "print(dm.train_set.tokenizer._itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "\n",
    "# Baseline model to test my changes against\n",
    "models.append(GPTLanguageModel(config, vocab_size).to(device))\n",
    "\n",
    "# My development model\n",
    "pos_encodings = [\"index-embedding\", \"index-encoding\", \"temporal-encoding\"]\n",
    "for pe in pos_encodings:\n",
    "    config = DemoConfig()\n",
    "    config.pos_encoding = pe\n",
    "    models.append(GPTModelForCausalLM(config, vocab_size).to(device))\n",
    "\n",
    "m_names = [\"kaparthy benchmark\"] + pos_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_curves_train = [[] for _ in models]\n",
    "loss_curves_val = [[] for _ in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hoelkOrFY8bN",
    "outputId": "961304cd-e379-40d4-dd56-8de0b91d2861"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.816613 M parameters\n",
      "Epoch 0:\tTrain loss 1.50. Val loss 1.22\n",
      "Epoch 5:\tTrain loss 0.85. Val loss 0.92\n",
      "Epoch 10:\tTrain loss 0.84. Val loss 0.91\n",
      "Epoch 14:\tTrain loss 0.83. Val loss 0.91\n",
      "DEPRESSION bmi 2 1 . 1 diastolic_blood_pressure 8 0 . 0 bmi 2 1 . 3 diastolic_blood_pressure 8 4 . 0 diastolic_blood_pressure 8 5 . 0 basophil_count 0 . 1 eosinophil_count\n",
      "10.777829 M parameters\n",
      "Epoch 0:\tTrain loss 1.35. Val loss 1.06\n",
      "Epoch 5:\tTrain loss 0.84. Val loss 0.91\n",
      "Epoch 10:\tTrain loss 0.82. Val loss 0.90\n",
      "Epoch 14:\tTrain loss 0.82. Val loss 0.90\n",
      "DEPRESSION PSORIASIS ALLERGICRHINITISCONJ HF diastolic_blood_pressure 7 9 . 0 diastolic_blood_pressure 7 3 . 0 diastolic_blood_pressure 7 8 . 0 TYPE2DIABETES bmi 2 9 . 7 basophil_count 0 . 0 2 diastolic_blood_pressure\n",
      "10.679525 M parameters\n",
      "Epoch 0:\tTrain loss 1.95. Val loss 1.56\n",
      "Epoch 5:\tTrain loss 0.94. Val loss 1.01\n",
      "Epoch 10:\tTrain loss 0.98. Val loss 1.06\n",
      "Epoch 14:\tTrain loss 0.93. Val loss 1.01\n",
      "DEPRESSION IHD_NOMI HYPERTENSION bmi 3 5 . 0 diastolic_blood_pressure 7 1 . 0 diastolic_blood_pressure 7 0 . 0 diastolic_blood_pressure 7 0 . 0 bmi 2 6 . 8 diastolic_blood_pressure 6 0\n",
      "10.679717 M parameters\n",
      "Epoch 0:\tTrain loss 2.12. Val loss 1.77\n",
      "Epoch 5:\tTrain loss 0.89. Val loss 0.96\n",
      "Epoch 10:\tTrain loss 0.84. Val loss 0.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Using positional temporal-encoding requires ages, \n",
      "                                but this head has no way of sampling age at next event.\n",
      "                                Using 50 days as intervals\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14:\tTrain loss 0.81. Val loss 0.93\n",
      "DEPRESSION DEPRESSION bmi 1 diastolic_blood_pressure 6 basophil_count 0 bmi 2 eosinophil_count 0 basophil_count 0 bmi 1 diastolic_blood_pressure 7 basophil_count 0 bmi 2 eosinophil_count 0 basophil_count 0 diastolic_blood_pressure 7 diastolic_blood_pressure 7 .\n"
     ]
    }
   ],
   "source": [
    "for m_idx, model in enumerate(models):\n",
    "    model = model.to(device)\n",
    "\n",
    "    # print the number of parameters in the model\n",
    "    print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
    "\n",
    "    # create a PyTorch optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        model.train()\n",
    "        for i, batch in enumerate(dm.train_dataloader()):\n",
    "            # evaluate the loss\n",
    "            logits, loss = model(batch['input_ids'].to(device),\n",
    "                                 positions=batch['input_pos'].to(device),\n",
    "                                 ages=batch['input_ages'].to(device),\n",
    "                                 targets=batch['target_ids'].to(device),\n",
    "                                 attention_mask=batch['attention_mask'].to(device)\n",
    "                                 )\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        epoch_loss /= i\n",
    "        loss_curves_train[m_idx].append(epoch_loss)\n",
    "\n",
    "        # every once in a while evaluate the loss on val set\n",
    "        with torch.no_grad(): \n",
    "            model.eval()\n",
    "            if epoch % eval_interval == 0 or epoch == epochs - 1:\n",
    "                val_loss = 0\n",
    "                for j, batch in enumerate(dm.val_dataloader()):\n",
    "                    _, loss = model(batch['input_ids'].to(device), \n",
    "                                    positions=batch['input_pos'].to(device),\n",
    "                                    ages=batch['input_ages'].to(device), \n",
    "                                    targets=batch['target_ids'].to(device),\n",
    "                                    attention_mask=batch['attention_mask'].to(device)   \n",
    "                                   )\n",
    "                    val_loss += loss.item()\n",
    "                val_loss /= j\n",
    "                loss_curves_val[m_idx].append(val_loss)\n",
    "                print(f\"Epoch {epoch}:\\tTrain loss {epoch_loss:.2f}. Val loss {val_loss:.2f}\")          \n",
    "                # TODO: Note not fully accurate as last batch is likely not the same size, will be fixed with lightning\n",
    "                \n",
    "    prompt = [\"DEPRESSION\"]\n",
    "    context = torch.from_numpy(np.array(dm.encode(prompt)).reshape((1,-1))).to(device)\n",
    "    fut_tokens, fut_positions, fut_ages = model.generate(context, max_new_tokens=30)\n",
    "    fut_words = dm.decode(fut_tokens[0].tolist())\n",
    "    print(fut_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.figure()\n",
    "cols = [\"k\", \"r\", \"b\", \"y\"]\n",
    "for m_idx, _ in enumerate(models):\n",
    "    plt.plot(np.linspace(0,epochs,len(loss_curves_train[m_idx])), loss_curves_train[m_idx], label=f\"{m_names[m_idx]}-train\", c=cols[m_idx], linestyle='dashed')\n",
    "    plt.plot(np.linspace(0,epochs,len(loss_curves_val[m_idx])), loss_curves_val[m_idx], label=f\"{m_names[m_idx]}-val\", c=cols[m_idx])\n",
    "\n",
    "    # plt.plot(np.arange(v.shape[0]), v, label=f\"{m_idx}-val\", c=cols[m_idx])\n",
    "plt.legend()\n",
    "plt.savefig(\"figs/loss_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability of type II diabetes before and after a type I diagnosis\n",
    "\n",
    "keys: \n",
    "\n",
    "    70: 'TYPE1DM'\n",
    "    31: 'TYPE2DIABETES'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small context comparison, high bmi and blood pressure vs low for diabetes risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low risk measurements\n",
      "=========\n",
      "probability of type I diabetes 0.128%\n",
      "probability of type II diabetes 1.372%\n",
      "\n",
      "High risk measurements\n",
      "=========\n",
      "probability of type I diabetes 0.128%\n",
      "probability of type II diabetes 1.372%\n"
     ]
    }
   ],
   "source": [
    "model_idx= 2\n",
    "\n",
    "# Low risk context\n",
    "print(f\"Low risk measurements\\n=========\")\n",
    "prompt = [\"DEPRESSION\", \"bmi\", \"2\", \"2\", \".\", \"5\", \"diastolic_blood_pressure\", \"1\", \"0\", \"1\", \".\", \"0\"] * 1\n",
    "indices = torch.FloatTensor([i for i, _ in enumerate(prompt)]).reshape((1,-1))\n",
    "encoded_prompt = torch.from_numpy(np.array(dm.encode(prompt)).reshape((1,-1))).to(device)\n",
    "lgts, _ = models[model_idx](encoded_prompt, positions=indices)\n",
    "probs = torch.nn.functional.softmax(lgts, dim=2)\n",
    "print(f\"probability of type I diabetes {100*float(probs[0, 0, 70].cpu().detach().numpy()):.3f}%\")\n",
    "print(f\"probability of type II diabetes {100*float(probs[0, 0, 31].cpu().detach().numpy()):.3f}%\")\n",
    "\n",
    "\n",
    "# High risk context\n",
    "print(f\"\\nHigh risk measurements\\n=========\")\n",
    "prompt = [\"DEPRESSION\", \"bmi\", \"4\", \"3\", \".\", \"5\", \"diastolic_blood_pressure\", \"7\", \"0\", \".\", \"0\"] * 1\n",
    "indices = torch.FloatTensor([i for i, _ in enumerate(prompt)]).reshape((1,-1))\n",
    "encoded_prompt = torch.from_numpy(np.array(dm.encode(prompt)).reshape((1,-1))).to(device)\n",
    "lgts, _ = models[model_idx](encoded_prompt, positions=indices)\n",
    "probs = torch.nn.functional.softmax(lgts, dim=2)\n",
    "print(f\"probability of type I diabetes {100*float(probs[0, 0, 70].cpu().detach().numpy()):.3f}%\")\n",
    "print(f\"probability of type II diabetes {100*float(probs[0, 0, 31].cpu().detach().numpy()):.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inter event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "MODEL_IDX 0\n",
      "==================\n",
      "Neither type diagnosed in prompt:\n",
      "probability of type I diabetes 0.0003%\n",
      "probability of type II diabetes 0.0007%\n",
      "\n",
      "Neither type diagnosed in prompt, but start with potentially orthogonal condition:\n",
      "probability of type I diabetes 0.0672%\n",
      "probability of type II diabetes 1.0084%\n",
      "\n",
      "Type 1 diagnosis in prompt:\n",
      "probability of type I diabetes 0.3328%\n",
      "probability of type II diabetes 4.9562%\n",
      "\n",
      "Type 2 diagnosis in prompt:\n",
      "probability of type I diabetes 1.1500%\n",
      "probability of type II diabetes 0.5140%\n",
      "\n",
      "\n",
      "MODEL_IDX 1\n",
      "==================\n",
      "Neither type diagnosed in prompt:\n",
      "probability of type I diabetes 0.0002%\n",
      "probability of type II diabetes 0.0002%\n",
      "\n",
      "Neither type diagnosed in prompt, but start with potentially orthogonal condition:\n",
      "probability of type I diabetes 0.0664%\n",
      "probability of type II diabetes 0.8696%\n",
      "\n",
      "Type 1 diagnosis in prompt:\n",
      "probability of type I diabetes 0.7543%\n",
      "probability of type II diabetes 6.7574%\n",
      "\n",
      "Type 2 diagnosis in prompt:\n",
      "probability of type I diabetes 0.6476%\n",
      "probability of type II diabetes 0.7081%\n",
      "\n",
      "\n",
      "MODEL_IDX 2\n",
      "==================\n",
      "Neither type diagnosed in prompt:\n",
      "probability of type I diabetes 0.0003%\n",
      "probability of type II diabetes 0.0023%\n",
      "\n",
      "Neither type diagnosed in prompt, but start with potentially orthogonal condition:\n",
      "probability of type I diabetes 0.1283%\n",
      "probability of type II diabetes 1.3718%\n",
      "\n",
      "Type 1 diagnosis in prompt:\n",
      "probability of type I diabetes 0.1117%\n",
      "probability of type II diabetes 1.0993%\n",
      "\n",
      "Type 2 diagnosis in prompt:\n",
      "probability of type I diabetes 0.1049%\n",
      "probability of type II diabetes 1.2373%\n",
      "\n",
      "\n",
      "MODEL_IDX 3\n",
      "==================\n",
      "Neither type diagnosed in prompt:\n",
      "probability of type I diabetes 0.0001%\n",
      "probability of type II diabetes 0.0001%\n",
      "\n",
      "Neither type diagnosed in prompt, but start with potentially orthogonal condition:\n",
      "probability of type I diabetes 0.1659%\n",
      "probability of type II diabetes 0.2340%\n",
      "\n",
      "Type 1 diagnosis in prompt:\n",
      "probability of type I diabetes 0.3520%\n",
      "probability of type II diabetes 0.5680%\n",
      "\n",
      "Type 2 diagnosis in prompt:\n",
      "probability of type I diabetes 0.2342%\n",
      "probability of type II diabetes 0.5464%\n"
     ]
    }
   ],
   "source": [
    "make_indices = lambda prompt_list: torch.FloatTensor([i for i, _ in enumerate(prompt_list)]).reshape((1,-1)).to(device)\n",
    "\n",
    "# delta_tokens = [pair[1] for pair in dm.train_set.tokenizer._itos.items() if pair[1].isupper()]  # The conditons we add delta time to\n",
    "# def make_ages(prompt_list, start_age=18, day_delta=100):\n",
    "#     ages = [start_age * 365]\n",
    "#     for idx, token in enumerate(prompt_list[1:]):\n",
    "#         print(token)\n",
    "#         if token in delta_tokens:\n",
    "#             ages.append(ages[-1]+day_delta)\n",
    "#         else:\n",
    "#             ages.append(ages[-1])\n",
    "#     return ages\n",
    "make_ages = lambda prompt_list: torch.FloatTensor([18*365 for _ in prompt_list]).reshape((1,-1)).to(device)\n",
    "\n",
    "\n",
    "for model_idx in range(4):\n",
    "    print(f\"\\n\\nMODEL_IDX {model_idx}\\n==================\")\n",
    "    \n",
    "    # Control\n",
    "    print(f\"Neither type diagnosed in prompt:\")\n",
    "    prompt = [\"bmi\", \"2\", \"2\", \".\", \"5\", \"diastolic_blood_pressure\", \"1\", \"0\", \"1\", \".\", \"0\"] * 1\n",
    "    encoded_prompt = torch.from_numpy(np.array(dm.encode(prompt)).reshape((1,-1))).to(device)\n",
    "    lgts, _ = models[model_idx](encoded_prompt,\n",
    "                                positions=make_indices(prompt),\n",
    "                                ages=make_ages(prompt))\n",
    "    probs = torch.nn.functional.softmax(lgts, dim=2)\n",
    "    print(f\"probability of type I diabetes {100*float(probs[0, 0, 70].cpu().detach().numpy()):.4f}%\")\n",
    "    print(f\"probability of type II diabetes {100*float(probs[0, 0, 31].cpu().detach().numpy()):.4f}%\")\n",
    "\n",
    "    # Control 2\n",
    "    print(f\"\\nNeither type diagnosed in prompt, but start with potentially orthogonal condition:\")\n",
    "    prompt = [\"DEPRESSION\", \"bmi\", \"2\", \"2\", \".\", \"5\", \"diastolic_blood_pressure\", \"1\", \"0\", \"1\", \".\", \"0\"] * 1\n",
    "    encoded_prompt = torch.from_numpy(np.array(dm.encode(prompt)).reshape((1,-1))).to(device)\n",
    "    lgts, _ = models[model_idx](encoded_prompt,\n",
    "                                positions=make_indices(prompt),\n",
    "                                ages=make_ages(prompt))\n",
    "    probs = torch.nn.functional.softmax(lgts, dim=2)\n",
    "    print(f\"probability of type I diabetes {100*float(probs[0, 0, 70].cpu().detach().numpy()):.4f}%\")\n",
    "    print(f\"probability of type II diabetes {100*float(probs[0, 0, 31].cpu().detach().numpy()):.4f}%\")\n",
    "\n",
    "\n",
    "    # Low risk context\n",
    "    print(f\"\\nType 1 diagnosis in prompt:\")\n",
    "    prompt = [\"TYPE1DM\", \"bmi\", \"2\", \"2\", \".\", \"5\", \"diastolic_blood_pressure\", \"1\", \"0\", \"1\", \".\", \"0\"] * 1\n",
    "    encoded_prompt = torch.from_numpy(np.array(dm.encode(prompt)).reshape((1,-1))).to(device)\n",
    "    lgts, _ = models[model_idx](encoded_prompt,\n",
    "                                positions=make_indices(prompt),\n",
    "                                ages=make_ages(prompt))\n",
    "    probs = torch.nn.functional.softmax(lgts, dim=2)\n",
    "    print(f\"probability of type I diabetes {100*float(probs[0, 0, 70].cpu().detach().numpy()):.4f}%\")\n",
    "    print(f\"probability of type II diabetes {100*float(probs[0, 0, 31].cpu().detach().numpy()):.4f}%\")\n",
    "\n",
    "\n",
    "    # High risk context\n",
    "    print(f\"\\nType 2 diagnosis in prompt:\")\n",
    "    prompt = [\"TYPE2DIABETES\", \"bmi\", \"4\", \"3\", \".\", \"5\", \"diastolic_blood_pressure\", \"7\", \"0\", \".\", \"0\"] * 1\n",
    "    encoded_prompt = torch.from_numpy(np.array(dm.encode(prompt)).reshape((1,-1))).to(device)\n",
    "    lgts, _ = models[model_idx](encoded_prompt,\n",
    "                                positions=make_indices(prompt),\n",
    "                                ages=make_ages(prompt))\n",
    "    probs = torch.nn.functional.softmax(lgts, dim=2)\n",
    "    print(f\"probability of type I diabetes {100*float(probs[0, 0, 70].cpu().detach().numpy()):.4f}%\")\n",
    "    print(f\"probability of type II diabetes {100*float(probs[0, 0, 31].cpu().detach().numpy()):.4f}%\")\n",
    "\n",
    "    # Note: adding depression at the beginning of the promp increases probability from 0.0002% across the board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"bmi 1 8 . 6 bmi 3 0 . 6\"\n",
    "# context = torch.from_numpy(np.array(dm.encode(prompt)).reshape((1,-1)))\n",
    "# print(dm.decode(models[0].generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fjjvMifYZf7x"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "sys_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
