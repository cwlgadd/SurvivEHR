{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJpXpmjEYC_T"
   },
   "source": [
    "## Building a GPT\n",
    "\n",
    "Companion notebook to the [Zero To Hero](https://karpathy.ai/zero-to-hero.html) video on GPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h5hjCcLDr2WC",
    "outputId": "ccc60f0c-fd78-4dbe-8598-0512d1036aad"
   },
   "outputs": [],
   "source": [
    "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
    "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "O6medjfRsLD9"
   },
   "outputs": [],
   "source": [
    "# read it in to inspect it\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6xWI_VyAsN8F",
    "outputId": "ed819dd0-72e5-40a6-d2ed-928ff73bfda6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2c5V0FvqseE0",
    "outputId": "25ca7adc-b8c0-42d1-b08c-e0863c5c314e"
   },
   "outputs": [],
   "source": [
    "# let's look at the first 1000 characters\n",
    "# print()\n",
    "# for i in text[:20]:\n",
    "#     print(f\"{i} encodes to {encode(i)}\")\n",
    "# print(encode(text[:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZcvKeBXoZFOY"
   },
   "source": [
    "### Full finished code, for reference\n",
    "\n",
    "You may want to refer directly to the git repo instead though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# optim hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "print(device)\n",
    "\n",
    "# Set GPT config to be equivalent\n",
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class DemoConfig:\n",
    "    block_size: int = 256             # what is the maximum context length for predictions?\n",
    "    n_layer: int = 6\n",
    "    n_head: int = 6\n",
    "    n_embd: int = 384\n",
    "    dropout: float = 0.2\n",
    "    bias: bool = True\n",
    "    attention_type: str = \"global\"\n",
    "    \n",
    "config = DemoConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - config.block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+config.block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+config.block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "# start_pos_train = torch.where(train_data == 0)[0]\n",
    "# start_pos_val = torch.where(val_data == 0)[0]\n",
    "# def get_batch(split):\n",
    "#     # Generate a small batch of data of inputs x and targets y, but ensure inputs x start at new paragraph.\n",
    "#     data = train_data if split == 'train' else val_data\n",
    "#     start_positions = start_pos_train if split == \"train\" else start_pos_val\n",
    "#     ix = torch.randint(len(start_positions)-2, (batch_size,))\n",
    "#     x = torch.stack([data[start_positions[i]:start_positions[i]+block_size] for i in ix])\n",
    "#     y = torch.stack([data[start_positions[i]+1:start_positions[i]+block_size+1] for i in ix])\n",
    "#     return x, y\n",
    "    \n",
    "@torch.no_grad()\n",
    "def estimate_loss(eval_iters=200):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CPRD.src.models.gpt_pico.transformer import GPTLanguageModel\n",
    "\n",
    "model_base = GPTLanguageModel(config, vocab_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CPRD.src.models.gpt_simple.transformer import GPTModel\n",
    "\n",
    "config.learn_position_encoding = False\n",
    "model1 = GPTModel(config, vocab_size).to(device)\n",
    "\n",
    "config.learn_position_encoding = True\n",
    "model2 = GPTModel(config, vocab_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [model_base, model1, model2]\n",
    "loss_curves_train = [[] for _ in models]\n",
    "loss_curves_val = [[] for _ in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hoelkOrFY8bN",
    "outputId": "961304cd-e379-40d4-dd56-8de0b91d2861"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.788929 M parameters\n",
      "step 0: train loss 4.2848, val loss 4.2826\n",
      "step 500: train loss 1.9992, val loss 2.0904\n",
      "step 1000: train loss 1.5945, val loss 1.7714\n",
      "step 1500: train loss 1.4369, val loss 1.6495\n",
      "step 2000: train loss 1.3363, val loss 1.5687\n",
      "step 2500: train loss 1.2732, val loss 1.5222\n",
      "step 3000: train loss 1.2228, val loss 1.5002\n",
      "step 3500: train loss 1.1813, val loss 1.4874\n",
      "step 4000: train loss 1.1429, val loss 1.4846\n",
      "step 4500: train loss 1.1090, val loss 1.4781\n",
      "\n",
      "Had you to proceed, had silence for perfume your apprellable day;\n",
      "And acqualityth of you and the honour of honour\n",
      "At you, trouble mempt; how you sing how,\n",
      "My pripessed well, which, the whoa'd foul pride\n",
      "And makes him; most unrestroy' waters,\n",
      "Than in his face.\n",
      "\n",
      "FISAL:\n",
      "Good night!\n",
      "\n",
      "SAMPSON:\n",
      "A gentleman, which I have harrdlingly\n",
      "Have one doned to myself towns; but they but not\n",
      "Flattering, nothing at like the all. If I\n",
      "I wake it; for forsake, anone I sue, then.\n",
      "He loving Hastings Marcius, as yound Clarence\n",
      "The valour, graves when he caught of they words.\n",
      "Now, prithee, the griment from drown brap. Say, what, I\n",
      "Pletter it: she be some worn, and grouns a corrupts;\n",
      "Before other, when art thou shalt an art\n",
      "The ickless o'er-times, they punted, offence\n",
      "With crowns those we lies peace, there hope, pretting,\n",
      "In view are uppoted, proud dray,\n",
      "Swith hath not kiss'd to giving himory.\n",
      "\n",
      "Shepherd:\n",
      "A hort as enjost whether us,\n",
      "We loather 'blown'd,'--\n",
      "God hatred him in mother,--Sister, my lord,\n",
      "Withdraws; now though you, Beholy, Isabelland:\n",
      "A back a happy children of me,\n",
      "Of nothing but his son's house, alackness'st,\n",
      "Either than you a'em, but only wore:\n",
      "If it can and not seen the indeed of you arm\n",
      "To be we toward force my weeds,\n",
      "And with your thrown will bead the Duke's rother,\n",
      "And made it it must be conquir the gate.\n",
      "\n",
      "MERCUTIO:\n",
      "Backing, I will renown to know! Valenting!\n",
      "\n",
      "HORTIO:\n",
      "It shall be blown mine ears.\n",
      "\n",
      "BENVOLIO:\n",
      "So Romeo, this is long in him:\n",
      "The fiar is a ne'er promise.\n",
      "\n",
      "MERCUTIO:\n",
      "Here die flowned for the Duke.\n",
      "\n",
      "BENVOLIO:\n",
      "Tut, my lord?\n",
      "\n",
      "Thieves are all thou orious dovor.\n",
      "\n",
      "MERCHIONA:\n",
      "Lest I have it had not a kind forth\n",
      "The god-like a run or buttor.\n",
      "\n",
      "Messenger:\n",
      "A good time for worthy base a punish and twith\n",
      "handing. When she follow you should of day\n",
      "As I did those contentent year'd their battle,\n",
      "Adving and all moisted to protest I must corn'd\n",
      "A tomore for both. I have levy born Ess,\n",
      "My retue in a thought foolm: hath ask'd his life?\n",
      "\n",
      "EDWARD:\n",
      "Hail, will, this lekeling of his womb:\n",
      "Thr\n",
      "10.690625 M parameters\n",
      "step 0: train loss 4.3741, val loss 4.3691\n",
      "step 500: train loss 2.2386, val loss 2.3281\n",
      "step 1000: train loss 1.7774, val loss 1.9219\n",
      "step 1500: train loss 1.5895, val loss 1.7820\n",
      "step 2000: train loss 1.4900, val loss 1.6975\n",
      "step 2500: train loss 1.4174, val loss 1.6621\n",
      "step 3000: train loss 1.3726, val loss 1.6181\n",
      "step 3500: train loss 1.3360, val loss 1.5967\n",
      "step 4000: train loss 1.3044, val loss 1.5820\n",
      "step 4500: train loss 1.2800, val loss 1.5715\n",
      "\n",
      "Mustance, by-this, welcome said deestil; and fecies me by their secuesed\n",
      "Are their arms: and rebbut him as your grave eyes are so a foy.\n",
      "\n",
      "QUEN MARGARET:\n",
      "Weping tell him that love;\n",
      "Revolt of their confusion\n",
      "og but wind the rod your season\n",
      "To this first o'erbors; they must neet now to breath, whose a hear for Rusend.\n",
      "For Cliford, whethin themeel hoare\n",
      "convented flints;\n",
      "not, by God Geven unto my house,\n",
      "And not oft? I would, thou wred here?\n",
      "Did Claudio ancient half;\n",
      "Where time so and see,--which is pare that go\n",
      "That save at has form: I\n",
      "A had nothing storn's of taunts; yet on incertred are of all of traible\n",
      "As the purpil'd:\n",
      "How no understand by from her sintent Pauntians:\n",
      "The othe gently make and deseserved them are lovight a Rome\n",
      "Hadst said plottingents! look?\n",
      "\n",
      "Third Musicen:\n",
      "But not kee, getlems,\n",
      "To give that tremoves\n",
      "which no stifle to for loss.\n",
      "\n",
      "GLOUCESTER:\n",
      "Their Somerset,\n",
      "Like the tof barie you, mistress in my smile father's own by my modist proud Madge,\n",
      "Is it, for your too to five, do my head not impresious bire\n",
      "Of warrious litle, his for our.\n",
      "\n",
      "DERBY:\n",
      "I come and hours wife\n",
      "Gaod I wise that for, soothine within: he down in it way slonger flattted and tears sweed to thy feedian!\n",
      "The man, like still for hellow the super\n",
      "Of ere ladther the wish.\n",
      "\n",
      "SLY:\n",
      "He had yet had deserves\n",
      "Unme divirtued: but see the noble own of such a joy-head,\n",
      "Than he meeake you. How downey, foug's angres speaks,\n",
      "and noble friend wath ase fire Banca hamberlar.\n",
      "Offor evenient and exprieme to the forest me, I repant you and pi'd: get\n",
      "a till to use: ven themine\n",
      "As more that we missed,\n",
      "To iexcured abiting, in chief, he these touches. Is it is a peach?\n",
      "\n",
      "QUEN MARGARET:\n",
      "\n",
      "No opiness ere af as no mamatch; apparent to the cample and Henry livior men heavend a sister up of tyranth climather\n",
      "Wine Lord Clifford come with that eve y governs,\n",
      "'From some butt thanking? When its depulsed so bedingment,\n",
      "Nor speed or privile,\n",
      "To many, you'll manchorous woran. O to them here Tower,\n",
      "And seconse no more two long a gals\n",
      "10.788929 M parameters\n",
      "step 0: train loss 4.3046, val loss 4.3042\n",
      "step 500: train loss 1.8533, val loss 1.9740\n",
      "step 1000: train loss 1.5474, val loss 1.7301\n",
      "step 1500: train loss 1.4084, val loss 1.6193\n",
      "step 2000: train loss 1.3274, val loss 1.5627\n",
      "step 2500: train loss 1.2746, val loss 1.5355\n",
      "step 3000: train loss 1.2270, val loss 1.5026\n",
      "step 3500: train loss 1.1880, val loss 1.5020\n",
      "step 4000: train loss 1.1544, val loss 1.4891\n",
      "step 4500: train loss 1.1214, val loss 1.4864\n",
      "\n",
      "iite this princely fool?\n",
      "\n",
      "DERBY:\n",
      "Yield to your grace\n",
      "As I thank him gone.\n",
      "\n",
      "BRUTUS:\n",
      "A gentleman retors mink.\n",
      "\n",
      "RIVERS:\n",
      "You slew me and speak.\n",
      "3 KING HENRY VI:\n",
      "Say thou tell me, Oxford, were there alliends,\n",
      "But 'tis to blood the\n",
      "castle, I will took your draws, Sir Jotton Margaret,\n",
      "And make the moon of this ply ourse--fool\n",
      "But record, that I was this.\n",
      "\n",
      "QUEEN:\n",
      "Women!\n",
      "\n",
      "Fiend Gentleman:\n",
      "I think fast you she's and down,--for sit three he is now\n",
      "Before, and make any thoughts,\n",
      "Bear me not: but\n",
      "Why, fell in?\n",
      "\n",
      "CAMILLO:\n",
      "By my voices; I can:\n",
      "Once can doth revive.\n",
      "\n",
      "ISABELLA:\n",
      "Rather is like a bell\n",
      "An interrel sorrow.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "Where fond circumstance: why, I must her,\n",
      "Therefore most welcome birth\n",
      "of help to move them. I have send forgive you arm too:\n",
      "In some cousintess\n",
      "And since it that needs,\n",
      "Though and less. What was the hope of point four\n",
      "The wars of that the stream of iropher Surrey hour mouther\n",
      "Were you have absent thousand in this woman\n",
      "Will minute out o'erbstand but so;\n",
      "Think he seems of thy pale,\n",
      "His elden blood, then the power to be dead.\n",
      "\n",
      "POLIXENES:\n",
      "I had reash you to me.\n",
      "\n",
      "MENENIUS:\n",
      "And that is the Lewis peruse the curbs\n",
      "Are than in my very from these threatens\n",
      "It tutous creature in your wife;\n",
      "But it was for the lead ere to-night.\n",
      "\n",
      "RICHARD:\n",
      "Thou art king, and thou bost her!\n",
      "\n",
      "WARWICK:\n",
      "It is: we do live.\n",
      "\n",
      "CLAUDIO:\n",
      "\n",
      "LEONTES:\n",
      "I ceal,\n",
      "Are you do hear means your and my heart.\n",
      "3n with tempest against myself.\n",
      "\n",
      "ANGELO:\n",
      "I through be neathed gods\n",
      "A mother's warmant love,\n",
      "But vetil shall to choose\n",
      "To Bolingbroke,\n",
      "I am now I so't guess\n",
      "And no lesser'd and the duke of his attempo to breathe of Hereford, I cannot swear\n",
      "The sport and thou doth great him sworn,\n",
      "His general praying power thy beauty:\n",
      "And, who is able absenent: it is\n",
      "The summers of all the morns\n",
      "Imiasuse: an are talk'd by Westmons;\n",
      "Or, we have nods\n",
      "this courteens us as fortune and speeds here he they should say the gaze of my longer; and take until tetcher\n",
      "And to his old on a hones,\n",
      "And tell they in the poor edregnan\n"
     ]
    }
   ],
   "source": [
    "for idx, model in enumerate(models):\n",
    "    m = model.to(device)\n",
    "\n",
    "    # print the number of parameters in the model\n",
    "    print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "    # create a PyTorch optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for i in range(max_iters):\n",
    "\n",
    "        # every once in a while evaluate the loss on train and val sets\n",
    "        if i % eval_interval == 0 or iter == max_iters - 1:\n",
    "            losses = estimate_loss()\n",
    "            print(f\"step {i}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "        # sample a batch of data\n",
    "        xb, yb = get_batch('train')\n",
    "        # print(f\"in: {xb.shape}, out {yb.shape}\")\n",
    "\n",
    "        # evaluate the loss\n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_curves_train[idx].append(losses[\"train\"])\n",
    "        loss_curves_val[idx].append(losses[\"val\"])\n",
    "\n",
    "    # generate from the model\n",
    "    context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "    print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = [torch.cat([tmp.unsqueeze(0) for tmp in loss_curves_train[m_idx]], dim=0).numpy() for m_idx, _ in enumerate(models)]\n",
    "vs = [torch.cat([tmp.unsqueeze(0) for tmp in loss_curves_val[m_idx]], dim=0).numpy() for m_idx, _ in enumerate(models)]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.figure()\n",
    "cols = [\"k\", \"r\", \"b\"]\n",
    "for m_idx, t in enumerate(ts):\n",
    "    plt.plot(np.arange(t.shape[0]), t, label=f\"{m_idx}-train\", c=cols[m_idx], linestyle='dashed')\n",
    "for m_idx, v in enumerate(vs):\n",
    "    plt.plot(np.arange(v.shape[0]), v, label=f\"{m_idx}-val\", c=cols[m_idx])\n",
    "plt.legend()\n",
    "plt.savefig(\"figs/tinyshakespeare/loss_curves.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"Hath thou no remorse?\"\n",
    "# context = torch.from_numpy(np.array(encode(prompt)).reshape((1,-1)))\n",
    "# print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fjjvMifYZf7x"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "sys_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
