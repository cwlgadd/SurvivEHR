{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJpXpmjEYC_T"
   },
   "source": [
    "# Demo Notebook:\n",
    "## Transformer For Causal Language Modelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "/rds/homes/g/gaddcz/Projects/CPRD/examples/modelling\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sqlite3\n",
    "from dataclasses import dataclass\n",
    "import logging\n",
    "from CPRD.data.foundational_loader import FoundationalDataModule\n",
    "from CPRD.src.models.benchmarks.karpathy_gpt.transformer import GPTLanguageModel\n",
    "from CPRD.src.models.transformer.task_heads.causal_lm import TransformerForCausalLM\n",
    "\n",
    "# TODO:\n",
    "# replace experiment boilerplate with pytorch lightning\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "# device = \"cpu\"    # just for if i need more informative debugging statements\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set config to be equivalent architecture of kaparthy benchmark,\n",
    "#      note: there will be fewer paramaters due to weight tying in the causal language modelling head\n",
    "@dataclass\n",
    "class DemoConfig:\n",
    "    learn_positional_embedding: bool = True\n",
    "    block_size: int = 256        # what is the maximum context length for predictions?\n",
    "    n_layer: int = 6\n",
    "    n_head: int = 6\n",
    "    n_embd: int = 384\n",
    "    bias: bool = True\n",
    "    attention_type: str = \"global\"    \n",
    "    dropout: float = 0.0\n",
    "    unk_freq_threshold: float = 0.0\n",
    "\n",
    "config = DemoConfig()\n",
    "\n",
    "@dataclass\n",
    "class OptConfig:\n",
    "    batch_size: int = 64\n",
    "    eval_interval: int = 1\n",
    "    learning_rate: float = 3e-4\n",
    "    epochs: int = 20\n",
    "    \n",
    "opt = OptConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data loader on a reduced cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Building polars dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using N=10000 random samples, from the available 117102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using measurements\n",
      "INFO:root:Using diagnoses\n",
      "INFO:root:Dropping samples with no dynamic events\n",
      "INFO:root:Using non-tabular tokenizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8647 training, 481 validation, and 480 test samples\n",
      "100 vocab elements\n"
     ]
    }
   ],
   "source": [
    "from CPRD.data.database import queries\n",
    "\n",
    "# Get a list of patients which fit a reduced set of criterion\n",
    "PATH_TO_DB = \"/rds/projects/s/subramaa-mum-predict/CharlesGadd_Oxford/FoundationModel/preprocessing/processed/cprd.db\"\n",
    "conn = sqlite3.connect(PATH_TO_DB)\n",
    "cursor = conn.cursor()\n",
    "identifiers1 = queries.query_measurement([\"bmi\", \"diastolic_blood_pressure\"], cursor)        \n",
    "identifiers2 = queries.query_diagnosis([\"DEPRESSION\", \"TYPE1DM\", \"TYPE2DIABETES\"], cursor)    #  \"DEPRESSION\"  ,  \"ANXIETY\"\n",
    "all_identifiers = list(set(identifiers1).intersection(identifiers2))    # Turn smaller list into the set\n",
    "\n",
    "# Lets take only the first N for faster run-time\n",
    "N = np.min((len(all_identifiers), 10000))\n",
    "print(f\"Using N={N} random samples, from the available {len(all_identifiers)}\")\n",
    "identifiers = random.choices(all_identifiers, k=N)\n",
    "\n",
    "# Build \n",
    "dm = FoundationalDataModule(identifiers=identifiers,\n",
    "                            tokenizer=\"non-tabular\",\n",
    "                            batch_size=opt.batch_size,\n",
    "                            max_seq_length=config.block_size,\n",
    "                            unk_freq_threshold=config.unk_freq_threshold,\n",
    "                            include_measurements=True,\n",
    "                            include_diagnoses=True,\n",
    "                            preprocess_measurements=False)\n",
    "vocab_size = dm.train_set.tokenizer.vocab_size\n",
    "\n",
    "print(f\"{len(dm.train_set)} training, {len(dm.val_set)} validation, and {len(dm.test_set)} test samples\")\n",
    "print(f\"{vocab_size} vocab elements\")\n",
    "# print(dm.train_set.tokenizer._itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (88, 3)\n",
      "┌───────────────────────────────────┬────────┬──────────┐\n",
      "│ EVENT                             ┆ counts ┆ freq     │\n",
      "│ ---                               ┆ ---    ┆ ---      │\n",
      "│ str                               ┆ u32    ┆ f64      │\n",
      "╞═══════════════════════════════════╪════════╪══════════╡\n",
      "│ UNK                               ┆ 0      ┆ 0.0      │\n",
      "│ diastolic_blood_pressure          ┆ 181777 ┆ 0.422629 │\n",
      "│ bmi                               ┆ 70392  ┆ 0.16366  │\n",
      "│ eosinophil_count                  ┆ 66905  ┆ 0.155553 │\n",
      "│ basophil_count                    ┆ 44291  ┆ 0.102976 │\n",
      "│ corrected_serum_calcium_level     ┆ 12459  ┆ 0.028967 │\n",
      "│ DEPRESSION                        ┆ 6910   ┆ 0.016066 │\n",
      "│ serum_level                       ┆ 5513   ┆ 0.012818 │\n",
      "│ calculated_LDL_cholesterol_level  ┆ 5249   ┆ 0.012204 │\n",
      "│ ANXIETY                           ┆ 3812   ┆ 0.008863 │\n",
      "│ HYPERTENSION                      ┆ 2697   ┆ 0.00627  │\n",
      "│ TYPE2DIABETES                     ┆ 2247   ┆ 0.005224 │\n",
      "│ OSTEOARTHRITIS                    ┆ 2058   ┆ 0.004785 │\n",
      "│ ASTHMA_PUSHASTHMA                 ┆ 2032   ┆ 0.004724 │\n",
      "│ ATOPICECZEMA                      ┆ 1818   ┆ 0.004227 │\n",
      "│ aspartate_transam                 ┆ 1445   ┆ 0.00336  │\n",
      "│ ALLERGICRHINITISCONJ              ┆ 1439   ┆ 0.003346 │\n",
      "│ ANY_DEAFNESS_HEARING_LOSS         ┆ 1351   ┆ 0.003141 │\n",
      "│ PREVALENT_IBS                     ┆ 964    ┆ 0.002241 │\n",
      "│ ALLCA_NOBCC_VFINAL                ┆ 962    ┆ 0.002237 │\n",
      "│ ALCOHOLMISUSE                     ┆ 930    ┆ 0.002162 │\n",
      "│ IHD_NOMI                          ┆ 911    ┆ 0.002118 │\n",
      "│ blood_urea                        ┆ 872    ┆ 0.002027 │\n",
      "│ CKDSTAGE3TO5                      ┆ 871    ┆ 0.002025 │\n",
      "│ PERIPHERAL_NEUROPATHY             ┆ 741    ┆ 0.001723 │\n",
      "│ HYPOTHYROIDISM_DRAFT_V1           ┆ 690    ┆ 0.001604 │\n",
      "│ COPD                              ┆ 637    ┆ 0.001481 │\n",
      "│ calcium_adjusted_level            ┆ 607    ┆ 0.001411 │\n",
      "│ AF                                ┆ 505    ┆ 0.001174 │\n",
      "│ PSORIASIS                         ┆ 494    ┆ 0.001149 │\n",
      "│ combined_total_vitamin_D2_and_D3… ┆ 480    ┆ 0.001116 │\n",
      "│ HF                                ┆ 475    ┆ 0.001104 │\n",
      "│ GOUT                              ┆ 448    ┆ 0.001042 │\n",
      "│ OSTEOPOROSIS                      ┆ 435    ┆ 0.001011 │\n",
      "│ SUBSTANCEMISUSE                   ┆ 412    ┆ 0.000958 │\n",
      "│ MINFARCTION                       ┆ 391    ┆ 0.000909 │\n",
      "│ ALL_DEMENTIA                      ┆ 369    ┆ 0.000858 │\n",
      "│ STROKEUNSPECIFIED                 ┆ 356    ┆ 0.000828 │\n",
      "│ hydroxyvitamin3                   ┆ 288    ┆ 0.00067  │\n",
      "│ PAD_STRICT                        ┆ 271    ┆ 0.00063  │\n",
      "│ hydroxyvitamin2                   ┆ 266    ┆ 0.000618 │\n",
      "│ OTHER_CHRONIC_LIVER_DISEASE_OPTI… ┆ 262    ┆ 0.000609 │\n",
      "│ VALVULARDISEASES                  ┆ 260    ┆ 0.000604 │\n",
      "│ TYPE1DM                           ┆ 227    ┆ 0.000528 │\n",
      "│ OSA                               ┆ 198    ┆ 0.00046  │\n",
      "│ EPILEPSY                          ┆ 196    ┆ 0.000456 │\n",
      "│ NAFLD                             ┆ 196    ┆ 0.000456 │\n",
      "│ FIBROMYALGIA                      ┆ 189    ┆ 0.000439 │\n",
      "│ POLYCYSTIC_OVARIAN_SYNDROME_PCOS  ┆ 178    ┆ 0.000414 │\n",
      "│ RHEUMATOIDARTHRITIS               ┆ 175    ┆ 0.000407 │\n",
      "│ PMRANDGCA                         ┆ 165    ┆ 0.000384 │\n",
      "│ HYPERTHYROIDISM                   ┆ 163    ┆ 0.000379 │\n",
      "│ ENDOMETRIOSIS_ADENOMYOSIS_V2      ┆ 162    ┆ 0.000377 │\n",
      "│ EATINGDISORDERS                   ┆ 161    ┆ 0.000374 │\n",
      "│ creatinine_ratio                  ┆ 132    ┆ 0.000307 │\n",
      "│ PTSDDIAGNOSIS                     ┆ 125    ┆ 0.000291 │\n",
      "│ VISUAL_IMPAIRMENT                 ┆ 118    ┆ 0.000274 │\n",
      "│ ISCHAEMICSTROKE                   ┆ 117    ┆ 0.000272 │\n",
      "│ SCHIZOPHRENIAMM                   ┆ 100    ┆ 0.000232 │\n",
      "│ BRONCHIECTASIS                    ┆ 97     ┆ 0.000226 │\n",
      "│ BIPOLAR                           ┆ 95     ┆ 0.000221 │\n",
      "│ AORTICANEURYSM                    ┆ 76     ┆ 0.000177 │\n",
      "│ CHRONIC_LIVER_DISEASE_ALCOHOL     ┆ 74     ┆ 0.000172 │\n",
      "│ brain_natriuretic_peptide_level   ┆ 72     ┆ 0.000167 │\n",
      "│ CHRONICFATIGUESYNDROMEMM          ┆ 62     ┆ 0.000144 │\n",
      "│ MENIERESDISEASE                   ┆ 59     ┆ 0.000137 │\n",
      "│ ULCERATIVE_COLITIS                ┆ 59     ┆ 0.000137 │\n",
      "│ PERNICIOUSANAEMIA                 ┆ 56     ┆ 0.00013  │\n",
      "│ PARKINSONS                        ┆ 55     ┆ 0.000128 │\n",
      "│ ILD_SH                            ┆ 52     ┆ 0.000121 │\n",
      "│ AUTISM                            ┆ 49     ┆ 0.000114 │\n",
      "│ CROHNS_DISEASE                    ┆ 48     ┆ 0.000112 │\n",
      "│ LEARNINGDISABILITY                ┆ 48     ┆ 0.000112 │\n",
      "│ STROKE_HAEMRGIC                   ┆ 45     ┆ 0.000105 │\n",
      "│ PSORIATICARTHRITIS2021            ┆ 44     ┆ 0.000102 │\n",
      "│ LEUKAEMIA_PREVALENCE              ┆ 40     ┆ 0.000093 │\n",
      "│ LYMPHOMA_PREVALENCE               ┆ 39     ┆ 0.000091 │\n",
      "│ MS                                ┆ 33     ┆ 0.000077 │\n",
      "│ HIVAIDS                           ┆ 31     ┆ 0.000072 │\n",
      "│ SJOGRENSSYNDROME                  ┆ 16     ┆ 0.000037 │\n",
      "│ PLASMACELL_NEOPLASM               ┆ 13     ┆ 0.00003  │\n",
      "│ SYSTEMIC_LUPUS_ERYTHEMATOSUS      ┆ 12     ┆ 0.000028 │\n",
      "│ HAEMOCHROMATOSIS                  ┆ 11     ┆ 0.000026 │\n",
      "│ SYSTEMIC_SCLEROSIS                ┆ 8      ┆ 0.000019 │\n",
      "│ ADDISON_DISEASE                   ┆ 8      ┆ 0.000019 │\n",
      "│ blood_calcium                     ┆ 7      ┆ 0.000016 │\n",
      "│ DOWNSSYNDROME                     ┆ 5      ┆ 0.000012 │\n",
      "│ SICKLE_CELL_DISEASE               ┆ 2      ┆ 0.000005 │\n",
      "└───────────────────────────────────┴────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "pl.Config.set_tbl_rows(vocab_size + 1)\n",
    "print(dm.tokenizer._event_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create models and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using Positional Embedding. This module uses the index position of an event within the block of events.\n",
      "INFO:root:Using Positional Encoding. This module uses the index position of an event within the block of events.\n"
     ]
    }
   ],
   "source": [
    "models, m_names = [], []\n",
    "\n",
    "# Baseline model to test my changes against\n",
    "# models.append(GPTLanguageModel(config, vocab_size).to(device))\n",
    "# m_names.append(\"kaparthy benchmark\")\n",
    "\n",
    "# My development model\n",
    "for pe in [True, False]:\n",
    "    config = DemoConfig()\n",
    "    config.learn_positional_embedding = pe\n",
    "    models.append(TransformerForCausalLM(config, vocab_size).to(device))\n",
    "    m_names.append(f\"{'pos_embedding' if pe else 'pos_encoding'}\")\n",
    "\n",
    "loss_curves_train = [[] for _ in models]\n",
    "loss_curves_val = [[] for _ in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hoelkOrFY8bN",
    "outputId": "961304cd-e379-40d4-dd56-8de0b91d2861"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model `pos_embedding`, with 10.777344 M parameters\n",
      "\n",
      "Epoch 0:\tTrain loss 1.81. Val loss 1.56\n",
      "Epoch 1:\tTrain loss 1.20. Val loss 1.31\n",
      "Epoch 2:\tTrain loss 1.07. Val loss 1.20\n",
      "Epoch 3:\tTrain loss 1.04. Val loss 1.18\n",
      "Epoch 4:\tTrain loss 1.02. Val loss 1.17\n",
      "Epoch 5:\tTrain loss 1.01. Val loss 1.15\n",
      "Epoch 6:\tTrain loss 1.01. Val loss 1.16\n",
      "Epoch 7:\tTrain loss 1.00. Val loss 1.15\n",
      "Epoch 8:\tTrain loss 1.00. Val loss 1.15\n",
      "Epoch 9:\tTrain loss 1.00. Val loss 1.15\n",
      "Epoch 10:\tTrain loss 0.99. Val loss 1.15\n",
      "Epoch 11:\tTrain loss 0.99. Val loss 1.14\n",
      "Epoch 12:\tTrain loss 0.99. Val loss 1.15\n",
      "Epoch 13:\tTrain loss 0.98. Val loss 1.15\n",
      "\t DEPRESSION ANXIETY diastolic_blood_pressure 7 9 . 0 basophil_count 0 . 1 eosinophil_count 0 . 3 basophil_count 0 . 1 eosinophil_count 0 . 3 diastolic_blood_pressure 7 0 . 0 eosinophil_count 0 .\n",
      "\n",
      "Training model `pos_encoding`, with 10.67904 M parameters\n",
      "\n",
      "Epoch 0:\tTrain loss 2.55. Val loss 2.47\n",
      "Epoch 1:\tTrain loss 2.10. Val loss 2.27\n",
      "Epoch 2:\tTrain loss 1.86. Val loss 1.94\n",
      "Epoch 3:\tTrain loss 1.43. Val loss 1.53\n",
      "Epoch 4:\tTrain loss 1.37. Val loss 1.46\n",
      "Epoch 5:\tTrain loss 1.22. Val loss 1.36\n",
      "Epoch 6:\tTrain loss 1.17. Val loss 1.35\n",
      "Epoch 7:\tTrain loss 1.15. Val loss 1.30\n",
      "Epoch 8:\tTrain loss 1.61. Val loss 1.48\n",
      "Epoch 9:\tTrain loss 1.21. Val loss 1.34\n",
      "\t DEPRESSION ANXIETY bmi 2 0 . 9 eosinophil_count 0 . 0 DEPRESSION diastolic_blood_pressure 7 0 . 0 eosinophil_count 0 . 2 bmi 2 5 . 7 2 diastolic_blood_pressure 8 0 .\n"
     ]
    }
   ],
   "source": [
    "for m_idx, (model, m_name) in enumerate(zip(models, m_names)):\n",
    "    \n",
    "    print(f\"\\nTraining model `{m_name}`, with {sum(p.numel() for p in model.parameters())/1e6} M parameters\\n\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # create a PyTorch optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=opt.learning_rate)\n",
    "\n",
    "    best_val, epochs_since_best = np.inf, 0\n",
    "    for epoch in range(opt.epochs):\n",
    "        epoch_loss = 0\n",
    "        model.train()\n",
    "        for i, batch in enumerate(dm.train_dataloader()):\n",
    "            # evaluate the loss\n",
    "            _, loss = model(batch['tokens'].to(device),\n",
    "                            attention_mask=batch['attention_mask'].to(device)\n",
    "                            )\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        epoch_loss /= i\n",
    "        loss_curves_train[m_idx].append(epoch_loss)\n",
    "\n",
    "        # evaluate the loss on val set\n",
    "        with torch.no_grad(): \n",
    "            model.eval()\n",
    "            if epoch % opt.eval_interval == 0 or epoch == opt.epochs - 1:\n",
    "                val_loss = 0\n",
    "                for j, batch in enumerate(dm.val_dataloader()):\n",
    "                    _, loss = model(batch['tokens'].to(device), \n",
    "                                    attention_mask=batch['attention_mask'].to(device)   \n",
    "                                   )\n",
    "                    val_loss += loss.item()\n",
    "                val_loss /= j\n",
    "                loss_curves_val[m_idx].append(val_loss)\n",
    "                print(f\"Epoch {epoch}:\\tTrain loss {epoch_loss:.2f}. Val loss {val_loss:.2f}\")          \n",
    "                # TODO: Note not fully accurate as last batch is likely not the same size, will be fixed with lightning\n",
    "        \n",
    "            if val_loss >= best_val:\n",
    "                epochs_since_best += 1\n",
    "                if epochs_since_best >= 2:\n",
    "                    break\n",
    "            else:\n",
    "                best_val = val_loss\n",
    "                epochs_since_best = 0\n",
    "\n",
    "    # Test trained model with a prompt\n",
    "    # ----------------    \n",
    "    # set context: an initial diagnosis of depression\n",
    "    tokens = torch.from_numpy(np.array(dm.encode([\"DEPRESSION\"])).reshape((1,-1))).to(device)\n",
    "    # generate: then sample the next 10 tokens\n",
    "    new_tokens = model.generate(tokens, max_new_tokens=30)[0].tolist()\n",
    "    generated = dm.decode(new_tokens)\n",
    "    print(f\"\\t {generated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"k\", \"r\", \"b\", \"y\"]\n",
    "\n",
    "# Plot loss\n",
    "plt.figure()\n",
    "for m_idx, _ in enumerate(models):\n",
    "    # Training\n",
    "    iterations = np.linspace(0, len(loss_curves_train[m_idx]), len(loss_curves_train[m_idx])) * opt.eval_interval\n",
    "    plt.plot(iterations, loss_curves_train[m_idx], label=f\"{m_names[m_idx]}-train\", c=cols[m_idx], linestyle='dashed')\n",
    "    # Validation\n",
    "    iterations = np.linspace(0, len(loss_curves_val[m_idx]), len(loss_curves_val[m_idx])) * opt.eval_interval\n",
    "    plt.plot(iterations, loss_curves_val[m_idx], label=f\"{m_names[m_idx]}-val\", c=cols[m_idx])\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(f\"figs/transformer/loss.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diabetes\n",
    "\n",
    "Probability of type II diabetes before and after a type I diagnosis\n",
    "\n",
    "keys: \n",
    "\n",
    "    70: 'TYPE1DM'\n",
    "    31: 'TYPE2DIABETES'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_token1 = dm.tokenizer._stoi[\"TYPE1DM\"]\n",
    "target_token2 = dm.tokenizer._stoi[\"TYPE2DIABETES\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small context comparison, high bmi and blood pressure vs low for diabetes risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_risk_prompt = [\"bmi\", \"2\", \"2\", \".\", \"5\", \"diastolic_blood_pressure\", \"7\", \"9\", \".\", \"0\"]\n",
    "high_risk_prompt = [\"bmi\", \"3\", \"7\", \".\", \"5\", \"diastolic_blood_pressure\", \"9\", \"9\", \".\", \"0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "pos_embedding\n",
      "=============\n",
      "\n",
      "Control: Low risk: \t (bmi,2,2,.,5,diastolic_blood_pressure,7,9,.,0): \n",
      "torch.Size([9, 100])\n",
      "probability of type I diabetes 0.0012%\n",
      "probability of type II diabetes 0.0011%\n",
      "\n",
      "Control: High risk: \t (bmi,3,7,.,5,diastolic_blood_pressure,9,9,.,0): \n",
      "torch.Size([9, 100])\n",
      "probability of type I diabetes 0.0012%\n",
      "probability of type II diabetes 0.0011%\n",
      "\n",
      "Control: Low risk + depression: \t (DEPRESSION,bmi,2,2,.,5,diastolic_blood_pressure,7,9,.,0): \n",
      "torch.Size([10, 100])\n",
      "probability of type I diabetes 0.0561%\n",
      "probability of type II diabetes 0.1972%\n",
      "\n",
      "Low risk context: Type 1 diagnosis in prompt: \t (TYPE1DM,bmi,2,2,.,5,diastolic_blood_pressure,7,9,.,0): \n",
      "torch.Size([10, 100])\n",
      "probability of type I diabetes 0.4271%\n",
      "probability of type II diabetes 0.7650%\n",
      "\n",
      "Low risk context: Type 1I diagnosis in prompt: \t (TYPE2DIABETES,bmi,2,2,.,5,diastolic_blood_pressure,7,9,.,0): \n",
      "torch.Size([10, 100])\n",
      "probability of type I diabetes 0.3746%\n",
      "probability of type II diabetes 0.3872%\n",
      "\n",
      "\n",
      "pos_encoding\n",
      "============\n",
      "\n",
      "Control: Low risk: \t (bmi,2,2,.,5,diastolic_blood_pressure,7,9,.,0): \n",
      "torch.Size([9, 100])\n",
      "probability of type I diabetes 0.0011%\n",
      "probability of type II diabetes 0.0023%\n",
      "\n",
      "Control: High risk: \t (bmi,3,7,.,5,diastolic_blood_pressure,9,9,.,0): \n",
      "torch.Size([9, 100])\n",
      "probability of type I diabetes 0.0011%\n",
      "probability of type II diabetes 0.0023%\n",
      "\n",
      "Control: Low risk + depression: \t (DEPRESSION,bmi,2,2,.,5,diastolic_blood_pressure,7,9,.,0): \n",
      "torch.Size([10, 100])\n",
      "probability of type I diabetes 0.2203%\n",
      "probability of type II diabetes 1.1138%\n",
      "\n",
      "Low risk context: Type 1 diagnosis in prompt: \t (TYPE1DM,bmi,2,2,.,5,diastolic_blood_pressure,7,9,.,0): \n",
      "torch.Size([10, 100])\n",
      "probability of type I diabetes 0.2068%\n",
      "probability of type II diabetes 0.8978%\n",
      "\n",
      "Low risk context: Type 1I diagnosis in prompt: \t (TYPE2DIABETES,bmi,2,2,.,5,diastolic_blood_pressure,7,9,.,0): \n",
      "torch.Size([10, 100])\n",
      "probability of type I diabetes 0.1947%\n",
      "probability of type II diabetes 1.1132%\n"
     ]
    }
   ],
   "source": [
    "prompts, desc = [], []\n",
    "\n",
    "desc.append(\"Control: Low risk\")\n",
    "prompts.append(low_risk_prompt)\n",
    "\n",
    "desc.append(\"Control: High risk\")\n",
    "prompts.append(high_risk_prompt)\n",
    "\n",
    "desc.append(\"Control: Low risk + depression\")\n",
    "prompts.append([\"DEPRESSION\"] + low_risk_prompt)\n",
    "\n",
    "desc.append(\"Low risk context: Type 1 diagnosis in prompt\")\n",
    "prompts.append([\"TYPE1DM\"] + low_risk_prompt)\n",
    "\n",
    "desc.append(\"Low risk context: Type 1I diagnosis in prompt\")\n",
    "prompts.append([\"TYPE2DIABETES\"] + low_risk_prompt)\n",
    "\n",
    "for model_idx, model in enumerate(models):\n",
    "    print(f\"\\n\\n{m_names[model_idx]}\\n\" + \"=\"*len(m_names[model_idx]))\n",
    "    \n",
    "    for p_idx, prompt in enumerate(prompts):\n",
    "        print(f\"\\n{desc[p_idx]}: \\t ({','.join(prompt)}): \")\n",
    "        encoded_prompt = torch.from_numpy(np.array(dm.encode(prompt)).reshape((1,-1))).to(device)\n",
    "        lgts, _ = model(encoded_prompt)\n",
    "        print(lgts.shape)\n",
    "        probs = torch.nn.functional.softmax(lgts, dim=1)\n",
    "        print(f\"probability of type I diabetes {100*float(probs[0, target_token1].cpu().detach().numpy()):.4f}%\")\n",
    "        print(f\"probability of type II diabetes {100*float(probs[0, target_token2].cpu().detach().numpy()):.4f}%\")\n",
    "\n",
    "# Note: adding a diagnosis (even if potentially orthogonal) at the beginning of the prompt increases probability of either type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: model architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "pos_embedding\n",
      "=============\n",
      "\n",
      "\n",
      "TransformerForCausalLM(\n",
      "  (transformer): Transformer(\n",
      "    (wpe): PositionalEmbedding(\n",
      "      (wpe): Embedding(256, 384)\n",
      "    )\n",
      "    (wte): Embedding(100, 384, padding_idx=0)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (blocks): ModuleList(\n",
      "      (0): Block(\n",
      "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (acti): ReLU()\n",
      "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): Block(\n",
      "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (acti): ReLU()\n",
      "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): Block(\n",
      "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (acti): ReLU()\n",
      "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): Block(\n",
      "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (acti): ReLU()\n",
      "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): Block(\n",
      "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (acti): ReLU()\n",
      "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): Block(\n",
      "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (acti): ReLU()\n",
      "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=384, out_features=100, bias=False)\n",
      ")\n",
      "\n",
      "\n",
      "pos_encoding\n",
      "============\n",
      "\n",
      "\n",
      "TransformerForCausalLM(\n",
      "  (transformer): Transformer(\n",
      "    (wpe): PositionalEncoding()\n",
      "    (wte): Embedding(100, 384, padding_idx=0)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (blocks): ModuleList(\n",
      "      (0): Block(\n",
      "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (acti): ReLU()\n",
      "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): Block(\n",
      "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (acti): ReLU()\n",
      "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): Block(\n",
      "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (acti): ReLU()\n",
      "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): Block(\n",
      "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (acti): ReLU()\n",
      "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): Block(\n",
      "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (acti): ReLU()\n",
      "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): Block(\n",
      "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (acti): ReLU()\n",
      "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=384, out_features=100, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for model_idx, model in enumerate(models):\n",
    "    print(f\"\\n\\n{m_names[model_idx]}\\n\" + \"=\"*len(m_names[model_idx]))\n",
    "    print(f\"\\n\\n{model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "sys_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
