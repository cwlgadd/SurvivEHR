is_decoder: True

data:
  batch_size: 64                    # Batch size
  unk_freq_threshold: 0.0           # Frequency required for token to be included. Tokens below threshold are transformed to <UNK>
  min_workers: 12                   # The number of workers to create for dataloader
  global_diagnoses: False           # Whether we enforce diagnoses which occurred before context block to prepend the block
  repeating_events: True            # Whether we allow repeated events. For example, if two Body Mass Index records exist then True retains both, whilst False keeps only the latest record.
  path_to_db: /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/cprd.db                         # Path to SQL database
  path_to_ds: /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/                # Path to Polars dataset
  # Path to meta_information. This is created when building datasets. It can then be modified, for example here we updated the default outlier bounds to custom values
  meta_information_path: /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/meta_information_QuantJenny.pickle
  subsample_training: null

experiment:
  type: 'pre-train'                  # Options: ['pre-train', 'zero-shot', 'fine-tune']
  project_name: SurvivEHR
  run_id: ${head.SurvLayer}PreTrain_small_${experiment.seed}
  fine_tune_id: null
  notes: null                        # A detailed description of the run. Use this argument to capture any context or details that may help you recall the purpose or setup of this run in the future.
  tags: null                         # Tags to be applied to a run in Weights and Biases
  train: True
  test: True
  verbose: True
  seed: 1337
  log: True
  log_dir: /rds/projects/s/subramaa-mum-predict/CharlesGadd_Oxford/FoundationModelOutput/                    # Where to place any log files
  ckpt_dir: /rds/projects/s/subramaa-mum-predict/CharlesGadd_Oxford/FoundationModelOutput/checkpoints/       # Where to save checkpoints during training 
  
fine_tuning:
  # Fine-tuning outcomes
  # If we are evaluating the pre-trained model (with zero-shot, or fine-tuning), which outcomes do we consider
  fine_tune_outcomes: null           # For zero-shot, few-shot, and fine-tuning experiments 
  custom_outcome_method:
    _target_: null
  # Custom Stratification method for callbacks (splitting logs by stratification labels determined by custom method)
  custom_stratification_method:
    _target_: null                    # Used to add custom functions which take as argument the batch and return stratification labels for metrics calculated during callbacks
  use_callbacks:
    hidden_embedding: True
    performance_metrics: True
    rmst: False
  head:
    surv_weight: 1
    value_weight: 0
    
optim: 
  num_epochs: 1                      # Number of epochs
  learning_rate: 3e-4                # AdamW learning rate (initial)
  scheduler_warmup: True             # Whether to add warmup to learning rate scheduler 
  scheduler: decaycawarmrestarts     # Type of scheduler to use (CAWarmRestarts, CosineAnnealingLR, DecayCAWarmRestarts, default: ReduceLROnPlateau)
  scheduler_periods: 10000           # If scheduler has periodicity (i.e. how long is warmup, how long between warmups in CA)
  learning_rate_decay: 0.8           # Decay rate when using DecayCAWarmRestarts
  val_check_interval: 2500           # How many training batches to perform between validation check
  early_stop: True                   # If we should do early stopping
  early_stop_patience: 30            # How many val_check intervals to continue without seeing an improvement
  log_every_n_steps: 20              # Logging frequency
  limit_val_batches: 0.025           # The fraction of patients we validate on each time (out of entire validation set)
  limit_test_batches: null           # The fraction of patients we test on each time (out of entire test set)
  accumulate_grad_batches: 1         # 

transformer:
  block_type: "Neo"                  # Architecture (Neo, or Nano)
  block_size: 256                    # what is the maximum context length for predictions?
  n_layer: 6                         # Number of MHA layers
  n_head: 6                          # Number of attention heads
  n_embd: 384                        # Embedding dimension
  layer_norm_bias: False             # Whether to add bias to layer norm (inspired by gpt 2 having = False)
  attention_type: "global"           # Type of attention. Neo: ["global", "local"], Nano ["global"]
  bias: True                         # Bias in the MLP layer inside the blocks
  dropout: 0.0                       # Dropout before attention blocks, and after MLP layer inside blocks
  attention_dropout: 0.0             # Dropout inside of attention
  resid_dropout: 0.0                 # Dropout in residual connection
  private_heads: 0                   # The number of heads to make private for value and survival prediction.
                                     #    E.g. if private_heads == 2, we have 2 MHA blocks whose n_embd//n_head dimensions are only used to predict values,
                                     #         and another two only used to predict survival curves - and the remainder are shared.
  use_fine_tune_adapter: False        # Whether to freeze the Transformer body in fine-tuning and add an adapter module to reduce parameter count
  adapter_dim: 8

head:
  # survival head
  SurvLayer: "cr"                    # Survival head type, ["competing-risk" | "cr"] or ["single-risk" | "sr"]
  surv_weight: 1                     # How much weight should be given to this aspect of the head
  # value head
  tokens_for_univariate_regression: None   # These are the tokens for which we create a value prediction head. This must be set after tokenization has been initialised
  value_weight: 0.1                  # How much weight should be given to this aspect of the head
