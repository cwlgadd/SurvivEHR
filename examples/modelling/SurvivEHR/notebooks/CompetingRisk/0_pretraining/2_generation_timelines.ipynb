{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJpXpmjEYC_T"
   },
   "source": [
    "# Generation Demo Notebook:\n",
    "## SurvivEHR: Competing Risk Survival Transformer For Causal Sequence Modelling \n",
    "\n",
    "In this notebook we demonstrate how a pre-trained model can be used for generation of future patient timelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added path '/rds/homes/g/gaddcz/Projects/CPRD/virtual-envTorch2.0-icelake/lib/python3.10/site-packages' at start of search paths.\n",
      "/rds/homes/g/gaddcz/Projects/CPRD/examples/modelling/SurvivEHR/notebooks/CompetingRisk/0_pretraining\n",
      "env: SLURM_NTASKS_PER_NODE=28       # TODO: define an env variable to fix for a local hpc environment issue, this shouldn't be needed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "node_type = os.getenv('BB_CPU')\n",
    "venv_dir = f'/rds/homes/g/gaddcz/Projects/CPRD/virtual-envTorch2.0-{node_type}'\n",
    "venv_site_pkgs = Path(venv_dir) / 'lib' / f'python{sys.version_info.major}.{sys.version_info.minor}' / 'site-packages'\n",
    "if venv_site_pkgs.exists():\n",
    "    sys.path.insert(0, str(venv_site_pkgs))\n",
    "    print(f\"Added path '{venv_site_pkgs}' at start of search paths.\")\n",
    "else:\n",
    "    print(f\"Path '{venv_site_pkgs}' not found. Check that it exists and/or that it exists for node-type '{node_type}'.\")\n",
    "\n",
    "!pwd\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%env SLURM_NTASKS_PER_NODE=28       # TODO: define an env variable to fix for a local hpc environment issue, this shouldn't be needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from pycox.evaluation import EvalSurv\n",
    "from tqdm import tqdm\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "from contextlib import redirect_stdout\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "from CPRD.examples.modelling.SurvivEHR.run_experiment import run\n",
    "from CPRD.examples.modelling.SurvivEHR.setup_causal_experiment import CausalExperiment\n",
    "from CPRD.src.models.survival.task_heads.causal import SurvStreamGPTForCausalModelling\n",
    "\n",
    "from FastEHR.dataloader import FoundationalDataModule\n",
    "from FastEHR.database.collector import SQLiteDataCollector\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "warnings.simplefilter('error', np.VisibleDeprecationWarning)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = \"cpu\"    # if more informative debugging statements are needed\n",
    "print(f\"Using device: {device}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation with SurvivEHR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_to_sample(batch, idx, remove_masked=True):\n",
    "    # Take `idx` patient from this batch\n",
    "    batch = {k: v[[idx]] for k, v in batch.items()}\n",
    "\n",
    "    if remove_masked:\n",
    "        mask = (batch[\"tokens\"][0, :] != 0)\n",
    "        batch[\"tokens\"] = batch[\"tokens\"][:, mask]\n",
    "        batch[\"ages\"] = batch[\"ages\"][:, mask]\n",
    "        batch[\"values\"] = batch[\"values\"][:, mask]\n",
    "\n",
    "    return batch\n",
    "\n",
    "def clip_outliers(token, unstandardised_value):\n",
    "    \"\"\"\n",
    "    Because of heavy right tails in the value distributions, standardisation of some token values over-estimates the lower quantile.\n",
    "    This method ensures no values beyond those seen in the data are reported.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        assert not np.isnan(unstandardised_value)\n",
    "        token_meta = dm.meta_information[\"measurement_tables\"][dm.meta_information[\"measurement_tables\"][\"event\"] == token]\n",
    "        _min = token_meta[\"min\"]\n",
    "        _max = token_meta[\"max\"]\n",
    "        unstandardised_value = np.min((unstandardised_value, _max))\n",
    "        unstandardised_value = np.max((unstandardised_value, _min))\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    return unstandardised_value\n",
    "\n",
    "def report_generation(static, tokens, ages, values, attention_mask, true_seq_len, dm, eos_token=\"DEATH\", **kwargs):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    tokens = tokens[0, :]\n",
    "    ages = ages[0, :]\n",
    "    values = values[0, :]\n",
    "    attention_mask = attention_mask[0, :]\n",
    "\n",
    "    static = dm.test_set._decode_covariates(static.cpu())\n",
    "    print(\"STATIC INFORMATION\")\n",
    "    print(\"=\"*120)\n",
    "    for key, item in static.items():\n",
    "        print(f\"\\t{key}:\".ljust(20) + f\"{item[0]}\")\n",
    "\n",
    "    # Report\n",
    "    tokens = dm.tokenizer.decode(tokens.tolist()).split(\" \")\n",
    "    diagnoses = []\n",
    "    last_age_day, last_age_week = 0, 0\n",
    "    print(\"\\n\\nGiven patient context\".upper())\n",
    "    print(\"=\"*120)\n",
    "    print(f\"\\tEVENT\".ljust(75) + \"| AGE IN WEEKS (days, years)\".ljust(30) + \" | VALUE\")\n",
    "    for idx_event, (token, _age, value, attn_mask) in enumerate(zip(tokens, ages, values, attention_mask)):\n",
    "\n",
    "        if attn_mask == 0:\n",
    "            break\n",
    "            \n",
    "        # Unscale age and bin to week fidelity\n",
    "        age_day = int(_age * dm.test_set.time_scale)\n",
    "        age_week = int(age_day / 7) \n",
    "        age_years = int(age_day / 365)\n",
    "\n",
    "        # If new event create break\n",
    "        if age_week != last_age_week:\n",
    "            print(\"\\t\" + \".\"*60 + \"new week\" + \".\"*60)\n",
    "\n",
    "        # Report next event\n",
    "        age = f\"{age_week}\\t ({age_day}, {age_years})\"\n",
    "        unstandardised_value = clip_outliers(token, dm.unstandardise(token, value))\n",
    "        value = f\"{unstandardised_value:.2f}\".ljust(10) + f\"({value:.2f})\"\n",
    "        print(f\"\\t{token.ljust(75)}| {age.ljust(30)}| {value}\".ljust(20))\n",
    "        \n",
    "        if token.upper() == token:\n",
    "            diagnoses.append(token)\n",
    "\n",
    "        if idx_event == true_seq_len - 1:\n",
    "            print(\"\\n\" + \"=\"*120)\n",
    "            print(\"Diagnosis summary\".upper())\n",
    "            print(f\"{diagnoses}\")\n",
    "            print(\"=\"*120)\n",
    "            print(\"\\n\")\n",
    "            print(\"Predicted future events\".upper())\n",
    "            print(\"=\"*120)\n",
    "            print(f\"\\tEVENT\".ljust(75) + \"| AGE IN WEEKS (days, years)\".ljust(30) + \"| VALUE\")\n",
    "\n",
    "\n",
    "        last_age_day = age_day\n",
    "        last_age_week = age_week\n",
    "        if token == eos_token:\n",
    "            break\n",
    "\n",
    "def log_generation(tokens, ages, values, attention_mask, observed_seq_lens, dm, eos_token=\"DEATH\"):\n",
    "\n",
    "    data = []\n",
    "    \n",
    "    for patient_idx in range(tokens.shape[0]):\n",
    "\n",
    "        # patient_observed_seq_len\n",
    "        observed_seq_len = observed_seq_lens[patient_idx] - 1\n",
    "        \n",
    "        # Remove the prompt context at the start of the generation\n",
    "        generated_tokens = tokens[patient_idx, observed_seq_len:].cpu().numpy()\n",
    "        generated_ages   = ages[patient_idx, observed_seq_len:].cpu().numpy()\n",
    "        generated_values = values[patient_idx, observed_seq_len:].cpu().numpy()\n",
    "        generated_attention_mask = attention_mask[patient_idx, observed_seq_len:].cpu().numpy()\n",
    "    \n",
    "        for generation_step in range(generated_tokens.shape[0]-1):\n",
    "\n",
    "            # If we reached padding then stop\n",
    "            if generated_attention_mask[generation_step+1] == 0:\n",
    "                break\n",
    "            \n",
    "            # Event transition\n",
    "            previous_token = dm.decode([generated_tokens[generation_step]])\n",
    "            next_token = dm.decode([generated_tokens[generation_step+1]])\n",
    "            \n",
    "            # Age, unscaled to years old\n",
    "            next_age = generated_ages[generation_step+1]\n",
    "            age_day = int(next_age * dm.test_set.time_scale)\n",
    "            age_week = int(age_day / 7) \n",
    "            age_years = int(age_day / 365)\n",
    "\n",
    "            # Ignore any events occuring after terminating token\n",
    "            if previous_token == eos_token:\n",
    "                break\n",
    "\n",
    "            # Log transition, and how many steps into generation this occurred. \n",
    "            record = [previous_token, next_token, generation_step, age_years]\n",
    "            data.append(record)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trained_models = [\"SurvivEHR-cr-small-debug7_exp1000-v1-v4-v1\"]\n",
    "config_names = [\"config_CompetingRisk11M\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate patient timelines for a handful of selected patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [ \"PreTrain\", \"FineTune_Hypertension\", \"FineTune_CVD\", \"FineTune_MultiMorbidity50+\"]\n",
    "patients_of_interest = [None,[0],[10],[1]]\n",
    "\n",
    "for pre_trained_model, config_name in zip(pre_trained_models, config_names):\n",
    "    os.makedirs(f\"figs/generation/{pre_trained_model}/\", exist_ok=True) \n",
    "\n",
    "    # load the configuration file, override any settings \n",
    "    with initialize(version_base=None, config_path=\"../../../confs\", job_name=\"testing_notebook\"):\n",
    "        cfg = compose(config_name=config_name, \n",
    "                      overrides=[# Experiment setup\n",
    "                                 f\"experiment.run_id='{pre_trained_model}'\",\n",
    "                                 \"experiment.train=False\",\n",
    "                                 \"experiment.test=False\",\n",
    "                                 \"experiment.log=False\",\n",
    "                                 # Dataloader\n",
    "                                 \"data.meta_information_path=/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/meta_information_QuantJenny.pickle\",\n",
    "                                 \"data.min_workers=3\",\n",
    "                                ]\n",
    "                     )     \n",
    "    experiment, dm = run(cfg)     \n",
    "    print(f\"Loaded model {pre_trained_model} with {sum(p.numel() for p in experiment.parameters())/1e6} M parameters\")\n",
    "    \n",
    "    for idx_dataset, dataset in enumerate(datasets):\n",
    "        print(f\"Generating patient timelines for dataset {dataset}\")\n",
    "        \n",
    "        gen_save_path = f'figs/generation/{pre_trained_model}/{dataset}_dataset/'\n",
    "        os.makedirs(gen_save_path, exist_ok=True) \n",
    "    \n",
    "        # Load dataset\n",
    "        dm = FoundationalDataModule(path_to_db=cfg.data.path_to_db,\n",
    "                                    path_to_ds=f\"/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/{dataset}/\",\n",
    "                                    overwrite_meta_information=cfg.data.meta_information_path,\n",
    "                                    load=True,\n",
    "                                    supervised=False if dataset.lower()==\"pretrain\" else True,\n",
    "                                    )\n",
    "        \n",
    "        # Load the first batch\n",
    "        for batch in dm.test_dataloader():\n",
    "\n",
    "            # Put all items on gpu\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            # # Optionally, get only the samples of interest from batch \n",
    "            # #       (shuffle is by default turned off in test data, so this should align to dm.test_set[idx])\n",
    "            # batch = batch_to_sample(batch, 6, remove_masked=True)\n",
    "            break\n",
    "        \n",
    "        for idx_gen in range(5):\n",
    "\n",
    "            # Generate forward\n",
    "            tokens, ages, values, attention_mask = experiment.model.generate(\n",
    "                static_covariates=batch[\"static_covariates\"],\n",
    "                tokens=batch['tokens'],\n",
    "                ages=batch['ages'],\n",
    "                values=batch['values'],\n",
    "                attention_mask=batch['attention_mask'],\n",
    "                max_new_tokens=50,\n",
    "                exceed_block_size=True,\n",
    "                )\n",
    "\n",
    "            # Report generated outcomes\n",
    "            patients = patients_of_interest[idx_dataset] if patients_of_interest[idx_dataset] is not None else [i for i in range(tokens.shape[0])]\n",
    "            for idx_patient in tqdm(patients, ascii=True, desc=f\"Reporting generation {idx_gen + 1} results for all patients in batch\"):\n",
    "                \n",
    "                out_dir = gen_save_path + f'patient{idx_patient}/'\n",
    "                os.makedirs(out_dir, exist_ok=True)\n",
    "                with open(out_dir + f\"generation{idx_gen}.txt\", 'w') as f:\n",
    "                    with redirect_stdout(f):\n",
    "                        report_generation(\n",
    "                            static         = batch[\"static_covariates\"][idx_patient], \n",
    "                            tokens         = tokens[[idx_patient],:],\n",
    "                            ages           = ages[[idx_patient],:], \n",
    "                            values         = values[[idx_patient], :], \n",
    "                            attention_mask = attention_mask[[idx_patient], :],\n",
    "                            true_seq_len   = batch[\"attention_mask\"][[idx_patient],:].sum(), \n",
    "                            dm             = dm\n",
    "                        )\n",
    "                        \n",
    "            # Log for plotting\n",
    "            # log_generation(tokens, ages, values, observed_seq_len, dm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate patient timelines for many test patients and condense into files for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Running cr on 72 CPUs and 1 GPUs\n",
      "INFO:root:====================================================================================================\n",
      "INFO:root:# Loading DataModule for dataset /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/. This will be loaded in causal form.\n",
      "INFO:root:====================================================================================================\n",
      "INFO:root:Creating unsupervised collator for DataModule\n",
      "INFO:root:Using meta information from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/meta_information_QuantJenny.pickle\n",
      "INFO:root:Using train file-row count dictionary from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/file_row_count_dict_train.pickle\n",
      "INFO:root:Using test file-row count dictionary from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/file_row_count_dict_test.pickle\n",
      "INFO:root:Using val file-row count dictionary from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/file_row_count_dict_val.pickle\n",
      "INFO:root:Tokenzier created based on 7,555,415,275 tokens\n",
      "INFO:root:Using tabular tokenizer, created from meta information and containing 265 tokens\n",
      "INFO:root:Set seed to 1337\n",
      "INFO:root:Loaded /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/split=train/ dataset, with 23,613,894 samples\n",
      "INFO:root:Set seed to 42\n",
      "INFO:root:Loaded /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/split=test/ dataset, with 1,508,320 samples\n",
      "INFO:root:Set seed to 42\n",
      "INFO:root:Loaded /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/split=val/ dataset, with 1,426,714 samples\n",
      "INFO:root:is_decoder: true\n",
      "data:\n",
      "  batch_size: 64\n",
      "  unk_freq_threshold: 0.0\n",
      "  min_workers: 3\n",
      "  global_diagnoses: false\n",
      "  repeating_events: true\n",
      "  path_to_db: /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/cprd.db\n",
      "  path_to_ds: /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/\n",
      "  meta_information_path: /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/meta_information_QuantJenny.pickle\n",
      "  subsample_training: null\n",
      "experiment:\n",
      "  type: pre-train\n",
      "  project_name: SurvivEHR\n",
      "  run_id: SurvivEHR-cr-small-debug7_exp1000-v1-v4-v1\n",
      "  fine_tune_id: null\n",
      "  notes: null\n",
      "  tags: null\n",
      "  train: false\n",
      "  test: false\n",
      "  verbose: true\n",
      "  seed: 1337\n",
      "  log: false\n",
      "  log_dir: /rds/projects/s/subramaa-mum-predict/CharlesGadd_Oxford/FoundationModelOutput/\n",
      "  ckpt_dir: /rds/projects/s/subramaa-mum-predict/CharlesGadd_Oxford/FoundationModelOutput/checkpoints/\n",
      "fine_tuning:\n",
      "  fine_tune_outcomes: null\n",
      "  custom_outcome_method:\n",
      "    _target_: null\n",
      "  custom_stratification_method:\n",
      "    _target_: null\n",
      "  use_callbacks:\n",
      "    hidden_embedding:\n",
      "      num_batches: 1\n",
      "      mask_static: false\n",
      "      mask_value: false\n",
      "    performance_metrics: true\n",
      "    rmst: false\n",
      "  head:\n",
      "    surv_weight: 1\n",
      "    value_weight: 0\n",
      "    learning_rate: 0.0005\n",
      "optim:\n",
      "  num_epochs: 1\n",
      "  learning_rate: 0.0003\n",
      "  scheduler_warmup: true\n",
      "  scheduler: decaycawarmrestarts\n",
      "  scheduler_periods: 10000\n",
      "  learning_rate_decay: 0.8\n",
      "  val_check_interval: 2500\n",
      "  early_stop: true\n",
      "  early_stop_patience: 30\n",
      "  log_every_n_steps: 20\n",
      "  limit_val_batches: 0.025\n",
      "  limit_test_batches: null\n",
      "  accumulate_grad_batches: 1\n",
      "transformer:\n",
      "  block_type: Neo\n",
      "  block_size: 256\n",
      "  n_layer: 6\n",
      "  n_head: 6\n",
      "  n_embd: 384\n",
      "  layer_norm_bias: false\n",
      "  attention_type: global\n",
      "  bias: true\n",
      "  dropout: 0.0\n",
      "  attention_dropout: 0.0\n",
      "  resid_dropout: 0.0\n",
      "  private_heads: 0\n",
      "  use_fine_tune_adapter: false\n",
      "  adapter_dim: 8\n",
      "head:\n",
      "  SurvLayer: cr\n",
      "  surv_weight: 1\n",
      "  tokens_for_univariate_regression:\n",
      "  - 84\n",
      "  - 86\n",
      "  - 107\n",
      "  - 154\n",
      "  - 42\n",
      "  - 231\n",
      "  - 63\n",
      "  - 85\n",
      "  - 247\n",
      "  - 51\n",
      "  - 103\n",
      "  - 146\n",
      "  - 66\n",
      "  - 172\n",
      "  - 152\n",
      "  - 261\n",
      "  - 235\n",
      "  - 193\n",
      "  - 184\n",
      "  - 101\n",
      "  - 223\n",
      "  - 135\n",
      "  - 226\n",
      "  - 202\n",
      "  - 145\n",
      "  - 244\n",
      "  - 165\n",
      "  - 64\n",
      "  - 185\n",
      "  - 238\n",
      "  - 220\n",
      "  - 232\n",
      "  - 240\n",
      "  - 237\n",
      "  - 17\n",
      "  - 239\n",
      "  - 217\n",
      "  - 118\n",
      "  - 219\n",
      "  - 251\n",
      "  - 25\n",
      "  - 150\n",
      "  - 111\n",
      "  - 102\n",
      "  - 15\n",
      "  - 110\n",
      "  - 130\n",
      "  - 128\n",
      "  - 122\n",
      "  - 99\n",
      "  - 77\n",
      "  - 96\n",
      "  - 140\n",
      "  - 50\n",
      "  - 80\n",
      "  - 108\n",
      "  - 138\n",
      "  - 27\n",
      "  - 139\n",
      "  - 125\n",
      "  - 117\n",
      "  - 105\n",
      "  - 109\n",
      "  - 243\n",
      "  - 236\n",
      "  - 208\n",
      "  - 98\n",
      "  - 176\n",
      "  - 209\n",
      "  - 194\n",
      "  - 61\n",
      "  - 73\n",
      "  - 214\n",
      "  - 206\n",
      "  - 229\n",
      "  - 225\n",
      "  - 183\n",
      "  - 186\n",
      "  - 215\n",
      "  - 187\n",
      "  - 248\n",
      "  - 179\n",
      "  - 163\n",
      "  - 180\n",
      "  - 181\n",
      "  - 153\n",
      "  - 245\n",
      "  - 54\n",
      "  - 246\n",
      "  - 112\n",
      "  - 212\n",
      "  - 141\n",
      "  - 204\n",
      "  - 227\n",
      "  - 173\n",
      "  - 55\n",
      "  - 119\n",
      "  - 262\n",
      "  - 127\n",
      "  - 71\n",
      "  - 59\n",
      "  - 144\n",
      "  - 113\n",
      "  - 170\n",
      "  - 241\n",
      "  - 168\n",
      "  - 47\n",
      "  - 159\n",
      "  value_weight: 0.1\n",
      "\n",
      "INFO:root:====================================================================================================\n",
      "INFO:root:# Pre-training experiment\n",
      "INFO:root:====================================================================================================\n",
      "INFO:root:Loading a pre-trained model with the checkpoint path /rds/projects/s/subramaa-mum-predict/CharlesGadd_Oxford/FoundationModelOutput/checkpoints/SurvivEHR-cr-small-debug7_exp1000-v1-v4-v1.ckpt.\n",
      "INFO:torch.distributed.nn.jit.instantiator:Created a temporary directory at /tmp/tmps97qcq_c\n",
      "INFO:torch.distributed.nn.jit.instantiator:Writing /tmp/tmps97qcq_c/_remote_module_non_scriptable.py\n",
      "INFO:root:Using Temporal Positional Encoding. This module uses the patient's age at an event within their time series.\n",
      "INFO:root:Using a DeSurv Competing-Risk head.\n",
      "INFO:root:\tWith concurrent strategy=add_noise for handling simultaneous events.\n",
      "INFO:root:\tEvaluating on a time grid between [0.0, 1.0] with 1000 intervals\n",
      "INFO:root:Created hidden state embedding callback  \n",
      "INFO:root:Created PerformanceMetrics callback for causal self-supervised tasks.\n",
      "INFO:root:\t \n",
      "INFO:root:Interactive job = True\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/rds/bear-apps/2022a/EL8-ice/software/PyTorch-Lightning/2.1.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:67: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "INFO:root:Creating supervised collator for DataModule\n",
      "INFO:root:Scaling supervised target ages by a factor of 1.0 times the context scale.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model SurvivEHR-cr-small-debug7_exp1000-v1-v4-v1 with 11.20919 M parameters\n",
      "Generating patient timelines for dataset FineTune_MultiMorbidity50+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using meta information from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/meta_information_QuantJenny.pickle\n",
      "INFO:root:Using train file-row count dictionary from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_MultiMorbidity50+/file_row_count_dict_train.pickle\n",
      "INFO:root:Using test file-row count dictionary from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_MultiMorbidity50+/file_row_count_dict_test.pickle\n",
      "INFO:root:Using val file-row count dictionary from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_MultiMorbidity50+/file_row_count_dict_val.pickle\n",
      "INFO:root:Tokenzier created based on 7,555,415,275 tokens\n",
      "INFO:root:Using tabular tokenizer, created from meta information and containing 265 tokens\n",
      "INFO:root:Set seed to 42\n",
      "INFO:root:Loaded /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_MultiMorbidity50+/split=train/ dataset, with 1,650,998 samples\n",
      "INFO:root:Set seed to 42\n",
      "INFO:root:Loaded /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_MultiMorbidity50+/split=test/ dataset, with 107,557 samples\n",
      "INFO:root:Set seed to 42\n",
      "INFO:root:Loaded /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_MultiMorbidity50+/split=val/ dataset, with 91,487 samples\n",
      "1001it [1:53:51,  6.82s/it]                          \n",
      "INFO:root:Creating supervised collator for DataModule\n",
      "INFO:root:Scaling supervised target ages by a factor of 1.0 times the context scale.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating patient timelines for dataset FineTune_CVD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using meta information from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/meta_information_QuantJenny.pickle\n",
      "INFO:root:Using train file-row count dictionary from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/file_row_count_dict_train.pickle\n",
      "INFO:root:Using test file-row count dictionary from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/file_row_count_dict_test.pickle\n",
      "INFO:root:Using val file-row count dictionary from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/file_row_count_dict_val.pickle\n",
      "INFO:root:Tokenzier created based on 7,555,415,275 tokens\n",
      "INFO:root:Using tabular tokenizer, created from meta information and containing 265 tokens\n",
      "INFO:root:Set seed to 42\n",
      "INFO:root:Loaded /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/split=train/ dataset, with 572,096 samples\n",
      "INFO:root:Set seed to 42\n",
      "INFO:root:Loaded /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/split=test/ dataset, with 35,758 samples\n",
      "INFO:root:Set seed to 42\n",
      "INFO:root:Loaded /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/split=val/ dataset, with 33,280 samples\n",
      "100%|██████████| 559/559 [1:03:31<00:00,  6.82s/it]\n",
      "INFO:root:Creating supervised collator for DataModule\n",
      "INFO:root:Scaling supervised target ages by a factor of 1.0 times the context scale.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating patient timelines for dataset FineTune_Hypertension\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using meta information from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/meta_information_QuantJenny.pickle\n",
      "INFO:root:Using train file-row count dictionary from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_Hypertension/file_row_count_dict_train.pickle\n",
      "INFO:root:Using test file-row count dictionary from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_Hypertension/file_row_count_dict_test.pickle\n",
      "INFO:root:Using val file-row count dictionary from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_Hypertension/file_row_count_dict_val.pickle\n",
      "INFO:root:Tokenzier created based on 7,555,415,275 tokens\n",
      "INFO:root:Using tabular tokenizer, created from meta information and containing 265 tokens\n",
      "INFO:root:Set seed to 42\n",
      "INFO:root:Loaded /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_Hypertension/split=train/ dataset, with 572,096 samples\n",
      "INFO:root:Set seed to 42\n",
      "INFO:root:Loaded /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_Hypertension/split=test/ dataset, with 35,758 samples\n",
      "INFO:root:Set seed to 42\n",
      "INFO:root:Loaded /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_Hypertension/split=val/ dataset, with 33,280 samples\n",
      "100%|██████████| 559/559 [1:04:26<00:00,  6.92s/it]\n",
      "INFO:root:Creating unsupervised collator for DataModule\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating patient timelines for dataset PreTrain\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using meta information from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/meta_information_QuantJenny.pickle\n",
      "INFO:root:Using train file-row count dictionary from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/file_row_count_dict_train.pickle\n",
      "INFO:root:Using test file-row count dictionary from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/file_row_count_dict_test.pickle\n",
      "INFO:root:Using val file-row count dictionary from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/file_row_count_dict_val.pickle\n",
      "INFO:root:Tokenzier created based on 7,555,415,275 tokens\n",
      "INFO:root:Using tabular tokenizer, created from meta information and containing 265 tokens\n",
      "INFO:root:Set seed to 42\n",
      "INFO:root:Loaded /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/split=train/ dataset, with 23,613,894 samples\n",
      "INFO:root:Set seed to 42\n",
      "INFO:root:Loaded /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/split=test/ dataset, with 1,508,320 samples\n",
      "INFO:root:Set seed to 42\n",
      "INFO:root:Loaded /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/split=val/ dataset, with 1,426,714 samples\n",
      "  9%|▉         | 53/559 [06:07<58:03,  6.88s/it]  "
     ]
    }
   ],
   "source": [
    "generations_per_patient = 5\n",
    "datasets = [ \"FineTune_MultiMorbidity50+\", \"FineTune_CVD\", \"FineTune_Hypertension\", \"PreTrain\",]\n",
    "# fraction_of_batches = 1.0  # 0.035\n",
    "number_of_batches = 1000\n",
    "\n",
    "for pre_trained_model, config_name in zip(pre_trained_models, config_names):\n",
    "    os.makedirs(f\"figs/generation/{pre_trained_model}/\", exist_ok=True) \n",
    "\n",
    "    # load the configuration file, override any settings \n",
    "    with initialize(version_base=None, config_path=\"../../../confs\", job_name=\"testing_notebook\"):\n",
    "        cfg = compose(config_name=config_name, \n",
    "                      overrides=[# Experiment setup\n",
    "                                 f\"experiment.run_id='{pre_trained_model}'\",\n",
    "                                 \"experiment.train=False\",\n",
    "                                 \"experiment.test=False\",\n",
    "                                 \"experiment.log=False\",\n",
    "                                 # Dataloader\n",
    "                                 \"data.meta_information_path=/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/meta_information_QuantJenny.pickle\",\n",
    "                                 \"data.min_workers=3\",\n",
    "                                ]\n",
    "                     )     \n",
    "    \n",
    "    experiment, dm = run(cfg)     \n",
    "    print(f\"Loaded model {pre_trained_model} with {sum(p.numel() for p in experiment.parameters())/1e6} M parameters\")\n",
    "    \n",
    "    for idx_dataset, dataset in enumerate(datasets):\n",
    "        print(f\"Generating patient timelines for dataset {dataset}\")\n",
    "        \n",
    "        # Create save path\n",
    "        gen_save_path = f'figs/generation/{pre_trained_model}/{dataset}_dataset/'\n",
    "        os.makedirs(gen_save_path, exist_ok=True) \n",
    "\n",
    "        # Load dataset\n",
    "        dm = FoundationalDataModule(path_to_db=cfg.data.path_to_db,\n",
    "                                    path_to_ds=f\"/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/{dataset}/\",\n",
    "                                    overwrite_meta_information=cfg.data.meta_information_path,\n",
    "                                    load=True,\n",
    "                                    supervised=False if dataset.lower()==\"pretrain\" else True,\n",
    "                                    )\n",
    "        \n",
    "        dataset_generated_data = []\n",
    "        # number_of_batches = int(fraction_of_batches * len(dm.test_dataloader())) \n",
    "        number_of_batches = np.min((number_of_batches, len(dm.test_dataloader())))\n",
    "        for idx_batch, batch in tqdm(enumerate(dm.test_dataloader()), total=number_of_batches):\n",
    "            \n",
    "            if idx_batch > number_of_batches:\n",
    "                break\n",
    "\n",
    "            # Put all items on gpu\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            for idx_gen in range(generations_per_patient):\n",
    "\n",
    "                # Generate forward\n",
    "                tokens, ages, values, attention_mask = experiment.model.generate(\n",
    "                    static_covariates=batch[\"static_covariates\"],\n",
    "                    tokens=batch['tokens'],\n",
    "                    ages=batch['ages'],\n",
    "                    values=batch['values'],\n",
    "                    attention_mask=batch['attention_mask'],\n",
    "                    max_new_tokens=5,\n",
    "                    exceed_block_size=True,\n",
    "                    )                \n",
    "\n",
    "                # Report generated outcomes for one patient\n",
    "                # idx_patient = 0\n",
    "                # report_generation(\n",
    "                #     static         = batch[\"static_covariates\"][idx_patient], \n",
    "                #     tokens         = tokens[[idx_patient],:],\n",
    "                #     ages           = ages[[idx_patient],:], \n",
    "                #     values         = values[[idx_patient], :], \n",
    "                #     attention_mask = attention_mask[[idx_patient], :],\n",
    "                #     true_seq_len   = batch[\"attention_mask\"][[idx_patient],:].sum(), \n",
    "                #     dm             = dm\n",
    "                #     )\n",
    "                \n",
    "                # Log for plotting                \n",
    "                observed_seq_lens = batch[\"attention_mask\"].sum(axis=1).long().tolist() \n",
    "                gen_data = log_generation(tokens, ages, values, attention_mask, observed_seq_lens, dm)\n",
    "                \n",
    "                # Record\n",
    "                dataset_generated_data.append(gen_data)\n",
    "\n",
    "        dataset_generated_data = np.concatenate(dataset_generated_data)\n",
    "        dataset_generated_data = pd.DataFrame(dataset_generated_data, columns=[\"Previous event\", \"Next event\", \"Generation step\", \"Age (years)\"])\n",
    "        dataset_generated_data.to_csv(gen_save_path + f'next_event_{dataset}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dataset_generated_data = np.concatenate(dataset_generated_data)\n",
    "# # dataset_generated_data = pd.DataFrame(dataset_generated_data, columns=[\"Previous event\", \"Next token\", \"Generation step\", \"Age (years)\"])\n",
    "# display(dataset_generated_data.head())\n",
    "# print(len(dataset_generated_data))\n",
    "# dataset_generated_data.to_csv(gen_save_path + 'next_event_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise NotImplementedErrror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = next(iter(dm.val_dataloader()))\n",
    "batch = dm.collate_fn.convert_to_supervised(batch, supervised_time_scale=1.0)\n",
    "print(batch.keys())\n",
    "batch_size, seq_len = batch['tokens'].shape\n",
    "print(batch_size)\n",
    "print(seq_len)\n",
    "print(batch[\"tokens\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = \"25_Hydroxyvitamin_D2_level_92\"\n",
    "\n",
    "print(dm.unstandardise(token, 0))\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl \n",
    "print(dm.tokenizer._event_counts[\"FREQUENCY\"].sum())\n",
    "print(dm.tokenizer._event_counts)\n",
    "\n",
    "anxiety_freq = dm.tokenizer._event_counts.filter(pl.col(\"EVENT\") == \"ANXIETY\")[\"FREQUENCY\"].item()\n",
    "display(anxiety_freq)\n",
    "\n",
    "# display([i for i in dm.tokenizer._event_counts[\"EVENT\"] if i.upper() == i ])\n",
    "display(len(dm.tokenizer._event_counts.filter(pl.col(\"FREQUENCY\") >= anxiety_freq)) - 1)  # remove UNK token\n",
    "display(len(dm.tokenizer._event_counts.filter(pl.col(\"FREQUENCY\") < anxiety_freq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(survs[0][0])\n",
    "# print(l)\n",
    "plt.plot(experiment.model.surv_layer.t_eval, survs[0][0][0,:])\n",
    "plt.savefig(\"fig.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check against the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_to_directory = os.getcwd() + \"/../data/\"\n",
    "PATH_TO_DB = \"/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/cprd.db\"\n",
    "\n",
    "collector = SQLiteDataCollector(db_path=PATH_TO_DB)\n",
    "collector.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collector.cursor.execute(\"\"\"SELECT name FROM sqlite_master WHERE type='table' LIMIT 3;\"\"\")   # \n",
    "results = collector.cursor.fetchall()\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SEX                 | F\n",
    "IMD                 | 4.0\n",
    "ETHNICITY           | WHITE\n",
    "birth_year          | 1933.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collector.cursor.execute(\"\"\"SELECT * FROM static_table WHERE sex=='F' AND imd=='4' AND ethnicity=='WHITE' AND YEAR_OF_BIRTH LIKE '1933-%'\"\"\")   # \n",
    "\n",
    "patient_ids = []\n",
    "results = collector.cursor.fetchall()\n",
    "for result in results:\n",
    "    # print(result)\n",
    "    patient_ids.append(result[1])\n",
    "patient_ids_str1 = \", \".join(str(pid) for pid in patient_ids)\n",
    "print(f\"{len(patient_ids)} patients with static match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = f\"\"\"\n",
    "#             SELECT DISTINCT PATIENT_ID\n",
    "#             FROM diagnosis_table\n",
    "#             WHERE EVENT IN ('HYPERTENSION', 'ANY_DEAFNESS_HEARING_LOSS_V2', 'IHDINCLUDINGMI_OPTIMALV2, OSTEOARTHRITIS,TYPE2DIABETES') \n",
    "#                 AND patient_id IN ({patient_ids_str})\n",
    "#             GROUP BY patient_id\n",
    "#             HAVING COUNT(DISTINCT event) >= 5\n",
    "#             \"\"\"\n",
    "\n",
    "query = f\"\"\"SELECT patient_id\n",
    "            FROM diagnosis_table\n",
    "            WHERE patient_id IN ({patient_ids_str1} )\n",
    "            GROUP BY patient_id\n",
    "            HAVING \n",
    "                 COUNT(\n",
    "                      DISTINCT CASE WHEN event IN ('HYPERTENSION', 'ANY_DEAFNESS_HEARING_LOSS_V2', 'IHDINCLUDINGMI_OPTIMALV2', 'OSTEOARTHRITIS', 'TYPE2DIABETES')\n",
    "                                    THEN event\n",
    "                               END\n",
    "                    ) = 5\n",
    "            ORDER BY patient_id;\n",
    "            \"\"\"\n",
    "\n",
    "patient_ids = []\n",
    "collector.cursor.execute(query)   # measurement_ACE_Inhibitors_D2T\n",
    "results = collector.cursor.fetchall()\n",
    "for result in results:\n",
    "    patient_ids.append(result[0])\n",
    "patient_ids_str2 = \", \".join(str(pid) for pid in patient_ids)\n",
    "print(f\"{len(patient_ids)} patients with static match and all of these events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "patient_ids = []\n",
    "collector.cursor.execute(f\"\"\"SELECT * FROM diagnosis_table WHERE patient_id IN ({patient_ids_str2}) ORDER BY patient_id ASC, date ASC\"\"\")   # event=='ALCOHOLMISUSE_V2' AND date LIKE '2008-%' AND \n",
    "results = collector.cursor.fetchall()\n",
    "for result in results:\n",
    "    print(result)\n",
    "    patient_ids.append(result[1])\n",
    "\n",
    "# collector.cursor.execute(\"\"\"SELECT * FROM diagnosis_table WHERE event=='HYPERTENSION' AND date LIKE '2003-%' LIMIT 10\"\"\")   # measurement_ACE_Inhibitors_D2T\n",
    "# results = collector.cursor.fetchall()\n",
    "# for result in results:\n",
    "#     print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_ids_str = \", \".join(str(pid) for pid in patient_ids)\n",
    "collector.cursor.execute(f\"\"\"SELECT * FROM measurement_Systolic_blood_pressure_4 WHERE patient_id == 2666145020970\"\"\")   # 5437879821203\n",
    "results = collector.cursor.fetchall()\n",
    "for result in results:\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collector.cursor.execute(\"\"\"SELECT * FROM static_table WHERE practice_id=='21573' AND patient_id=='6626432621573'\"\"\")   # \n",
    "\n",
    "results = collector.cursor.fetchall()\n",
    "for result in results:\n",
    "    print(result)\n",
    "\n",
    "\n",
    "collector.cursor.execute(\"\"\"SELECT * FROM measurement_Body_mass_index_3 WHERE practice_id=='21573' AND patient_id=='6626432621573'\"\"\")   # \n",
    "\n",
    "results = collector.cursor.fetchall()\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the configuration file, override any settings \n",
    "with initialize(version_base=None, config_path=\"../../confs\", job_name=\"testing_notebook\"):\n",
    "    cfg = compose(config_name=\"config_CompetingRisk11M\", \n",
    "                  # overrides=[\n",
    "                  #     ]\n",
    "                 )\n",
    "\n",
    "# Just load in pretrained model\n",
    "cfg.experiment.train = False\n",
    "cfg.experiment.test = False\n",
    "cfg.experiment.log = False\n",
    "cfg.experiment.run_id=\"CR_11M\"\n",
    "\n",
    "\n",
    "\n",
    "print(OmegaConf.to_yaml(cfg))\n",
    "\n",
    "save_path = f\"/rds/projects/s/subramaa-mum-predict/CharlesGadd_Oxford/FoundationModelOutput/checkpoints/{cfg.experiment.run_id}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " # TODO: define an env variable to fix for a local hpc environment issue, this shouldn't be needed\n",
    "%env SLURM_NTASKS_PER_NODE=28      \n",
    "\n",
    "model, dm = run(cfg)     \n",
    "print(f\"Loaded model with {sum(p.numel() for p in model.parameters())/1e6} M parameters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dm.train_set.view_sample(10, max_dynamic_events=None, report_time=True)\n",
    "\n",
    "# for batch in dm.train_dataloader():\n",
    "#     break\n",
    "# print(batch[\"tokens\"].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation from real prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define encoding functions (TODO: add this wrap to datamodule\n",
    "encode_prompt = lambda prompt_list: torch.from_numpy(np.array(dm.encode(prompt_list)).reshape((1,-1))).to(device)\n",
    "encode_value = lambda prompt_list, value_list: torch.tensor(np.array([dm.standardise(_cat, _val) for _cat, _val in zip(prompt_list, value_list) ]).reshape((1,-1)), dtype=torch.float32).to(device)\n",
    "encode_age = lambda age_list: torch.tensor([365 * _age for _age in age_list], dtype=torch.int64).reshape((1,-1)).to(device)\n",
    "\n",
    "def table(_tokens,_ages,_values):\n",
    "    # print table rows \n",
    "    assert _tokens.shape[0] == 1\n",
    "    assert _ages.shape[0] == 1\n",
    "    assert _values.shape[0] == 1\n",
    "    \n",
    "    for _idx, (_cat, _age, _value) in enumerate(zip(dm.decode(_tokens[0].tolist()).split(\" \"), \n",
    "                                                    _ages[0, :], \n",
    "                                                    _values[0, :]\n",
    "                                                    )\n",
    "                                                ):\n",
    "        _value = dm.unstandardise(_cat, _value)\n",
    "        print(f\"\\t{_cat}\".ljust(60) + f\"{_value:.02f}\".ljust(15) + f\"at age {_age/365:.0f} ({int(_age)} days)\")    # with value {_value}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.meta_information[\"diagnosis_table\"][\"event\"].to_list()\n",
    "dm.meta_information[\"measurement_tables\"][\"event\"].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brute force search, get some prompts from the test dataset which show some different criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexing_conditions_to_pivot_on = \"TYPE2DIABETES\"    # TYPE1DM, HYPERTENSION, OSTEOARTHRITIS, CKDSTAGE3TO5, HF_V3, ISCHAEMICSTROKE_V2, DEPRESSION\n",
    "# exclude_on_events = [\"Statins\",\n",
    "#                      \"Metformin_612_A10BD2\",\n",
    "#                      \"Lipid_lowering_drugs_Optimal\"]\n",
    "\n",
    "indexing_conditions_to_pivot_on = [\"POLYCYSTIC_OVARIAN_SYNDROME_PCOS_V2\",\n",
    "                                   \"COPD\",\n",
    "                                   # \"ENDOMETRIOSIS_ADENOMYOSIS_V2\"\n",
    "                                  ]\n",
    "exclude_on_events = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexing_token_to_pivot_on = dm.encode(indexing_conditions_to_pivot_on)\n",
    "print(indexing_token_to_pivot_on)\n",
    "\n",
    "\n",
    "tokens_to_exclude_on = dm.encode(exclude_on_events)\n",
    "print(tokens_to_exclude_on)\n",
    "\n",
    "patients_satisfying_criteria = []\n",
    "samples_satisfying_criteria = []\n",
    "example_count = 0\n",
    "\n",
    "for _idx, sample in tqdm(enumerate(dm.test_set), total=len(dm.test_set)):\n",
    "\n",
    "    number_of_index_events = sum([tkn for tkn in indexing_token_to_pivot_on if tkn in sample[\"tokens\"]])\n",
    "    \n",
    "    if (len(sample[\"tokens\"]) > 5) and (number_of_index_events==len(indexing_token_to_pivot_on)):\n",
    "\n",
    "        # todo: this is excluded events at any time, change to before the index event\n",
    "        number_of_excluded_events = sum([tkn for tkn in tokens_to_exclude_on if tkn in sample[\"tokens\"]])\n",
    "\n",
    "        if number_of_excluded_events == 0:\n",
    "            patients_satisfying_criteria.append(_idx)\n",
    "            samples_satisfying_criteria.append(sample)\n",
    "\n",
    "            if example_count >= 4:\n",
    "                break\n",
    "            else:\n",
    "                example_count += 1\n",
    "                print(example_count)\n",
    "\n",
    "    # elif _idx > 100000:\n",
    "    #     break\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(patients_satisfying_criteria)\n",
    "# patients_satisfying_criteria = [724, 1760, 2055, 2099, 2167]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _patient_idx in patients_satisfying_criteria:\n",
    "    print(_patient_idx)\n",
    "\n",
    "    # Get the sample\n",
    "    sample = dm.test_set[_patient_idx]\n",
    "    _index = (sample[\"tokens\"] == indexing_token_to_pivot_on).nonzero(as_tuple=True)[0].item()\n",
    "\n",
    "    # chunk by day\n",
    "    _day_at_index = int(sample[\"ages\"][_index])\n",
    "    _index_pre = sum(sample[\"ages\"] < _day_at_index)\n",
    "    _index_inc = sum(sample[\"ages\"] <= _day_at_index)\n",
    "    \n",
    "    for _phase, _split_at in enumerate([_index_pre, _index_inc]):\n",
    "\n",
    "        if _phase == 0:\n",
    "            print(f\"\\n\\nBefore {dm.decode([_token]).lower()} is seen in the medical history\")\n",
    "        else:\n",
    "            print('\\n------------------------------------ page break ------------------------------------')\n",
    "            print(f\"\\n\\nAfter the diagnosis of {dm.decode([_token]).lower()} is then seen in the medical history\")\n",
    "\n",
    "        _covariates = sample[\"static_covariates\"].reshape((1,-1))\n",
    "        _tokens = sample[\"tokens\"][:_split_at].reshape((1,-1))\n",
    "        _ages = sample[\"ages\"][:_split_at].reshape((1,-1))\n",
    "        _values = sample[\"values\"][:_split_at].reshape((1,-1))\n",
    "\n",
    "        # Report the initial part of their historical context\n",
    "        _dec_covariates = dm.train_set._decode_covariates(_covariates)\n",
    "        print(f\"\\n\\nMedical history of a \\n\\t\" + \\\n",
    "                        f\"{_dec_covariates['ETHNICITY'][0].lower()}, \" + \\\n",
    "                        f\"{'male' if _dec_covariates['SEX'][0] == 'M' else 'female'} patient, \" + \\\n",
    "                        f\"born in {int(_dec_covariates['birth_year'][0])}, \" + \\\n",
    "                        f\"with IMD (deprivation) level {int(_dec_covariates['IMD'][0])}. \\n\\n\" \n",
    "              )\n",
    "        table(_tokens, _ages, _values)\n",
    "\n",
    "        \n",
    "        # Predict the future and report\n",
    "        new_tokens, new_ages, new_values = model.generate(_tokens.to(device), _ages.to(device), _values.to(device), _covariates.to(device), max_new_tokens=20)\n",
    "        print(f\"\"\"\\nSurvivEHR then predicts the next events to be:\n",
    "               \"\"\")\n",
    "        table(new_tokens[:, _tokens.shape[1]:].reshape((1,-1)), \n",
    "              new_ages[:, _tokens.shape[1]:].reshape((1,-1)),\n",
    "              new_values[:, _tokens.shape[1]:].reshape((1,-1))\n",
    "             )\n",
    "\n",
    "    print('\\n----------------------------------------------------------------------------------------')\n",
    "    print('------------------------------------ document break ------------------------------------')\n",
    "    print('----------------------------------------------------------------------------------------')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.test_set.view_sample(patients_satisfying_criteria[1], max_dynamic_events=None, report_time=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation from fixed prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= model.to(device)\n",
    "\n",
    "baseline_covariates = {\"sex\": \"F\", \"deprivation\": 5.0, \"ethnicity\": \"WHITE\", \"year_of_birth\": 1997-65}\n",
    "\n",
    "multimorbidity_conditions = [\"TYPE2DIABETES\", \"Metformin_612_A10BD2\", \"DEPRESSION\", \"IHDINCLUDINGMI_OPTIMALV2\", \"COPD\"]\n",
    "at_ages = [40, 40, 44, 49, 65]\n",
    "\n",
    "prompt, ages_in_years, values = [], [], []\n",
    "\n",
    "for condition, age in zip(multimorbidity_conditions, at_ages):\n",
    "    # Default context start\n",
    "    prompt.append(condition)\n",
    "    ages_in_years.append(age)\n",
    "    values.append(np.nan)\n",
    "\n",
    "    # Convert for model\n",
    "    covariates = dm.train_set._encode_covariates(**baseline_covariates).reshape(1,-1).to(device)\n",
    "    tokens = encode_prompt(prompt)\n",
    "    values_scaled = encode_value(prompt, values)\n",
    "    ages_in_days = encode_age(ages_in_years)\n",
    "\n",
    "    # generate: sample the next 10 tokens\n",
    "    new_tokens, new_ages, new_values = model.generate(tokens, ages_in_days, values_scaled, covariates, max_new_tokens=10)\n",
    "    \n",
    "    # report:\n",
    "    print(f\"Baseline covariates: \\n{baseline_covariates}\\n\" + \"=\"*90)\n",
    "    print(f\"PROMPT:\")\n",
    "    for _idx, (_cat, _age, _value) in enumerate(zip(dm.decode(new_tokens[0].tolist()).split(\" \"), \n",
    "                                                    new_ages[0, :], \n",
    "                                                    new_values[0, :]\n",
    "                                                   )\n",
    "                                               ):\n",
    "        _value = dm.unstandardise(_cat, _value)\n",
    "        print(f\"{_cat}\".ljust(50) + f\"{_value:.02f}\".ljust(15) + f\"at age {_age/365:.0f} ({int(_age)} days)\")    # with value {_value}\n",
    "        if _idx == tokens.shape[-1] - 1:\n",
    "            print(\"=\"*90)\n",
    "            print(f\"GENERATION\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generate: sample the next 10 tokens\n",
    "new_tokens, new_ages, new_values = model.generate(tokens, ages_in_days, values_scaled, covariates, max_new_tokens=50)\n",
    "\n",
    "# report:\n",
    "print(f\"Baseline covariates: \\n{baseline_covariates}\\n\" + \"=\"*90)\n",
    "print(f\"PROMPT:\")\n",
    "for _idx, (_cat, _age, _value) in enumerate(zip(dm.decode(new_tokens[0].tolist()).split(\" \"), \n",
    "                                                new_ages[0, :], \n",
    "                                                new_values[0, :]\n",
    "                                               )\n",
    "                                           ):\n",
    "    # _value = dm.unstandardise(_cat, _value)\n",
    "    print(f\"{_cat}\".ljust(50) + f\"{_value:.02f}\".ljust(15) + f\"at age {_age/365:.0f} ({int(_age)} days)\")    # with value {_value}\n",
    "    if _idx == tokens.shape[-1] - 1:\n",
    "        print(\"=\"*90)\n",
    "        print(f\"GENERATION\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnoses: How related conditions are impacted by each other - multi-morbidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_prompts = [[\"TYPE2DIABETES\", \"Metformin_612_A10BD2\"],\n",
    "               [\"TYPE2DIABETES\", \"Metformin_612_A10BD2\", \"DEPRESSION\",],\n",
    "               [\"TYPE2DIABETES\", \"Metformin_612_A10BD2\", \"DEPRESSION\", \"IHDINCLUDINGMI_OPTIMALV2\"],\n",
    "               [\"TYPE2DIABETES\", \"Metformin_612_A10BD2\", \"DEPRESSION\", \"IHDINCLUDINGMI_OPTIMALV2\", \"COPD\"],\n",
    "              ]\n",
    "exp_promps_lbl = [\"T2D+Metformin\", \"+ Depression\", \"+IHD/MI\", \"+COPD\"]\n",
    "exp_ages = [[40, 40],\n",
    "            [40, 40, 44],\n",
    "            [40, 40, 44, 49],\n",
    "            [40, 40, 44, 49, 65],\n",
    "           ]\n",
    "exp_values = [[np.nan, np.nan],\n",
    "              [np.nan, np.nan, np.nan],\n",
    "              [np.nan, np.nan, np.nan, np.nan],\n",
    "              [np.nan, np.nan, np.nan, np.nan, np.nan],\n",
    "              ]\n",
    "\n",
    "with torch.no_grad(): \n",
    "    model.eval()\n",
    "\n",
    "    _exp_survs = []\n",
    "    for p_idx, (_exp_prompt, _exp_age, _exp_value) in enumerate(zip(exp_prompts, \n",
    "                                                                    exp_ages, \n",
    "                                                                    exp_values)):\n",
    "\n",
    "        _tokens = encode_prompt(_exp_prompt)\n",
    "        _values_scaled = encode_value(_exp_prompt, _exp_value)\n",
    "        _ages_in_days = encode_age(_exp_age)\n",
    "        \n",
    "        outputs, _, _ = model(_tokens,\n",
    "                              values=_values_scaled,\n",
    "                              ages=_ages_in_days,\n",
    "                              covariates=covariates,\n",
    "                              is_generation=True,\n",
    "                              return_loss=False,\n",
    "                              return_generation=True)\n",
    "        surv = outputs[\"surv\"][\"surv_CDF\"]\n",
    "        _exp_survs.append(surv)\n",
    "\n",
    "    for si, _ in enumerate(surv):\n",
    "        plt.close()\n",
    "        event_name = dm.decode([si + 1])\n",
    "        for p_idx in range(len(exp_prompts)):\n",
    "            plt.plot(model.surv_layer.t_eval / 365, _exp_survs[p_idx][si][0, :], label=f\"{exp_promps_lbl[p_idx]}\")\n",
    "        plt.xlabel(\"Time (years)\")\n",
    "        plt.ylabel(f\"$P(T>t)$ ({event_name})\")\n",
    "        plt.legend()\n",
    "        plt.savefig(save_path + f\"multimorbidity/{event_name}.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnoses: How related conditions are impacted by each other\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_prompts = [[\"DEPRESSION\"], [\"TYPE1DM\"], [\"TYPE2DIABETES\"], [\"Never_smoked_tobacco_85\"], [\"Ex_smoker_84\"]]\n",
    "exp_ages = [[20] for _ in range(len(exp_prompts))]\n",
    "exp_values = [[np.nan] for _ in range(len(exp_prompts))]\n",
    "\n",
    "with torch.no_grad(): \n",
    "    model.eval()\n",
    "\n",
    "    _exp_survs = []\n",
    "    for p_idx, (_exp_prompt, _exp_age, _exp_value) in enumerate(zip(exp_prompts, \n",
    "                                                                    exp_ages, \n",
    "                                                                    exp_values)):\n",
    "\n",
    "        _tokens = encode_prompt(_exp_prompt)\n",
    "        _values_scaled = encode_value(_exp_prompt, _exp_value)\n",
    "        _ages_in_days = encode_age(_exp_age)\n",
    "        \n",
    "        outputs, _, _ = model(_tokens,\n",
    "                              values=_values_scaled,\n",
    "                              ages=_ages_in_days,\n",
    "                              covariates=covariates,\n",
    "                              is_generation=True,\n",
    "                              return_loss=False,\n",
    "                              return_generation=True)\n",
    "        surv = outputs[\"surv\"][\"surv_CDF\"]\n",
    "        _exp_survs.append(surv)\n",
    "\n",
    "    for si, _ in enumerate(surv):\n",
    "        plt.close()\n",
    "        event_name = dm.decode([si + 1])\n",
    "        for p_idx in range(len(exp_prompts)):\n",
    "            plt.plot(model.surv_layer.t_eval / 365, _exp_survs[p_idx][si][0, :], label=f\"{'->'.join(exp_prompts[p_idx]).lower()}\")\n",
    "        plt.xlabel(\"Time (years)\")\n",
    "        plt.ylabel(f\"$P(T>t)$ ({event_name})\")\n",
    "        plt.legend()\n",
    "        plt.savefig(save_path + f\"diabetes/{event_name}.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Values: How increasing BMI affects diagnosis risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_of_interest = [\"Body_mass_index_3\", \"Diastolic_blood_pressure_5\", \n",
    "                      \"TYPE1DM\", \"TYPE2DIABETES\",\n",
    "                      \"HYPERTENSION\", \"OSTEOARTHRITIS\",\n",
    "                      \"CKDSTAGE3TO5\",\n",
    "                      \"HF_V3\", \"ISCHAEMICSTROKE_V2\",\n",
    "                      \"DEATH\"\n",
    "                     ]\n",
    "\n",
    "_exp_prompt = [\"Body_mass_index_3\"]\n",
    "_exp_age = [40]\n",
    "_exp_values = [[18.], [21.], [24.], [30.], [40.]]\n",
    "\n",
    "with torch.no_grad(): \n",
    "    model.eval()\n",
    "\n",
    "    _exp_survs = []\n",
    "    for p_idx, _exp_value in enumerate(_exp_values):\n",
    "\n",
    "        _tokens = encode_prompt(_exp_prompt)\n",
    "        _values_scaled = encode_value(_exp_prompt, _exp_value)\n",
    "        _ages_in_days = encode_age(_exp_age)\n",
    "\n",
    "        outputs, _, _ = model(_tokens,\n",
    "                              values=_values_scaled,\n",
    "                              ages=_ages_in_days,\n",
    "                              covariates=covariates,\n",
    "                              is_causal=False,\n",
    "                              return_loss=False,\n",
    "                              return_generation=True)\n",
    "        surv = outputs[\"surv\"][\"surv_CDF\"]\n",
    "        _exp_survs.append(surv)\n",
    "\n",
    "    for si, _ in enumerate(surv):\n",
    "        plt.close()\n",
    "        event_name = dm.decode([si + 1])\n",
    "        if event_name in events_of_interest:\n",
    "            for p_idx in range(len(_exp_values)):\n",
    "                plt.plot(model.surv_layer.t_eval / 365, _exp_survs[p_idx][si][0, :], label=f\"{_exp_values[p_idx][0]:.2f}\")\n",
    "            plt.xlabel(\"t (years)\")\n",
    "            plt.ylabel(f\"$P(T>t)$ ({event_name})\")\n",
    "            plt.legend()\n",
    "            plt.savefig(save_path + f\"bmi/{event_name}.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Values: How increasing DBP affects diagnosis risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_of_interest = [\"Body_mass_index_3\", \"Diastolic_blood_pressure_5\", \n",
    "                      \"TYPE1DM\", \"TYPE2DIABETES\",\n",
    "                      \"HYPERTENSION\", \"OSTEOARTHRITIS\",\n",
    "                      \"CKDSTAGE3TO5\",\n",
    "                      \"HF_V3\", \"ISCHAEMICSTROKE_V2\",\n",
    "                      \"DEATH\"\n",
    "                     ]\n",
    "\n",
    "\n",
    "_exp_prompt = [\"Diastolic_blood_pressure_5\"]\n",
    "_exp_age = [40]\n",
    "_exp_values = [[60.], [70.], [80.], [90.], [100.], [110.]]\n",
    "\n",
    "with torch.no_grad(): \n",
    "    model.eval()\n",
    "\n",
    "    _exp_survs = []\n",
    "    for p_idx, _exp_value in enumerate(_exp_values):\n",
    "\n",
    "        _tokens = encode_prompt(_exp_prompt)\n",
    "        _values_scaled = encode_value(_exp_prompt, _exp_value)\n",
    "        _ages_in_days = encode_age(_exp_age)\n",
    "\n",
    "        outputs, _, _ = model(_tokens,\n",
    "                              values=_values_scaled,\n",
    "                              ages=_ages_in_days,\n",
    "                              covariates=covariates,\n",
    "                              is_causal=False,\n",
    "                              return_loss=False,\n",
    "                              return_generation=True\n",
    "                             )\n",
    "        surv = outputs[\"surv\"][\"surv_CDF\"]\n",
    "        _exp_survs.append(surv)\n",
    "\n",
    "    for si, _ in enumerate(surv):\n",
    "        plt.close()\n",
    "        event_name = dm.decode([si + 1])\n",
    "        if event_name in events_of_interest:\n",
    "            for p_idx in range(len(_exp_values)):\n",
    "                plt.plot(model.surv_layer.t_eval / 365, _exp_survs[p_idx][si][0, :], label=f\"{_exp_values[p_idx][0]:.2f}\")\n",
    "            plt.xlabel(\"t (years)\")\n",
    "            plt.ylabel(\"P(T>t) ()\")\n",
    "            plt.legend()\n",
    "            plt.savefig(save_path + f\"diastolic_blood_pressure/{event_name}.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Values: How varying diagnosis affects value of DBP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements_of_interest = \"Diastolic_blood_pressure_5\"\n",
    "\n",
    "\n",
    "_exp_prompts = [[\"DEPRESSION\"], [\"TYPE2DIABETES\"], [\"HF_V3\"], [\"HYPERTENSION\"]]\n",
    "_exp_age = [20]\n",
    "_exp_value = [np.nan]\n",
    "\n",
    "\n",
    "with torch.no_grad(): \n",
    "    model.eval()\n",
    "\n",
    "    for p_idx, _exp_prompt in enumerate(_exp_prompts):\n",
    "\n",
    "        _tokens = encode_prompt(_exp_prompt)\n",
    "        _values_scaled = encode_value(_exp_prompt, _exp_value)\n",
    "        _ages_in_days = encode_age(_exp_age)\n",
    "        \n",
    "        outputs, _, _ = model(_tokens,\n",
    "                              values=_values_scaled,\n",
    "                              ages=_ages_in_days,\n",
    "                              covariates=covariates,\n",
    "                              is_causal=False,\n",
    "                              return_loss=False,\n",
    "                              return_generation=True\n",
    "                             )\n",
    "        val_dist = outputs[\"values_dist\"]\n",
    "\n",
    "        dist = val_dist[model.value_layer.token_key(dm.tokenizer._stoi[measurements_of_interest])]\n",
    "        print(f\"{'->'.join(_exp_prompt)}\".ljust(30) + \"leads to\".ljust(20) + f\"standardised {measurements_of_interest} ~ N({dist.loc.item():.1f}, {dist.scale.item():.1f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Values: How increasing bmi affects value of diastolic_blood_pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements_of_interest = \"Diastolic_blood_pressure_5\"\n",
    "\n",
    "\n",
    "_exp_prompt = [\"Body_mass_index_3\"]\n",
    "_exp_values = [[18.], [21.], [24.], [30.], [40.]]\n",
    "_exp_value = [np.nan]\n",
    "\n",
    "\n",
    "with torch.no_grad(): \n",
    "    model.eval()\n",
    "\n",
    "    for p_idx, _exp_value in enumerate(_exp_values):\n",
    "\n",
    "        _tokens = encode_prompt(_exp_prompt)\n",
    "        _values_scaled = encode_value(_exp_prompt, _exp_value)\n",
    "        _ages_in_days = encode_age(_exp_age)\n",
    "        \n",
    "        outputs, _, _ = model(_tokens,\n",
    "                              values=_values_scaled,\n",
    "                              ages=_ages_in_days,\n",
    "                              covariates=covariates,\n",
    "                              is_causal=False,\n",
    "                              return_loss=False,\n",
    "                              return_generation=True\n",
    "                             )\n",
    "        val_dist = outputs[\"values_dist\"]\n",
    "\n",
    "        dist = val_dist[model.value_layer.token_key(dm.tokenizer._stoi[measurements_of_interest])]\n",
    "        print(f\"{'->'.join(_exp_prompt)} of {_exp_value[0]}\".ljust(30) + \"leads to\".ljust(20) + f\"standardised {measurements_of_interest} ~ N({dist.loc.item():.1f}, {dist.scale.item():.1f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline, impact of gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_of_interest = [\"Body_mass_index_3\", \"Diastolic_blood_pressure_5\", \n",
    "                      \"TYPE1DM\", \"TYPE2DIABETES\",\n",
    "                      \"HYPERTENSION\", \"OSTEOARTHRITIS\",\n",
    "                      \"CKDSTAGE3TO5\",\n",
    "                      \"HF_V3\", \"ISCHAEMICSTROKE_V2\",\n",
    "                      \"POLYCYSTIC_OVARIAN_SYNDROME_PCOS_V2\",\n",
    "                      \"DEATH\",\n",
    "                      \"COCP_reg_contraception\",\n",
    "                      \"all_contraceptive\"\n",
    "                     ]\n",
    "\n",
    "_genders = [\"M\", \"F\", \"I\"]\n",
    "_exp_prompt = [\"Diastolic_blood_pressure_5\"]\n",
    "_exp_age = [20]\n",
    "_exp_value = [90.]\n",
    "\n",
    "with torch.no_grad(): \n",
    "    model.eval()\n",
    "\n",
    "    _exp_survs = []\n",
    "    for p_idx, _gender in enumerate(_genders):\n",
    "\n",
    "        _baseline_covariate = {\"sex\": _gender, \"deprivation\": 4.0, \"ethnicity\": \"WHITE\", \"year_of_birth\": 1997}\n",
    "        _covariates = dm.train_set._encode_covariates(**_baseline_covariate).reshape(1,-1).to(device)\n",
    "        _tokens = encode_prompt(_exp_prompt)\n",
    "        _values_scaled = encode_value(_exp_prompt, _exp_value)\n",
    "        _ages_in_days = encode_age(_exp_age)\n",
    "\n",
    "        outputs, _, _ = model(_tokens,\n",
    "                              values=_values_scaled,\n",
    "                              ages=_ages_in_days,\n",
    "                              covariates=_covariates,\n",
    "                              is_causal=False,\n",
    "                              return_loss=False,\n",
    "                              return_generation=True\n",
    "                             )        \n",
    "        surv = outputs[\"surv\"][\"surv_CDF\"]\n",
    "        _exp_survs.append(surv)\n",
    "\n",
    "    for si, _ in enumerate(surv):\n",
    "        plt.close()\n",
    "        event_name = dm.decode([si + 1])\n",
    "        if event_name in events_of_interest:\n",
    "            for p_idx in range(len(_genders)):\n",
    "                plt.plot(model.surv_layer.t_eval / 365, _exp_survs[p_idx][si][0, :], label=f\"{_genders[p_idx]}\")\n",
    "            plt.xlabel(\"t (years)\")\n",
    "            plt.ylabel(\"P(T>t) ()\")\n",
    "            plt.legend()\n",
    "            plt.savefig(save_path + f\"gender/{event_name}.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: model architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to html --no-input 2_generation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "sys_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
