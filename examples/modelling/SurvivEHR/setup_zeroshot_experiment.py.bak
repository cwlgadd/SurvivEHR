import pytorch_lightning as pl
import torch
import logging
from CPRD.src.models.survival.task_heads.causal import SurvStreamGPTForCausalModelling
# from CPRD.data.foundational_loader import convert_batch_to_none_causal
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, ReduceLROnPlateau, CosineAnnealingLR, LambdaLR, SequentialLR, ChainedScheduler
from CPRD.src.models.base_callback import Embedding
from CPRD.src.models.survival.custom_callbacks.clinical_prediction_model import PerformanceMetrics

import os
# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'

class ZeroShotExperiment(pl.LightningModule):
    """

    Note:
        - The losses returned by this experiment class are those of each sequence transition, with the outcome removed.
             This may be useful for assessing distributional shift, but it is not a test of outcome prediction performance.
    """

    def __init__(self,
                 cfg,
                 vocab_size
                ):
        
        super().__init__()
        self.save_hyperparameters()
        self.cfg = cfg
        self.model = SurvStreamGPTForCausalModelling(cfg, vocab_size)

    def forward(self, batch, is_generation=False, return_loss=True, return_generation=False):
        # Because of how DeSurv is coded we have the loss returned in the forward, so we have some redundancy

        # Convert the batch from causal to supervised (if not already converted. 
        # ... If we call this _pl_module from inside a callback, we will duplicate conversion, this is supressed inside of the method
        # batch = convert_batch_to_none_causal(batch)
        
        tokens = batch['tokens'].to(self.device)
        ages = batch['ages'].to(self.device)
        values = batch['values'].to(self.device)
        covariates = batch["static_covariates"].to(self.device)
        attention_mask = batch['attention_mask'].to(self.device)   

        return self.model(tokens,
                          ages,
                          values,
                          covariates,
                          attention_mask,
                          is_generation=is_generation,
                          return_loss=return_loss,
                          return_generation=return_generation
                          )

    def training_step(self, batch, batch_idx):
        _, loss_dict, _ = self(batch)        
        for _key in loss_dict.keys():
            self.log(f"train_" + _key, loss_dict[_key], prog_bar=False, logger=True, sync_dist=True)
        return loss_dict['loss'] 

    def validation_step(self, batch, batch_idx):
        _, loss_dict, _ = self(batch)        
        for _key in loss_dict.keys():
            self.log(f"val_" + _key, loss_dict[_key], prog_bar=False, logger=True, sync_dist=True)
        return loss_dict['loss'] 

    def test_step(self, batch, batch_idx):
        _, loss_dict, _ = self(batch)        
        for _key in loss_dict.keys():
            self.log(f"test_" + _key, loss_dict[_key], prog_bar=False, logger=True, sync_dist=True)
        return loss_dict['loss'] 

    def configure_optimizers(self):
        optimizer = torch.optim.AdamW(self.parameters(), lr=self.cfg.optim.learning_rate)

        freq = 1
        match self.cfg.optim.scheduler.lower():
            case 'cawarmrestarts':
                scheduler = CosineAnnealingWarmRestarts(optimizer, 
                                                        T_0=int(self.cfg.optim.scheduler_periods),
                                                        T_mult=2,
                                                        eta_min=self.cfg.optim.learning_rate / 5)
            case 'cosineannealinglr':
                scheduler = CosineAnnealingLR(optimizer,
                                              T_max=self.cfg.optim.lr_cosine_decay_period / self.cfg.data.batch_size,
                                              eta_min=self.cfg.optim.learning_rate / 5)
            case _:
                scheduler = ReduceLROnPlateau(optimizer)
                freq = self.cfg.optim.val_check_interval

        if self.cfg.optim.scheduler_warmup:
            # Create scheduler with linear warmup followed by Cosine Annealing with warm restarts.
            warmup = int(self.cfg.optim.scheduler_periods)
            lambda1 = lambda step: float(step) / warmup if step < warmup else 1
            scheduler_warm = LambdaLR(optimizer, lr_lambda=lambda1)
            scheduler = SequentialLR(optimizer, schedulers=[scheduler_warm, scheduler], milestones=[warmup])      
            
        lr_scheduler_config = {
            "frequency": freq,                                                          # How many epochs/steps should pass between calls to `scheduler.step()`
            "scheduler": scheduler,                                                     # The scheduler instance
            "interval": "step",                                                         # The unit of the scheduler's step size
            "frequency": 1,                                                             # How many epochs/steps should pass between calls to `scheduler.step()`
            "monitor": "val_loss",                                                      # Metric to monitor for scheduler, if needed
            "strict": False,                                                            # Enforce that "val_loss" is available when the scheduler is updated
            "name": 'Scheduler',                                                        # For `LearningRateMonitor`, specify a custom logged name
        }
        return {
            "optimizer": optimizer,
            "lr_scheduler": lr_scheduler_config
        }

def setup_zeroshot_experiment(checkpoint, cfg, dm):

    zero_shot_experiment = ZeroShotExperiment.load_from_checkpoint(checkpoint)
    logging.debug(zero_shot_experiment)

    # Initialize wandb logger
    if cfg.experiment.log == True:
        logger = pl.loggers.WandbLogger(project=cfg.experiment.project_name,
                                        name=cfg.experiment.run_id,
                                        save_dir=cfg.experiment.log_dir
                                        )
    else:
        logger = None

    # Make all callbacks
    val_batch = next(iter(dm.val_dataloader()))
    test_batch = next(iter(dm.test_dataloader()))
    callbacks = []
    
    # Hidden state embedding
    if True:
        logging.info("Creating hidden state embedding callback")
        embedding_callback = Embedding(val_batch=val_batch,
                                       test_batch=test_batch
                                      )
        callbacks.append(embedding_callback)

    # Add callbacks which apply to outcome prediction tasks
    if (cfg.experiment.fine_tune_outcomes is not None) and (cfg.experiment.log is True):  
        logging.info("Creating Clinical Prediction Model callbacks")
        logging.info(f"Using outcomes {cfg.experiment.fine_tune_outcomes}")
        outcome_tokens =  dm.encode(cfg.experiment.fine_tune_outcomes)
        metric_callback = PerformanceMetrics(outcome_tokens=outcome_tokens,
                                             log_ctd=True,
                                             log_ibs=True,
                                             log_inbll=True
                                             )
        # callbacks.append(metric_callback)
    else:
        logging.warning("To do zero-shot evaluation you must supply the outcomes, and set logging to be true")

    _trainer = pl.Trainer(
        logger=logger,
        callbacks=callbacks,
        max_epochs=cfg.optim.num_epochs,
        log_every_n_steps=cfg.optim.log_every_n_steps,
        val_check_interval=cfg.optim.val_check_interval,
        limit_val_batches=cfg.optim.limit_val_batches,
        limit_test_batches=cfg.optim.limit_test_batches,
        # devices=torch.cuda.device_count(),
        # gradient_clip_val=0.5
    )

    return zero_shot_experiment, ZeroShotExperiment, _trainer