{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJpXpmjEYC_T"
   },
   "source": [
    "# Demo Notebook:\n",
    "## Time to Event Transformer For Causal Sequence Modelling \n",
    "\n",
    "Including time, and excluding values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added path '/rds/homes/g/gaddcz/Projects/CPRD/virtual-env-icelake/lib/python3.10/site-packages' at start of search paths.\n",
      "env: SQLITE_TMPDIR=/rds/projects/g/gokhalkm-optimal/DataforCharles\n",
      "env: TMPDIR=/rds/projects/g/gokhalkm-optimal/DataforCharles\n",
      "/rds/projects/g/gokhalkm-optimal/DataforCharles\n",
      "/rds/projects/g/gokhalkm-optimal/DataforCharles\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "node_type = os.getenv('BB_CPU')\n",
    "venv_dir = f'/rds/homes/g/gaddcz/Projects/CPRD/virtual-env-{node_type}'\n",
    "venv_site_pkgs = Path(venv_dir) / 'lib' / f'python{sys.version_info.major}.{sys.version_info.minor}' / 'site-packages'\n",
    "if venv_site_pkgs.exists():\n",
    "    sys.path.insert(0, str(venv_site_pkgs))\n",
    "    print(f\"Added path '{venv_site_pkgs}' at start of search paths.\")\n",
    "else:\n",
    "    print(f\"Path '{venv_site_pkgs}' not found. Check that it exists and/or that it exists for node-type '{node_type}'.\")\n",
    "\n",
    "# Perform sqlite operations on disk\n",
    "%env SQLITE_TMPDIR=/rds/projects/g/gokhalkm-optimal/DataforCharles\n",
    "%env TMPDIR=/rds/projects/g/gokhalkm-optimal/DataforCharles\n",
    "!echo $SQLITE_TMPDIR\n",
    "!echo $TMPDIR\n",
    "!echo $USERPROFILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "/rds/homes/g/gaddcz/Projects/CPRD/examples/modelling/tteGPT\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning \n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sqlite3\n",
    "from dataclasses import dataclass\n",
    "import logging\n",
    "from CPRD.data.foundational_loader import FoundationalDataModule\n",
    "from CPRD.src.models.TTE.task_heads.causal import TTETransformerForCausalSequenceModelling\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "# TODO:\n",
    "# replace experiment boilerplate with pytorch lightning\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "# device = \"cpu\"    # if more informative debugging statements are needed\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set config to be equivalent architecture of kaparthy benchmark, however they are not comparable tasks.\n",
    "@dataclass\n",
    "class DemoConfig:\n",
    "    block_size: int = 256        # what is the maximum context length for predictions?\n",
    "    n_layer: int = 6\n",
    "    n_head: int = 6\n",
    "    n_embd: int = 384\n",
    "    bias: bool = True\n",
    "    attention_type: str = \"global\"    \n",
    "    dropout: float = 0.0\n",
    "    unk_freq_threshold: float = 0.0\n",
    "    TTELayer = \"Exponential\"       # alternatively \"Geometric\"\n",
    "\n",
    "config = DemoConfig()\n",
    "\n",
    "@dataclass\n",
    "class OptConfig:\n",
    "    batch_size: int = 64\n",
    "    eval_interval: int = 1\n",
    "    learning_rate: float = 3e-4\n",
    "    epochs: int = 3\n",
    "    \n",
    "opt = OptConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data loader on a reduced cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading Polars dataset from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/polars/\n",
      "INFO:root:Using tokenizer tabular\n",
      "INFO:root:Tokenzier created based on 3584.43M tokens\n",
      "INFO:root:Creating split=train/ dataset\n",
      "INFO:root:\t Loading split=train/ hash map for parquet\n",
      "INFO:root:\t Hash map created for split=train/ with 22,912,046 samples\n",
      "INFO:root:Creating split=test/ dataset\n",
      "INFO:root:\t Loading split=test/ hash map for parquet\n",
      "INFO:root:\t Hash map created for split=test/ with 1,207,449 samples\n",
      "INFO:root:Creating split=val/ dataset\n",
      "INFO:root:\t Loading split=val/ hash map for parquet\n",
      "INFO:root:\t Hash map created for split=val/ with 1,226,576 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184 vocab elements\n"
     ]
    }
   ],
   "source": [
    "# Get a list of patients which fit a reduced set of criterion\n",
    "# path_to_db = \"/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/archive/Version2/\"\n",
    "path_to_db = \"/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/\"\n",
    "\n",
    "# Build \n",
    "dm = FoundationalDataModule(path_to_db=path_to_db,\n",
    "                            load=True,\n",
    "                            tokenizer=\"tabular\",\n",
    "                            batch_size=opt.batch_size,\n",
    "                            max_seq_length=config.block_size,\n",
    "                            unk_freq_threshold=config.unk_freq_threshold,\n",
    "                            min_workers=20,\n",
    "                           )\n",
    "\n",
    "vocab_size = dm.train_set.tokenizer.vocab_size\n",
    "print(f\"{vocab_size} vocab elements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View a single patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to retrieve sample index 1 was 0.12956857681274414 seconds\n",
      "\n",
      "SEX                 | F\n",
      "IMD                 | 4.0\n",
      "ETHNICITY           | WHITE\n",
      "birth_year          | 1997.0\n",
      "\n",
      "Token                                                                      | Age               | Standardised value\n",
      "===================================================================================================================\n",
      "O_E___weight_2                                                             | 5687              | 0.18              \n",
      "Basophil_count_22                                                          | 5697              | 0.14              \n",
      "Eosinophil_count_21                                                        | 5697              | 0.24              \n",
      "Haematocrit_15                                                             | 5697              | -0.04             \n",
      "Haemoglobin_estimation_9                                                   | 5697              | nan               \n",
      "Lymphocyte_count_20                                                        | 5697              | 0.15              \n",
      "Mean_corpusc_Hb_conc__MCHC__14                                             | 5697              | nan               \n",
      "Mean_corpusc_haemoglobin_MCH__13                                           | 5697              | -0.20             \n",
      "Mean_corpuscular_volume__MCV__11                                           | 5697              | -0.09             \n",
      "Monocyte_count_23                                                          | 5697              | 0.05              \n",
      "Neutrophil_count_19                                                        | 5697              | -0.04             \n",
      "Platelet_count_12                                                          | 5697              | 0.11              \n"
     ]
    }
   ],
   "source": [
    "dm.train_set.view_sample(1, max_dynamic_events=12, report_time=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create models and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using Temporal Positional Encoding. This module uses the patient's age at an event within their time series.\n",
      "INFO:root:Using ExponentialTTELayer. This module predicts the time until next event as an exponential distribution\n"
     ]
    }
   ],
   "source": [
    "model = TTETransformerForCausalSequenceModelling(config, vocab_size).to(device)\n",
    "\n",
    "loss_curves_train = []\n",
    "loss_curves_train_clf = []\n",
    "loss_curves_train_tte = []\n",
    "\n",
    "loss_curves_val = []\n",
    "loss_curves_val_clf = []\n",
    "loss_curves_val_tte = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hoelkOrFY8bN",
    "outputId": "961304cd-e379-40d4-dd56-8de0b91d2861"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with 11.155393 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   1%|          | 1001/179001 [05:36<16:37:15,  2.97it/s]\n",
      "Validation epoch 0:   1%|          | 101/9583 [00:41<1:05:25,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\tTrain loss -0.68: (1.17, -2.53). Val loss -1.03: (1.01, -3.08)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   0%|          | 213/179001 [01:14<17:24:37,  2.85it/s]\n",
      "Exception ignored in: <function WeakValueDictionary.__init__.<locals>.remove at 0x7f1717d69f30>\n",
      "Traceback (most recent call last):\n",
      "  File \"/rds/bear-apps/2022a/EL8-ice/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/weakref.py\", line 106, in remove\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_80067/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">235104803.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">19</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_80067/235104803.py'</span>                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/rds/bear-apps/2022a/EL8-ice/software/PyTorch/1.12.1-foss-2022a-CUDA-11.7.0/lib/python3.10/site-</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1130</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1127 │   │   # this function, and just call forward.</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1128 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">o</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1129 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1130 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1131 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1132 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1133 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/rds/homes/g/gaddcz/Projects/CPRD/src/models/TTE/task_heads/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">causal.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">102</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 99 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   │     </span><span style=\"color: #808000; text-decoration-color: #808000\">e.g. see https://github.com/huggingface/transformers/blob/main/src/transformer</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">100 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">101 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>102 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>hidden_states = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.transformer(tokens=tokens,                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">103 │   │   │   │   │   │   │   │   │   │    </span>ages=ages,                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">104 │   │   │   │   │   │   │   │   │   │    </span>attention_mask=attention_mask)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># shape: (bsz, </span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">105 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/rds/bear-apps/2022a/EL8-ice/software/PyTorch/1.12.1-foss-2022a-CUDA-11.7.0/lib/python3.10/site-</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1130</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1127 │   │   # this function, and just call forward.</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1128 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">o</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1129 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1130 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1131 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1132 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1133 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/rds/homes/g/gaddcz/Projects/CPRD/src/models/TTE/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">base.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">128</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">125 │   │   │   </span>x = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.drop(x)                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">126 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">127 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> block <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.blocks:                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>128 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>x = block(x, attention_mask=attention_mask)                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">129 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">130 │   │   </span>x = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.ln_f(x)                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">131 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/rds/bear-apps/2022a/EL8-ice/software/PyTorch/1.12.1-foss-2022a-CUDA-11.7.0/lib/python3.10/site-</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1130</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1127 │   │   # this function, and just call forward.</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1128 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">o</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1129 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1130 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1131 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1132 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1133 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/rds/homes/g/gaddcz/Projects/CPRD/src/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">block.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">38</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">35 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">36 │   │   </span>residual = hidden_states                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">37 │   │   </span>hidden_states = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.ln_1(hidden_states)                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>38 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>attn_outputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.attn(                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">39 │   │   │   </span>hidden_states,                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">40 │   │   │   </span>layer_past=layer_past,                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">41 │   │   │   </span>attention_mask=attention_mask,                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/rds/bear-apps/2022a/EL8-ice/software/PyTorch/1.12.1-foss-2022a-CUDA-11.7.0/lib/python3.10/site-</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1130</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1127 │   │   # this function, and just call forward.</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1128 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">o</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1129 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1130 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1131 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1132 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1133 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/rds/homes/g/gaddcz/Projects/CPRD/src/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">self_attention.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">148</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">145 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">146 │   │   │   </span>present = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">147 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>148 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>attn_output, attn_weights = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._attn(query, key, value, attention_mask, head_m   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">149 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">150 │   │   </span>attn_output = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._merge_heads(attn_output, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.num_heads, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.head_dim)        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">151 │   │   </span>attn_output = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.out_proj(attn_output)                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/rds/homes/g/gaddcz/Projects/CPRD/src/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">self_attention.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">96</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_attn</span>                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 93 │   │   </span>mask_value = torch.finfo(attn_weights.dtype).min                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 94 │   │   # Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar ty</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 95 │   │   # Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on </span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 96 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>mask_value = torch.tensor(mask_value, dtype=attn_weights.dtype).to(attn_weights.   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 97 │   │   </span>attn_weights = torch.where(causal_mask, attn_weights, mask_value)                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 98 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 99 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> attention_mask <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_80067/\u001b[0m\u001b[1;33m235104803.py\u001b[0m:\u001b[94m19\u001b[0m in \u001b[92m<module>\u001b[0m                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_80067/235104803.py'\u001b[0m                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/rds/bear-apps/2022a/EL8-ice/software/PyTorch/1.12.1-foss-2022a-CUDA-11.7.0/lib/python3.10/site-\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33mpackages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1130\u001b[0m in \u001b[92m_call_impl\u001b[0m                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1127 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# this function, and just call forward.\u001b[0m                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1128 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_pre_hooks \u001b[95mo\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1129 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1130 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*\u001b[96minput\u001b[0m, **kwargs)                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1131 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1132 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1133 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m _global_backward_hooks:                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/rds/homes/g/gaddcz/Projects/CPRD/src/models/TTE/task_heads/\u001b[0m\u001b[1;33mcausal.py\u001b[0m:\u001b[94m102\u001b[0m in \u001b[92mforward\u001b[0m             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 99 \u001b[0m\u001b[2;33m│   │     \u001b[0m\u001b[33me.g. see https://github.com/huggingface/transformers/blob/main/src/transformer\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m100 \u001b[0m\u001b[2;33m│   │   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m101 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m102 \u001b[2m│   │   \u001b[0mhidden_states = \u001b[96mself\u001b[0m.transformer(tokens=tokens,                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m103 \u001b[0m\u001b[2m│   │   │   │   │   │   │   │   │   │    \u001b[0mages=ages,                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m104 \u001b[0m\u001b[2m│   │   │   │   │   │   │   │   │   │    \u001b[0mattention_mask=attention_mask)  \u001b[2m# shape: (bsz, \u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m105 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/rds/bear-apps/2022a/EL8-ice/software/PyTorch/1.12.1-foss-2022a-CUDA-11.7.0/lib/python3.10/site-\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33mpackages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1130\u001b[0m in \u001b[92m_call_impl\u001b[0m                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1127 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# this function, and just call forward.\u001b[0m                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1128 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_pre_hooks \u001b[95mo\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1129 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1130 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*\u001b[96minput\u001b[0m, **kwargs)                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1131 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1132 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1133 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m _global_backward_hooks:                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/rds/homes/g/gaddcz/Projects/CPRD/src/models/TTE/\u001b[0m\u001b[1;33mbase.py\u001b[0m:\u001b[94m128\u001b[0m in \u001b[92mforward\u001b[0m                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m125 \u001b[0m\u001b[2m│   │   │   \u001b[0mx = \u001b[96mself\u001b[0m.drop(x)                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m126 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m127 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m block \u001b[95min\u001b[0m \u001b[96mself\u001b[0m.blocks:                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m128 \u001b[2m│   │   │   \u001b[0mx = block(x, attention_mask=attention_mask)                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m129 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m130 \u001b[0m\u001b[2m│   │   \u001b[0mx = \u001b[96mself\u001b[0m.ln_f(x)                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m131 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/rds/bear-apps/2022a/EL8-ice/software/PyTorch/1.12.1-foss-2022a-CUDA-11.7.0/lib/python3.10/site-\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33mpackages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1130\u001b[0m in \u001b[92m_call_impl\u001b[0m                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1127 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# this function, and just call forward.\u001b[0m                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1128 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_pre_hooks \u001b[95mo\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1129 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1130 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*\u001b[96minput\u001b[0m, **kwargs)                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1131 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1132 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1133 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m _global_backward_hooks:                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/rds/homes/g/gaddcz/Projects/CPRD/src/modules/\u001b[0m\u001b[1;33mblock.py\u001b[0m:\u001b[94m38\u001b[0m in \u001b[92mforward\u001b[0m                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m35 \u001b[0m\u001b[2;33m│   │   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m36 \u001b[0m\u001b[2m│   │   \u001b[0mresidual = hidden_states                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m37 \u001b[0m\u001b[2m│   │   \u001b[0mhidden_states = \u001b[96mself\u001b[0m.ln_1(hidden_states)                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m38 \u001b[2m│   │   \u001b[0mattn_outputs = \u001b[96mself\u001b[0m.attn(                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m39 \u001b[0m\u001b[2m│   │   │   \u001b[0mhidden_states,                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m40 \u001b[0m\u001b[2m│   │   │   \u001b[0mlayer_past=layer_past,                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m41 \u001b[0m\u001b[2m│   │   │   \u001b[0mattention_mask=attention_mask,                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/rds/bear-apps/2022a/EL8-ice/software/PyTorch/1.12.1-foss-2022a-CUDA-11.7.0/lib/python3.10/site-\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33mpackages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1130\u001b[0m in \u001b[92m_call_impl\u001b[0m                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1127 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# this function, and just call forward.\u001b[0m                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1128 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_pre_hooks \u001b[95mo\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1129 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1130 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*\u001b[96minput\u001b[0m, **kwargs)                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1131 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1132 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1133 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m _global_backward_hooks:                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/rds/homes/g/gaddcz/Projects/CPRD/src/modules/\u001b[0m\u001b[1;33mself_attention.py\u001b[0m:\u001b[94m148\u001b[0m in \u001b[92mforward\u001b[0m                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m145 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m146 \u001b[0m\u001b[2m│   │   │   \u001b[0mpresent = \u001b[94mNone\u001b[0m                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m147 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m148 \u001b[2m│   │   \u001b[0mattn_output, attn_weights = \u001b[96mself\u001b[0m._attn(query, key, value, attention_mask, head_m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m149 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m150 \u001b[0m\u001b[2m│   │   \u001b[0mattn_output = \u001b[96mself\u001b[0m._merge_heads(attn_output, \u001b[96mself\u001b[0m.num_heads, \u001b[96mself\u001b[0m.head_dim)        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m151 \u001b[0m\u001b[2m│   │   \u001b[0mattn_output = \u001b[96mself\u001b[0m.out_proj(attn_output)                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/rds/homes/g/gaddcz/Projects/CPRD/src/modules/\u001b[0m\u001b[1;33mself_attention.py\u001b[0m:\u001b[94m96\u001b[0m in \u001b[92m_attn\u001b[0m                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 93 \u001b[0m\u001b[2m│   │   \u001b[0mmask_value = torch.finfo(attn_weights.dtype).min                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 94 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar ty\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 95 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on \u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 96 \u001b[2m│   │   \u001b[0mmask_value = torch.tensor(mask_value, dtype=attn_weights.dtype).to(attn_weights.   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 97 \u001b[0m\u001b[2m│   │   \u001b[0mattn_weights = torch.where(causal_mask, attn_weights, mask_value)                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 98 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 99 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m attention_mask \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for m_idx, (model, m_name) in enumerate(zip(models, m_names)):\n",
    "    \n",
    "print(f\"Training model with {sum(p.numel() for p in model.parameters())/1e6} M parameters\")\n",
    "model = model.to(device)\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=opt.learning_rate)\n",
    "\n",
    "best_val, epochs_since_best = np.inf, 0\n",
    "for epoch in range(opt.epochs):\n",
    "    \n",
    "    epoch_loss, epoch_clf_loss, epoch_tte_loss = 0, 0, 0\n",
    "    model.train()\n",
    "    for i, batch in tqdm(enumerate(dm.train_dataloader()), desc=f\"Training epoch {epoch}\", total=len(dm.train_dataloader())):\n",
    "        if i > 1000:\n",
    "            break\n",
    "            \n",
    "        # evaluate the loss\n",
    "        _, (losses_clf, loss_tte), loss = model(tokens=batch['tokens'].to(device),\n",
    "                                                ages=batch['ages'].to(device),\n",
    "                                                attention_mask=batch['attention_mask'].to(device)\n",
    "                                               )\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # record\n",
    "        epoch_loss += loss.item()            \n",
    "        epoch_clf_loss += torch.sum(losses_clf).item()\n",
    "        epoch_tte_loss += loss_tte.item()\n",
    "    \n",
    "    epoch_loss /= i\n",
    "    epoch_clf_loss /= i\n",
    "    epoch_tte_loss /= i\n",
    "    loss_curves_train.append(epoch_loss)\n",
    "    loss_curves_train_clf.append(epoch_clf_loss)\n",
    "    loss_curves_train_tte.append(epoch_tte_loss)\n",
    "\n",
    "    # evaluate the loss on val set\n",
    "    with torch.no_grad(): \n",
    "        model.eval()\n",
    "        if epoch % opt.eval_interval == 0 or epoch == opt.epochs - 1:\n",
    "            val_loss, val_clf_loss, val_tte_loss = 0, 0, 0\n",
    "            for j, batch in tqdm(enumerate(dm.val_dataloader()), desc=f\"Validation epoch {epoch}\", total=len(dm.val_dataloader())):\n",
    "                if j > 100:\n",
    "                    break\n",
    "                _, (losses_clf, loss_tte), loss = model(tokens=batch['tokens'].to(device), \n",
    "                                                              ages=batch['ages'].to(device),\n",
    "                                                              attention_mask=batch['attention_mask'].to(device)   \n",
    "                                                              )\n",
    "                # record\n",
    "                val_loss += loss.item()                    \n",
    "                val_clf_loss += torch.sum(losses_clf).item()\n",
    "                val_tte_loss += loss_tte.item()\n",
    "                \n",
    "            val_loss /= j\n",
    "            val_clf_loss /= j\n",
    "            val_tte_loss /= j\n",
    "            loss_curves_val.append(val_loss)\n",
    "            loss_curves_val_clf.append(val_clf_loss)\n",
    "            loss_curves_val_tte.append(val_tte_loss)\n",
    "\n",
    "            print(f\"Epoch {epoch}:\\tTrain loss {epoch_loss:.2f}: ({epoch_clf_loss:.2f}, {epoch_tte_loss:.2f}). Val loss {val_loss:.2f}: ({val_clf_loss:.2f}, {val_tte_loss:.2f})\")          \n",
    "            # TODO: Note not fully accurate as last batch is likely not the same size, will be fixed with lightning\n",
    "\n",
    "        if val_loss >= best_val:\n",
    "            epochs_since_best += 1\n",
    "            if epochs_since_best >= 5:\n",
    "                break\n",
    "        else:\n",
    "            best_val = val_loss\n",
    "            epochs_since_best = 0\n",
    "\n",
    "            # Save best seen model\n",
    "            torch.save(model.state_dict(), path_to_db + \"polars/SR.pt\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default context start\n",
    "prompt = [\"O_E___height_1\", \"O_E___weight_2\"]\n",
    "ages_in_years = [18.2, 18.2]\n",
    "\n",
    "# define encoding functions (TODO: add this wrap to datamodule\n",
    "encode_prompt = lambda prompt_list: torch.from_numpy(np.array(dm.encode(prompt_list)).reshape((1,-1))).to(device)\n",
    "encode_age = lambda age_list: torch.tensor([365 * _age for _age in age_list], dtype=torch.int64).reshape((1,-1)).to(device)\n",
    "\n",
    "# Convert for model\n",
    "tokens = encode_prompt(prompt)\n",
    "ages_in_days = encode_age(ages_in_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Using XXX requires value embeddings, but this head ``CURRENTLY`` has no way of sampling value of next event. Setting generated values to nan until this is implemented\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      "O_E___height_1                                    at age 18 (6643.0 days)\n",
      "O_E___weight_2                                    at age 18 (6643.0 days)\n",
      "===========================================================================\n",
      "GENERATION\n",
      "Diastolic_blood_pressure_5                        at age 21 (7525.1 days)\n",
      "Systolic_blood_pressure_4                         at age 21 (7535.9 days)\n",
      "ALLERGICRHINITISCONJ                              at age 23 (8357.9 days)\n",
      "AST___aspartate_transam_SGOT__46                  at age 23 (8358.6 days)\n",
      "Total_bilirubin_55                                at age 23 (8358.9 days)\n",
      "Eosinophil_count_21                               at age 23 (8473.7 days)\n",
      "Erythrocyte_sedimentation_rate_61                 at age 23 (8474.0 days)\n",
      "GFR_calculated_abbreviated_MDRD_34                at age 23 (8481.2 days)\n",
      "Haemoglobin_estimation_9                          at age 23 (8484.2 days)\n",
      "Lymphocyte_count_20                               at age 23 (8489.5 days)\n"
     ]
    }
   ],
   "source": [
    "# generate: sample the next 10 tokens\n",
    "new_tokens, new_ages = model.generate(tokens, ages_in_days, max_new_tokens=10)\n",
    "\n",
    "# report:\n",
    "print(f\"PROMPT:\")\n",
    "for _idx, (_cat, _age,) in enumerate(zip(dm.decode(new_tokens[0].tolist()).split(\" \"),\n",
    "                                         new_ages[0, :]\n",
    "                                        )\n",
    "                                    ):\n",
    "    # _value = dm.unstandardise(_cat, _value)\n",
    "    print(f\"{_cat}\".ljust(50) + f\"at age {_age/365:.0f} ({_age:.1f} days)\")    # with value {_value}\n",
    "    if _idx == tokens.shape[-1] - 1:\n",
    "        print(\"=\"*75)\n",
    "        print(f\"GENERATION\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing generation to real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to retrieve sample index 1 was 0.11035799980163574 seconds\n",
      "\n",
      "SEX                 | F\n",
      "IMD                 | 4.0\n",
      "ETHNICITY           | WHITE\n",
      "birth_year          | 1997.0\n",
      "\n",
      "Token                                                                      | Age               | Standardised value\n",
      "===================================================================================================================\n",
      "Haemoglobin_estimation_9                                                   | 5697              | nan               \n",
      "Lymphocyte_count_20                                                        | 5697              | 0.15              \n",
      "Mean_corpusc_Hb_conc__MCHC__14                                             | 5697              | nan               \n",
      "Mean_corpusc_haemoglobin_MCH__13                                           | 5697              | -0.20             \n",
      "Mean_corpuscular_volume__MCV__11                                           | 5697              | -0.09             \n",
      "Monocyte_count_23                                                          | 5697              | 0.05              \n",
      "Neutrophil_count_19                                                        | 5697              | -0.04             \n",
      "Platelet_count_12                                                          | 5697              | 0.11              \n",
      "Red_blood_cell__RBC__count_10                                              | 5697              | -0.00             \n",
      "Serum_TSH_level_71                                                         | 5697              | 0.15              \n"
     ]
    }
   ],
   "source": [
    "dm.train_set.view_sample(1, max_dynamic_events=10, report_time=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot loss\n",
    "plt.figure()\n",
    "# Training\n",
    "iterations = np.linspace(0, len(loss_curves_train), len(loss_curves_train)) * opt.eval_interval\n",
    "plt.plot(iterations, loss_curves_train, label=\"train\")\n",
    "# Validation\n",
    "iterations = np.linspace(0, len(loss_curves_val), len(loss_curves_val)) * opt.eval_interval\n",
    "plt.plot(iterations, loss_curves_val, label=\"val\", linestyle='dashed')\n",
    "plt.legend()\n",
    "plt.savefig(f\"figs/TTE/loss.png\")\n",
    "\n",
    "# Plot DeSurv loss\n",
    "plt.figure()\n",
    "# Training\n",
    "iterations = np.linspace(0, len(loss_curves_train_surv), len(loss_curves_train_surv)) * opt.eval_interval\n",
    "plt.plot(iterations, loss_curves_train_surv, label=\"train\")\n",
    "# Validation\n",
    "iterations = np.linspace(0, len(loss_curves_val_surv), len(loss_curves_val_surv)) * opt.eval_interval\n",
    "plt.plot(iterations, loss_curves_val_surv, label=\"val\", linestyle='dashed')\n",
    "plt.legend()\n",
    "plt.savefig(f\"figs/TTE/loss_desurv.png\")\n",
    "\n",
    "# Plot value loss\n",
    "plt.figure()\n",
    "# Training\n",
    "iterations = np.linspace(0, len(loss_curves_train_values), len(loss_curves_train_values)) * opt.eval_interval\n",
    "plt.plot(iterations, loss_curves_train_values, label=\"train\", )\n",
    "# Validation\n",
    "iterations = np.linspace(0, len(loss_curves_val_values), len(loss_curves_val_values)) * opt.eval_interval\n",
    "plt.plot(iterations, loss_curves_val_values, label=\"val\", linestyle='dashed')\n",
    "plt.legend()\n",
    "plt.savefig(f\"figs/TTE/loss_val.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diabetes: How related conditions are impacted by each other\n",
    "Probability of type II diabetes before and after a type I diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Control: \t (DEPRESSION): \n",
      "\tprobability of type I diabetes: 0.0300%\n",
      "\tprobability of type II diabetes: 0.1138%\n",
      "\n",
      "Type 1: \t (DEPRESSION,TYPE1DM): \n",
      "\tprobability of type I diabetes: 0.0374%\n",
      "\tprobability of type II diabetes: 0.1200%\n",
      "\n",
      "Type 2: \t (DEPRESSION,TYPE2DIABETES): \n",
      "\tprobability of type I diabetes: 0.0392%\n",
      "\tprobability of type II diabetes: 0.1236%\n"
     ]
    }
   ],
   "source": [
    "t1_token = dm.tokenizer._stoi[\"TYPE1DM\"]\n",
    "t2_token = dm.tokenizer._stoi[\"TYPE2DIABETES\"]\n",
    "\n",
    "\n",
    "base_prompt = [\"DEPRESSION\"]\n",
    "ages_in_years = [20]\n",
    "\n",
    "to_days = lambda a_list: torch.FloatTensor([365 * _a for _a in a_list]).reshape((1,-1)).to(device)\n",
    "\n",
    "# Create a set of prompts\n",
    "prompts, ages, values, desc = [], [], [], []\n",
    "desc.append(\"Control\")\n",
    "prompts.append(base_prompt)\n",
    "ages.append(ages_in_years)\n",
    "desc.append(\"Type 1\")\n",
    "prompts.append(base_prompt + [\"TYPE1DM\"])\n",
    "ages.append(ages_in_years + [21])\n",
    "desc.append(\"Type 2\")\n",
    "prompts.append(base_prompt + [\"TYPE2DIABETES\"])\n",
    "ages.append(ages_in_years + [21])\n",
    "\n",
    "with torch.no_grad(): \n",
    "    model.eval()\n",
    "\n",
    "    for p_idx, (prompt, age) in enumerate(zip(prompts, ages)):\n",
    "        print(f\"\\n{desc[p_idx]}: \\t ({','.join(prompt)}): \")\n",
    "        encoded_prompt = torch.from_numpy(np.array(dm.encode(prompt)).reshape((1,-1))).to(device)\n",
    "        (lgts, tte_dist), _, _ = model(encoded_prompt,\n",
    "                                               # values=torch.tensor(value).to(device),\n",
    "                                               ages=to_days(age),\n",
    "                                               is_generation=True)\n",
    "        probs = torch.nn.functional.softmax(lgts, dim=2)\n",
    "        print(f\"\\tprobability of type I diabetes: {100*float(probs[0, 0, t1_token].cpu().detach().numpy()):.4f}%\")\n",
    "        print(f\"\\tprobability of type II diabetes: {100*float(probs[0, 0, t2_token].cpu().detach().numpy()):.4f}%\")\n",
    "\n",
    "# Note: adding a diagnosis (even if potentially orthogonal) at the beginning of the prompt increases probability of either type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Age: How increasing prompt age affects likelihood of age related diagnoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Age 4\n",
      "======\n",
      "Top 10\n",
      "\tBody_mass_index_3: 22.87%\n",
      "\tO_E___height_1: 13.15%\n",
      "\tDiastolic_blood_pressure_5: 11.87%\n",
      "\tBasophil_count_22: 7.23%\n",
      "\tO_E___weight_2: 6.41%\n",
      "\tALLERGICRHINITISCONJ: 4.36%\n",
      "\tATOPICECZEMA: 3.61%\n",
      "\tASTHMA_PUSHASTHMA: 2.79%\n",
      "\tANXIETY: 2.65%\n",
      "\tDEPRESSION: 2.45%\n",
      "Bottom 30\n",
      "\tPlasma_sodium_level_25: 0.00%\n",
      "\tSICKLE_CELL_DISEASE_V2: 0.00%\n",
      "\tPAD: 0.00%\n",
      "\tPlasma_alkaline_phosphatase_level_49: 0.00%\n",
      "\tPlasma_N_terminal_pro_B_type_natriuretic_peptide_conc_70: 0.00%\n",
      "\tUNK: 0.00%\n",
      "\tPlasma_calcium_level_40: 0.00%\n",
      "\tPlasma_B_natriuretic_peptide_level_69: 0.00%\n",
      "\tAlbumin___creatinine_ratio_37: 0.00%\n",
      "\tSerum_pro_brain_natriuretic_peptide_level_65: 0.00%\n",
      "\tPlasma_total_bilirubin_level_54: 0.00%\n",
      "\tDOWNSSYNDROME: 0.00%\n",
      "\tCYSTICFIBROSIS: 0.01%\n",
      "\tSYSTEMIC_LUPUS_ERYTHEMATOSUS: 0.01%\n",
      "\tN_terminal_pro_brain_natriuretic_peptide_level_67: 0.01%\n",
      "\tPlasma_pro_brain_natriuretic_peptide_level_64: 0.01%\n",
      "\tPlasma_LDL_cholesterol_level_104: 0.01%\n",
      "\tPLASMACELL_NEOPLASM_V2: 0.01%\n",
      "\tMonocyte_count_23: 0.01%\n",
      "\tPlasma_cholesterol_HDL_ratio_96: 0.01%\n",
      "\tADDISONS_DISEASE: 0.01%\n",
      "\tHAEMOCHROMATOSIS_V2: 0.01%\n",
      "\tSYSTEMIC_SCLEROSIS: 0.01%\n",
      "\tPlasma_ferritin_level_62: 0.01%\n",
      "\tADDISON_DISEASE: 0.01%\n",
      "\tSerum_N_terminal_pro_B_type_natriuretic_peptide_conc_68: 0.01%\n",
      "\tPlasma_corrected_calcium_level_43: 0.01%\n",
      "\tBrain_natriuretic_peptide_level_66: 0.01%\n",
      "\tSJOGRENSSYNDROME: 0.01%\n",
      "\tILD_SH: 0.01%\n",
      "\n",
      "Age 8\n",
      "======\n",
      "Top 10\n",
      "\tBody_mass_index_3: 25.28%\n",
      "\tDiastolic_blood_pressure_5: 14.98%\n",
      "\tO_E___height_1: 10.14%\n",
      "\tBasophil_count_22: 8.05%\n",
      "\tO_E___weight_2: 4.94%\n",
      "\tALLERGICRHINITISCONJ: 3.74%\n",
      "\tATOPICECZEMA: 2.74%\n",
      "\tANXIETY: 2.73%\n",
      "\tDEPRESSION: 2.61%\n",
      "\tASTHMA_PUSHASTHMA: 2.43%\n",
      "Bottom 30\n",
      "\tPlasma_sodium_level_25: 0.00%\n",
      "\tSICKLE_CELL_DISEASE_V2: 0.00%\n",
      "\tPAD: 0.00%\n",
      "\tPlasma_alkaline_phosphatase_level_49: 0.00%\n",
      "\tPlasma_N_terminal_pro_B_type_natriuretic_peptide_conc_70: 0.00%\n",
      "\tUNK: 0.00%\n",
      "\tPlasma_calcium_level_40: 0.00%\n",
      "\tSerum_pro_brain_natriuretic_peptide_level_65: 0.00%\n",
      "\tPlasma_B_natriuretic_peptide_level_69: 0.00%\n",
      "\tCYSTICFIBROSIS: 0.00%\n",
      "\tAlbumin___creatinine_ratio_37: 0.01%\n",
      "\tDOWNSSYNDROME: 0.01%\n",
      "\tSYSTEMIC_LUPUS_ERYTHEMATOSUS: 0.01%\n",
      "\tPlasma_pro_brain_natriuretic_peptide_level_64: 0.01%\n",
      "\tPlasma_LDL_cholesterol_level_104: 0.01%\n",
      "\tPlasma_total_bilirubin_level_54: 0.01%\n",
      "\tN_terminal_pro_brain_natriuretic_peptide_level_67: 0.01%\n",
      "\tPLASMACELL_NEOPLASM_V2: 0.01%\n",
      "\tADDISONS_DISEASE: 0.01%\n",
      "\tPlasma_ferritin_level_62: 0.01%\n",
      "\tSYSTEMIC_SCLEROSIS: 0.01%\n",
      "\tPlasma_cholesterol_HDL_ratio_96: 0.01%\n",
      "\tHAEMOCHROMATOSIS_V2: 0.01%\n",
      "\tPlasma_corrected_calcium_level_43: 0.01%\n",
      "\tADDISON_DISEASE: 0.01%\n",
      "\tMonocyte_count_23: 0.01%\n",
      "\tSerum_N_terminal_pro_B_type_natriuretic_peptide_conc_68: 0.01%\n",
      "\tSJOGRENSSYNDROME: 0.01%\n",
      "\tILD_SH: 0.01%\n",
      "\tBrain_natriuretic_peptide_level_66: 0.01%\n",
      "\n",
      "Age 20\n",
      "======\n",
      "Top 10\n",
      "\tBody_mass_index_3: 26.87%\n",
      "\tDiastolic_blood_pressure_5: 15.54%\n",
      "\tO_E___height_1: 8.46%\n",
      "\tBasophil_count_22: 7.87%\n",
      "\tALLERGICRHINITISCONJ: 3.72%\n",
      "\tO_E___weight_2: 3.69%\n",
      "\tANXIETY: 3.42%\n",
      "\tDEPRESSION: 3.03%\n",
      "\tATOPICECZEMA: 2.84%\n",
      "\tASTHMA_PUSHASTHMA: 2.04%\n",
      "Bottom 30\n",
      "\tPlasma_sodium_level_25: 0.00%\n",
      "\tSICKLE_CELL_DISEASE_V2: 0.00%\n",
      "\tPlasma_N_terminal_pro_B_type_natriuretic_peptide_conc_70: 0.00%\n",
      "\tPlasma_alkaline_phosphatase_level_49: 0.00%\n",
      "\tPAD: 0.00%\n",
      "\tUNK: 0.00%\n",
      "\tPlasma_calcium_level_40: 0.00%\n",
      "\tPlasma_B_natriuretic_peptide_level_69: 0.00%\n",
      "\tAlbumin___creatinine_ratio_37: 0.00%\n",
      "\tPlasma_pro_brain_natriuretic_peptide_level_64: 0.00%\n",
      "\tDOWNSSYNDROME: 0.01%\n",
      "\tCYSTICFIBROSIS: 0.01%\n",
      "\tSYSTEMIC_LUPUS_ERYTHEMATOSUS: 0.01%\n",
      "\tN_terminal_pro_brain_natriuretic_peptide_level_67: 0.01%\n",
      "\tSerum_pro_brain_natriuretic_peptide_level_65: 0.01%\n",
      "\tPlasma_LDL_cholesterol_level_104: 0.01%\n",
      "\tPlasma_total_bilirubin_level_54: 0.01%\n",
      "\tPLASMACELL_NEOPLASM_V2: 0.01%\n",
      "\tADDISONS_DISEASE: 0.01%\n",
      "\tSYSTEMIC_SCLEROSIS: 0.01%\n",
      "\tPlasma_cholesterol_HDL_ratio_96: 0.01%\n",
      "\tHAEMOCHROMATOSIS_V2: 0.01%\n",
      "\tSerum_N_terminal_pro_B_type_natriuretic_peptide_conc_68: 0.01%\n",
      "\tPlasma_ferritin_level_62: 0.01%\n",
      "\tADDISON_DISEASE: 0.01%\n",
      "\tMonocyte_count_23: 0.01%\n",
      "\tSJOGRENSSYNDROME: 0.01%\n",
      "\tPlasma_corrected_calcium_level_43: 0.01%\n",
      "\tILD_SH: 0.01%\n",
      "\tBrain_natriuretic_peptide_level_66: 0.01%\n",
      "\n",
      "Age 30\n",
      "======\n",
      "Top 10\n",
      "\tBody_mass_index_3: 25.33%\n",
      "\tDiastolic_blood_pressure_5: 15.88%\n",
      "\tBasophil_count_22: 8.89%\n",
      "\tO_E___height_1: 8.80%\n",
      "\tANXIETY: 3.62%\n",
      "\tALLERGICRHINITISCONJ: 3.30%\n",
      "\tO_E___weight_2: 3.25%\n",
      "\tDEPRESSION: 3.14%\n",
      "\tATOPICECZEMA: 2.32%\n",
      "\tASTHMA_PUSHASTHMA: 1.75%\n",
      "Bottom 30\n",
      "\tPlasma_sodium_level_25: 0.00%\n",
      "\tSICKLE_CELL_DISEASE_V2: 0.00%\n",
      "\tPlasma_N_terminal_pro_B_type_natriuretic_peptide_conc_70: 0.00%\n",
      "\tPAD: 0.00%\n",
      "\tUNK: 0.00%\n",
      "\tPlasma_alkaline_phosphatase_level_49: 0.00%\n",
      "\tPlasma_B_natriuretic_peptide_level_69: 0.00%\n",
      "\tPlasma_calcium_level_40: 0.00%\n",
      "\tSerum_pro_brain_natriuretic_peptide_level_65: 0.00%\n",
      "\tDOWNSSYNDROME: 0.00%\n",
      "\tAlbumin___creatinine_ratio_37: 0.00%\n",
      "\tPlasma_pro_brain_natriuretic_peptide_level_64: 0.01%\n",
      "\tCYSTICFIBROSIS: 0.01%\n",
      "\tSYSTEMIC_LUPUS_ERYTHEMATOSUS: 0.01%\n",
      "\tPLASMACELL_NEOPLASM_V2: 0.01%\n",
      "\tN_terminal_pro_brain_natriuretic_peptide_level_67: 0.01%\n",
      "\tSYSTEMIC_SCLEROSIS: 0.01%\n",
      "\tADDISONS_DISEASE: 0.01%\n",
      "\tPlasma_total_bilirubin_level_54: 0.01%\n",
      "\tPlasma_LDL_cholesterol_level_104: 0.01%\n",
      "\tPlasma_ferritin_level_62: 0.01%\n",
      "\tPlasma_cholesterol_HDL_ratio_96: 0.01%\n",
      "\tHAEMOCHROMATOSIS_V2: 0.01%\n",
      "\tSerum_N_terminal_pro_B_type_natriuretic_peptide_conc_68: 0.01%\n",
      "\tMonocyte_count_23: 0.01%\n",
      "\tADDISON_DISEASE: 0.01%\n",
      "\tPlasma_corrected_calcium_level_43: 0.01%\n",
      "\tSJOGRENSSYNDROME: 0.01%\n",
      "\tILD_SH: 0.01%\n",
      "\tSerum_non_high_density_lipoprotein_cholesterol_level_107: 0.01%\n",
      "\n",
      "Age 60\n",
      "======\n",
      "Top 10\n",
      "\tBody_mass_index_3: 25.32%\n",
      "\tDiastolic_blood_pressure_5: 17.80%\n",
      "\tBasophil_count_22: 8.37%\n",
      "\tO_E___height_1: 8.27%\n",
      "\tO_E___weight_2: 3.88%\n",
      "\tALLERGICRHINITISCONJ: 2.75%\n",
      "\tATOPICECZEMA: 2.50%\n",
      "\tANXIETY: 2.46%\n",
      "\tDEPRESSION: 2.14%\n",
      "\tASTHMA_PUSHASTHMA: 1.95%\n",
      "Bottom 30\n",
      "\tPlasma_sodium_level_25: 0.00%\n",
      "\tPAD: 0.00%\n",
      "\tSICKLE_CELL_DISEASE_V2: 0.00%\n",
      "\tPlasma_alkaline_phosphatase_level_49: 0.00%\n",
      "\tUNK: 0.00%\n",
      "\tPlasma_N_terminal_pro_B_type_natriuretic_peptide_conc_70: 0.00%\n",
      "\tPlasma_calcium_level_40: 0.00%\n",
      "\tPlasma_B_natriuretic_peptide_level_69: 0.00%\n",
      "\tSerum_pro_brain_natriuretic_peptide_level_65: 0.00%\n",
      "\tDOWNSSYNDROME: 0.00%\n",
      "\tCYSTICFIBROSIS: 0.00%\n",
      "\tPlasma_pro_brain_natriuretic_peptide_level_64: 0.00%\n",
      "\tSYSTEMIC_LUPUS_ERYTHEMATOSUS: 0.01%\n",
      "\tMonocyte_count_23: 0.01%\n",
      "\tAlbumin___creatinine_ratio_37: 0.01%\n",
      "\tSYSTEMIC_SCLEROSIS: 0.01%\n",
      "\tN_terminal_pro_brain_natriuretic_peptide_level_67: 0.01%\n",
      "\tPlasma_ferritin_level_62: 0.01%\n",
      "\tPLASMACELL_NEOPLASM_V2: 0.01%\n",
      "\tADDISONS_DISEASE: 0.01%\n",
      "\tPlasma_total_bilirubin_level_54: 0.01%\n",
      "\tSerum_N_terminal_pro_B_type_natriuretic_peptide_conc_68: 0.01%\n",
      "\tPlasma_LDL_cholesterol_level_104: 0.01%\n",
      "\tADDISON_DISEASE: 0.01%\n",
      "\tHAEMOCHROMATOSIS_V2: 0.01%\n",
      "\tPlasma_corrected_calcium_level_43: 0.01%\n",
      "\tPlasma_cholesterol_HDL_ratio_96: 0.01%\n",
      "\tSJOGRENSSYNDROME: 0.01%\n",
      "\tBrain_natriuretic_peptide_level_66: 0.01%\n",
      "\tILD_SH: 0.01%\n",
      "\n",
      "Age 80\n",
      "======\n",
      "Top 10\n",
      "\tBody_mass_index_3: 24.90%\n",
      "\tDiastolic_blood_pressure_5: 17.59%\n",
      "\tBasophil_count_22: 9.44%\n",
      "\tO_E___height_1: 6.88%\n",
      "\tO_E___weight_2: 4.25%\n",
      "\tALLERGICRHINITISCONJ: 3.02%\n",
      "\tATOPICECZEMA: 2.73%\n",
      "\tANXIETY: 2.58%\n",
      "\tDEPRESSION: 2.33%\n",
      "\tASTHMA_PUSHASTHMA: 1.93%\n",
      "Bottom 30\n",
      "\tPlasma_sodium_level_25: 0.00%\n",
      "\tPAD: 0.00%\n",
      "\tSICKLE_CELL_DISEASE_V2: 0.00%\n",
      "\tPlasma_N_terminal_pro_B_type_natriuretic_peptide_conc_70: 0.00%\n",
      "\tUNK: 0.00%\n",
      "\tPlasma_alkaline_phosphatase_level_49: 0.00%\n",
      "\tPlasma_B_natriuretic_peptide_level_69: 0.00%\n",
      "\tPlasma_calcium_level_40: 0.00%\n",
      "\tCYSTICFIBROSIS: 0.00%\n",
      "\tSerum_pro_brain_natriuretic_peptide_level_65: 0.00%\n",
      "\tPlasma_pro_brain_natriuretic_peptide_level_64: 0.00%\n",
      "\tDOWNSSYNDROME: 0.00%\n",
      "\tSYSTEMIC_LUPUS_ERYTHEMATOSUS: 0.01%\n",
      "\tN_terminal_pro_brain_natriuretic_peptide_level_67: 0.01%\n",
      "\tAlbumin___creatinine_ratio_37: 0.01%\n",
      "\tADDISONS_DISEASE: 0.01%\n",
      "\tSYSTEMIC_SCLEROSIS: 0.01%\n",
      "\tPLASMACELL_NEOPLASM_V2: 0.01%\n",
      "\tHAEMOCHROMATOSIS_V2: 0.01%\n",
      "\tMonocyte_count_23: 0.01%\n",
      "\tPlasma_ferritin_level_62: 0.01%\n",
      "\tPlasma_total_bilirubin_level_54: 0.01%\n",
      "\tPlasma_LDL_cholesterol_level_104: 0.01%\n",
      "\tSerum_N_terminal_pro_B_type_natriuretic_peptide_conc_68: 0.01%\n",
      "\tADDISON_DISEASE: 0.01%\n",
      "\tPlasma_cholesterol_HDL_ratio_96: 0.01%\n",
      "\tPlasma_corrected_calcium_level_43: 0.01%\n",
      "\tSJOGRENSSYNDROME: 0.01%\n",
      "\tILD_SH: 0.01%\n",
      "\tSerum_total_cholesterol_level_98: 0.01%\n",
      "\n",
      "Age 90\n",
      "======\n",
      "Top 10\n",
      "\tBody_mass_index_3: 20.98%\n",
      "\tDiastolic_blood_pressure_5: 15.64%\n",
      "\tO_E___height_1: 10.44%\n",
      "\tBasophil_count_22: 7.52%\n",
      "\tO_E___weight_2: 5.14%\n",
      "\tATOPICECZEMA: 3.23%\n",
      "\tALLERGICRHINITISCONJ: 3.14%\n",
      "\tDEPRESSION: 2.65%\n",
      "\tASTHMA_PUSHASTHMA: 2.41%\n",
      "\tANXIETY: 2.32%\n",
      "Bottom 30\n",
      "\tPlasma_sodium_level_25: 0.00%\n",
      "\tPlasma_alkaline_phosphatase_level_49: 0.00%\n",
      "\tSICKLE_CELL_DISEASE_V2: 0.00%\n",
      "\tPAD: 0.00%\n",
      "\tPlasma_N_terminal_pro_B_type_natriuretic_peptide_conc_70: 0.00%\n",
      "\tUNK: 0.00%\n",
      "\tPlasma_B_natriuretic_peptide_level_69: 0.00%\n",
      "\tPlasma_calcium_level_40: 0.00%\n",
      "\tPlasma_pro_brain_natriuretic_peptide_level_64: 0.01%\n",
      "\tDOWNSSYNDROME: 0.01%\n",
      "\tCYSTICFIBROSIS: 0.01%\n",
      "\tAlbumin___creatinine_ratio_37: 0.01%\n",
      "\tSerum_pro_brain_natriuretic_peptide_level_65: 0.01%\n",
      "\tSYSTEMIC_LUPUS_ERYTHEMATOSUS: 0.01%\n",
      "\tMonocyte_count_23: 0.01%\n",
      "\tN_terminal_pro_brain_natriuretic_peptide_level_67: 0.01%\n",
      "\tSYSTEMIC_SCLEROSIS: 0.01%\n",
      "\tPLASMACELL_NEOPLASM_V2: 0.01%\n",
      "\tADDISONS_DISEASE: 0.01%\n",
      "\tPlasma_ferritin_level_62: 0.01%\n",
      "\tHAEMOCHROMATOSIS_V2: 0.01%\n",
      "\tPlasma_cholesterol_HDL_ratio_96: 0.01%\n",
      "\tPlasma_LDL_cholesterol_level_104: 0.01%\n",
      "\tPlasma_total_bilirubin_level_54: 0.01%\n",
      "\tSerum_N_terminal_pro_B_type_natriuretic_peptide_conc_68: 0.01%\n",
      "\tADDISON_DISEASE: 0.01%\n",
      "\tPlasma_corrected_calcium_level_43: 0.01%\n",
      "\tSJOGRENSSYNDROME: 0.01%\n",
      "\tBrain_natriuretic_peptide_level_66: 0.01%\n",
      "\tCombined_total_vitamin_D2_and_D3_level_93: 0.01%\n"
     ]
    }
   ],
   "source": [
    "prompt = [\"ALLERGICRHINITISCONJ\"]\n",
    "ages = [[4],[8],[20],[30],[60],[80],[90]]\n",
    "\n",
    "# target_conditions=[\"TYPE1DM\"]#, \"TYPE2DIABETES\", \"OSTEOARTHRITIS\", \"ANY_DEAFNESS_HEARING_LOSS\"]\n",
    "\n",
    "\n",
    "# for condition in target_conditions:\n",
    "#     print(f\"Probability of {condition}\")\n",
    "#     target_token = dm.tokenizer._stoi[condition]\n",
    "\n",
    "for p_idx, age in enumerate(ages):\n",
    "    print(f\"\\nAge {age[-1]}\\n======\")\n",
    "    encoded_prompt = torch.from_numpy(np.array(dm.encode(prompt)).reshape((1,-1))).to(device)\n",
    "    (lgts, tte_dist), _, _ = model(encoded_prompt,\n",
    "                                   ages=to_days(age),\n",
    "                                   is_generation=True)\n",
    "    probs = torch.nn.functional.softmax(lgts, dim=2) * 100\n",
    "\n",
    "    # top K\n",
    "    k = 10\n",
    "    print(f\"Top {k}\")\n",
    "    topk_prob, topk_ind = torch.topk(probs[0,0,:], k)\n",
    "    for i, j in zip(dm.decode(topk_ind.tolist()).split(\" \"), topk_prob):\n",
    "        print(f\"\\t{i}: {j:.2f}%\")\n",
    "\n",
    "    # bottom K\n",
    "    k = 30\n",
    "    print(f\"Bottom {k}\")\n",
    "    topk_prob, topk_ind = torch.topk(-probs[0,0,:], k)\n",
    "    for i, j in zip(dm.decode(topk_ind.tolist()).split(\" \"), topk_prob):\n",
    "        print(f\"\\t{i}: {-j:.2f}%\")\n",
    "    \n",
    "        # print(f\"Age: {age[-1]} years old:  {100*float(probs[0, 0, target_token].cpu().detach().numpy()):.4f}%\")\n",
    "\n",
    "# Note: adding a diagnosis (even if potentially orthogonal) at the beginning of the prompt increases probability of either type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: model architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TTETransformerForCausalSequenceModelling(\n",
       "  (transformer): TTETransformer(\n",
       "    (wpe): TemporalPositionalEncoding()\n",
       "    (wte): DataEmbeddingLayer(\n",
       "      (static_proj): Linear(in_features=16, out_features=384, bias=True)\n",
       "      (dynamic_embedding_layer): SplitDynamicEmbeddingLayer(\n",
       "        (cat_event_embed_layer): Embedding(184, 384, padding_idx=0)\n",
       "        (cat_event_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (num_value_embed_layer): EmbeddingBag(184, 384, mode=sum, padding_idx=0)\n",
       "        (num_value_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (blocks): ModuleList(\n",
       "      (0): Block(\n",
       "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiHeadedSelfAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (acti): ReLU()\n",
       "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiHeadedSelfAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (acti): ReLU()\n",
       "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiHeadedSelfAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (acti): ReLU()\n",
       "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiHeadedSelfAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (acti): ReLU()\n",
       "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiHeadedSelfAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (acti): ReLU()\n",
       "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiHeadedSelfAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (acti): ReLU()\n",
       "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=384, out_features=184, bias=False)\n",
       "  (tte_layer): ExponentialTTELayer(\n",
       "    (proj): Linear(in_features=384, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to html --no-input TTE.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "sys_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
