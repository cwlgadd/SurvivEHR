{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJpXpmjEYC_T"
   },
   "source": [
    "# Demo Notebook:\n",
    "## Time to Event Transformer For Causal Sequence Modelling \n",
    "\n",
    "Including time, and excluding values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added path '/rds/homes/g/gaddcz/Projects/CPRD/virtual-env-icelake/lib/python3.10/site-packages' at start of search paths.\n",
      "env: SQLITE_TMPDIR=/rds/projects/g/gokhalkm-optimal/DataforCharles\n",
      "env: TMPDIR=/rds/projects/g/gokhalkm-optimal/DataforCharles\n",
      "/rds/projects/g/gokhalkm-optimal/DataforCharles\n",
      "/rds/projects/g/gokhalkm-optimal/DataforCharles\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "node_type = os.getenv('BB_CPU')\n",
    "venv_dir = f'/rds/homes/g/gaddcz/Projects/CPRD/virtual-env-{node_type}'\n",
    "venv_site_pkgs = Path(venv_dir) / 'lib' / f'python{sys.version_info.major}.{sys.version_info.minor}' / 'site-packages'\n",
    "if venv_site_pkgs.exists():\n",
    "    sys.path.insert(0, str(venv_site_pkgs))\n",
    "    print(f\"Added path '{venv_site_pkgs}' at start of search paths.\")\n",
    "else:\n",
    "    print(f\"Path '{venv_site_pkgs}' not found. Check that it exists and/or that it exists for node-type '{node_type}'.\")\n",
    "\n",
    "# Perform sqlite operations on disk\n",
    "%env SQLITE_TMPDIR=/rds/projects/g/gokhalkm-optimal/DataforCharles\n",
    "%env TMPDIR=/rds/projects/g/gokhalkm-optimal/DataforCharles\n",
    "!echo $SQLITE_TMPDIR\n",
    "!echo $TMPDIR\n",
    "!echo $USERPROFILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "/rds/homes/g/gaddcz/Projects/CPRD/examples/modelling/tteGPT\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning \n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sqlite3\n",
    "from dataclasses import dataclass\n",
    "import logging\n",
    "from CPRD.data.foundational_loader import FoundationalDataModule\n",
    "from CPRD.src.models.TTE.task_heads.causal import TTETransformerForCausalSequenceModelling\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "# TODO:\n",
    "# replace experiment boilerplate with pytorch lightning\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "# device = \"cpu\"    # if more informative debugging statements are needed\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set config to be equivalent architecture of kaparthy benchmark, however they are not comparable tasks.\n",
    "@dataclass\n",
    "class DemoConfig:\n",
    "    block_size: int = 256        # what is the maximum context length for predictions?\n",
    "    n_layer: int = 6\n",
    "    n_head: int = 6\n",
    "    n_embd: int = 384\n",
    "    bias: bool = True\n",
    "    attention_type: str = \"global\"    \n",
    "    dropout: float = 0.0\n",
    "    unk_freq_threshold: float = 0.0\n",
    "    TTELayer = \"Exponential\"\n",
    "\n",
    "config = DemoConfig()\n",
    "\n",
    "@dataclass\n",
    "class OptConfig:\n",
    "    batch_size: int = 64\n",
    "    eval_interval: int = 1\n",
    "    learning_rate: float = 3e-4\n",
    "    epochs: int = 10\n",
    "    \n",
    "opt = OptConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n"
     ]
    }
   ],
   "source": [
    "print(os.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data loader on a reduced cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Building Polars dataset and saving to /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/archive/Version2/polars/\n",
      "INFO:root:Chunking by unique practice ID with no inclusion conditions\n",
      "INFO:root:Creating train/test/val splits using practice_ids\n",
      "INFO:root:Extracting practice_patient_ids for each practice\n",
      "                                             Train: 100%|██████████| 27/27 [00:00<00:00, 62.78it/s]\n",
      "                                              Test: 100%|██████████| 2/2 [00:00<00:00, 98.20it/s]\n",
      "                                        Validation: 100%|██████████| 2/2 [00:00<00:00, 86.67it/s]\n",
      "INFO:root:Collecting meta information from database. This will be used for tokenization and standardisation.\n",
      "                                      Measurements: 100%|██████████| 108/108 [00:11<00:00,  9.47it/s]\n",
      "INFO:root:Collating train split into a DL friendly format. Generating over practices IDs\n",
      "100%|██████████| 27/27 [04:20<00:00,  9.64s/it]\n",
      "INFO:root:Collating test split into a DL friendly format. Generating over practices IDs\n",
      "100%|██████████| 2/2 [00:14<00:00,  7.10s/it]\n",
      "INFO:root:Collating val split into a DL friendly format. Generating over practices IDs\n",
      "100%|██████████| 2/2 [00:14<00:00,  7.39s/it]\n",
      "INFO:root:Using tokenizer tabular\n",
      "INFO:root:Tokenzier created based on 80.20M tokens\n",
      "INFO:root:Creating dataset\n",
      "Calculating chunk index splits : 27it [00:05,  5.20it/s]\n",
      "INFO:root:Creating dataset\n",
      "Calculating chunk index splits : 2it [00:00,  6.29it/s]\n",
      "INFO:root:Creating dataset\n",
      "Calculating chunk index splits : 2it [00:00,  7.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "458330 training patients\n",
      "25347 validation patients\n",
      "22562 test patients\n",
      "184 vocab elements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Get a list of patients which fit a reduced set of criterion\n",
    "path_to_db = \"/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/archive/Version2/\"\n",
    "\n",
    "# Build \n",
    "dm = FoundationalDataModule(path_to_db=path_to_db,\n",
    "                            load=False,\n",
    "                            tokenizer=\"tabular\",\n",
    "                            batch_size=opt.batch_size,\n",
    "                            max_seq_length=config.block_size,\n",
    "                            unk_freq_threshold=config.unk_freq_threshold,\n",
    "                            min_workers=10,\n",
    "                           )\n",
    "\n",
    "vocab_size = dm.train_set.tokenizer.vocab_size\n",
    "\n",
    "print(f\"{len(dm.train_set)} training patients\")\n",
    "print(f\"{len(dm.val_set)} validation patients\")\n",
    "print(f\"{len(dm.test_set)} test patients\")\n",
    "print(f\"{vocab_size} vocab elements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.740551948547363\n",
      "1.9602737426757812\n",
      "6.127357482910156e-05\n",
      "3.0279159545898438e-05\n",
      "2.8133392333984375e-05\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# start = time.time()   # starting time\n",
    "# for row_idx, row in enumerate(dm.train_set):\n",
    "#     print(time.time() - start)\n",
    "#     start = time.time()\n",
    "#     if row_idx > opt.batch_size - 1:\n",
    "#         break\n",
    "\n",
    "start = time.time()   # starting time\n",
    "for batch_idx, batch in enumerate(dm.train_dataloader()):\n",
    "    print(time.time() - start)\n",
    "    # time.sleep(np.abs(np.random.normal(10,0.5)))\n",
    "    start = time.time()\n",
    "    if batch_idx > 3:\n",
    "        break\n",
    "# print(f\"{row} loaded in {time.time()-start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': tensor([[176, 183, 160,  ...,   0,   0,   0],\n",
       "         [106,  44,  45,  ...,   0,   0,   0],\n",
       "         [104, 111, 176,  ...,   0,   0,   0],\n",
       "         ...,\n",
       "         [176, 181, 182,  ...,   0,   0,   0],\n",
       "         [176, 183, 181,  ..., 178, 170, 159],\n",
       "         [176, 183, 160,  ...,   0,   0,   0]]),\n",
       " 'ages': tensor([[14226, 14226, 14226,  ...,     0,     0,     0],\n",
       "         [ 9264, 12419, 12419,  ...,     0,     0,     0],\n",
       "         [ 4673,  6347,  9042,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [10995, 10995, 10995,  ...,     0,     0,     0],\n",
       "         [16725, 16725, 16725,  ..., 18352, 18352, 18352],\n",
       "         [10325, 10325, 10325,  ...,     0,     0,     0]]),\n",
       " 'values': tensor([[7.5033e-08, 2.2157e-04, 1.0164e-03,  ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [       nan,        nan,        nan,  ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [       nan,        nan, 6.3632e-08,  ...,        nan,        nan,\n",
       "                 nan],\n",
       "         ...,\n",
       "         [7.6574e-08, 6.4584e-08, 1.4725e-03,  ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [1.0770e-07, 2.2698e-04, 1.1094e-07,  ..., 1.0805e-03, 1.7564e-02,\n",
       "          1.0339e-01],\n",
       "         [2.0029e-09, 2.0266e-04, 9.4259e-04,  ...,        nan,        nan,\n",
       "                 nan]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (183, 3)\n",
      "┌───────────────────────────────────┬─────────┬───────────┐\n",
      "│ EVENT                             ┆ COUNT   ┆ FREQUENCY │\n",
      "│ ---                               ┆ ---     ┆ ---       │\n",
      "│ str                               ┆ u32     ┆ f64       │\n",
      "╞═══════════════════════════════════╪═════════╪═══════════╡\n",
      "│ UNK                               ┆ 0       ┆ 0.0       │\n",
      "│ Plasma_N_terminal_pro_B_type_nat… ┆ 39      ┆ 4.8626e-7 │\n",
      "│ CYSTICFIBROSIS                    ┆ 135     ┆ 0.000002  │\n",
      "│ SICKLE_CELL_DISEASE_V2            ┆ 136     ┆ 0.000002  │\n",
      "│ SYSTEMIC_SCLEROSIS                ┆ 211     ┆ 0.000003  │\n",
      "│ ADDISON_DISEASE                   ┆ 250     ┆ 0.000003  │\n",
      "│ DOWNSSYNDROME                     ┆ 383     ┆ 0.000005  │\n",
      "│ PLASMACELL_NEOPLASM               ┆ 426     ┆ 0.000005  │\n",
      "│ HAEMOCHROMATOSIS_V2               ┆ 536     ┆ 0.000007  │\n",
      "│ SJOGRENSSYNDROME                  ┆ 557     ┆ 0.000007  │\n",
      "│ SYSTEMIC_LUPUS_ERYTHEMATOSUS      ┆ 604     ┆ 0.000008  │\n",
      "│ N_terminal_pro_brain_natriuretic… ┆ 673     ┆ 0.000008  │\n",
      "│ HIVAIDS                           ┆ 982     ┆ 0.000012  │\n",
      "│ Blood_calcium_level_38            ┆ 1079    ┆ 0.000013  │\n",
      "│ MS                                ┆ 1158    ┆ 0.000014  │\n",
      "│ LEUKAEMIA_PREVALENCE              ┆ 1268    ┆ 0.000016  │\n",
      "│ PSORIATICARTHRITIS2021            ┆ 1275    ┆ 0.000016  │\n",
      "│ Plasma_B_natriuretic_peptide_lev… ┆ 1435    ┆ 0.000018  │\n",
      "│ ILD_SH                            ┆ 1437    ┆ 0.000018  │\n",
      "│ CHRONIC_LIVER_DISEASE_ALCOHOL     ┆ 1688    ┆ 0.000021  │\n",
      "│ PERNICIOUSANAEMIA                 ┆ 1707    ┆ 0.000021  │\n",
      "│ CHRONICFATIGUESYNDROMEMM          ┆ 1764    ┆ 0.000022  │\n",
      "│ CROHNS_DISEASE                    ┆ 1800    ┆ 0.000022  │\n",
      "│ MENIERESDISEASE                   ┆ 1810    ┆ 0.000023  │\n",
      "│ Brain_natriuretic_peptide_level_… ┆ 1836    ┆ 0.000023  │\n",
      "│ LYMPHOMA_PREVALENCE               ┆ 1863    ┆ 0.000023  │\n",
      "│ STROKE_HAEMRGIC                   ┆ 1932    ┆ 0.000024  │\n",
      "│ PARKINSONS                        ┆ 2062    ┆ 0.000026  │\n",
      "│ BIPOLAR                           ┆ 2249    ┆ 0.000028  │\n",
      "│ Plasma_pro_brain_natriuretic_pep… ┆ 2628    ┆ 0.000033  │\n",
      "│ AORTICANEURYSM                    ┆ 2668    ┆ 0.000033  │\n",
      "│ ULCERATIVE_COLITIS                ┆ 2697    ┆ 0.000034  │\n",
      "│ BRONCHIECTASIS                    ┆ 2816    ┆ 0.000035  │\n",
      "│ SCHIZOPHRENIAMM                   ┆ 2833    ┆ 0.000035  │\n",
      "│ PTSDDIAGNOSIS                     ┆ 2889    ┆ 0.000036  │\n",
      "│ TYPE1DM                           ┆ 3184    ┆ 0.00004   │\n",
      "│ LEARNINGDISABILITY_V2             ┆ 3267    ┆ 0.000041  │\n",
      "│ AUTISM                            ┆ 3327    ┆ 0.000041  │\n",
      "│ VISUAL_IMPAIRMENT                 ┆ 3512    ┆ 0.000044  │\n",
      "│ ISCHAEMICSTROKE                   ┆ 3774    ┆ 0.000047  │\n",
      "│ FIBROMYALGIA                      ┆ 3858    ┆ 0.000048  │\n",
      "│ NAFLD                             ┆ 4095    ┆ 0.000051  │\n",
      "│ EATINGDISORDERS                   ┆ 4289    ┆ 0.000053  │\n",
      "│ ENDOMETRIOSIS_ADENOMYOSIS         ┆ 4562    ┆ 0.000057  │\n",
      "│ ENDOMETRIOSIS_ADENOMYOSIS_V2      ┆ 4562    ┆ 0.000057  │\n",
      "│ HYPERTHYROIDISM_V2                ┆ 4963    ┆ 0.000062  │\n",
      "│ RHEUMATOIDARTHRITIS               ┆ 4992    ┆ 0.000062  │\n",
      "│ Plasma_ferritin_level_62          ┆ 5002    ┆ 0.000062  │\n",
      "│ PMRANDGCA                         ┆ 5031    ┆ 0.000063  │\n",
      "│ Serum_pro_brain_natriuretic_pept… ┆ 5092    ┆ 0.000063  │\n",
      "│ OSA                               ┆ 5353    ┆ 0.000067  │\n",
      "│ Urine_microalbumin_creatinine_ra… ┆ 5785    ┆ 0.000072  │\n",
      "│ Total_25_hydroxyvitamin_D_level_… ┆ 5862    ┆ 0.000073  │\n",
      "│ OTHER_CHRONIC_LIVER_DISEASE_OPTI… ┆ 6394    ┆ 0.00008   │\n",
      "│ POLYCYSTIC_OVARIAN_SYNDROME_PCOS… ┆ 6518    ┆ 0.000081  │\n",
      "│ Albumin___creatinine_ratio_37     ┆ 6758    ┆ 0.000084  │\n",
      "│ Serum_vitamin_D2_level_89         ┆ 6898    ┆ 0.000086  │\n",
      "│ PAD_STRICT                        ┆ 6912    ┆ 0.000086  │\n",
      "│ EPILEPSY                          ┆ 8340    ┆ 0.000104  │\n",
      "│ INR___international_normalised_r… ┆ 9021    ┆ 0.000112  │\n",
      "│ Serum_N_terminal_pro_B_type_natr… ┆ 9243    ┆ 0.000115  │\n",
      "│ VALVULARDISEASES                  ┆ 9296    ┆ 0.000116  │\n",
      "│ 25_Hydroxyvitamin_D3_level_90     ┆ 10010   ┆ 0.000125  │\n",
      "│ 25_Hydroxyvitamin_D2_level_92     ┆ 10102   ┆ 0.000126  │\n",
      "│ SUBSTANCEMISUSE                   ┆ 10385   ┆ 0.000129  │\n",
      "│ STROKEUNSPECIFIED                 ┆ 11750   ┆ 0.000147  │\n",
      "│ MINFARCTION                       ┆ 11899   ┆ 0.000148  │\n",
      "│ ALL_DEMENTIA                      ┆ 12220   ┆ 0.000152  │\n",
      "│ Combined_total_vitamin_D2_and_D3… ┆ 13241   ┆ 0.000165  │\n",
      "│ HF                                ┆ 13963   ┆ 0.000174  │\n",
      "│ Plasma_free_T4_level_77           ┆ 15085   ┆ 0.000188  │\n",
      "│ GOUT                              ┆ 15096   ┆ 0.000188  │\n",
      "│ TSH_level_74                      ┆ 15216   ┆ 0.00019   │\n",
      "│ OSTEOPOROSIS                      ┆ 16012   ┆ 0.0002    │\n",
      "│ PSORIASIS                         ┆ 17155   ┆ 0.000214  │\n",
      "│ AF                                ┆ 17696   ┆ 0.000221  │\n",
      "│ COPD                              ┆ 18381   ┆ 0.000229  │\n",
      "│ Calcium_adjusted_level_41         ┆ 18661   ┆ 0.000233  │\n",
      "│ PERIPHERAL_NEUROPATHY             ┆ 19202   ┆ 0.000239  │\n",
      "│ Blood_urea_28                     ┆ 20489   ┆ 0.000255  │\n",
      "│ HYPOTHYROIDISM_DRAFT_V1           ┆ 21456   ┆ 0.000268  │\n",
      "│ Plasma_cholesterol_HDL_ratio_96   ┆ 23878   ┆ 0.000298  │\n",
      "│ CKDSTAGE3TO5                      ┆ 25246   ┆ 0.000315  │\n",
      "│ Serum_T4_level_78                 ┆ 26956   ┆ 0.000336  │\n",
      "│ IHD_NOMI                          ┆ 27181   ┆ 0.000339  │\n",
      "│ PREVALENT_IBS_V2                  ┆ 28955   ┆ 0.000361  │\n",
      "│ TYPE2DIABETES                     ┆ 30317   ┆ 0.000378  │\n",
      "│ Serum_total_25_hydroxy_vitamin_D… ┆ 30979   ┆ 0.000386  │\n",
      "│ ALCOHOLMISUSE                     ┆ 31555   ┆ 0.000393  │\n",
      "│ Serum_25_Hydroxy_vitamin_D3_leve… ┆ 33883   ┆ 0.000422  │\n",
      "│ ALLCA_NOBCC_VFINAL                ┆ 35375   ┆ 0.000441  │\n",
      "│ Free_T4_level_76                  ┆ 35394   ┆ 0.000441  │\n",
      "│ DEATH                             ┆ 38690   ┆ 0.000482  │\n",
      "│ AST___aspartate_transam_SGOT__46  ┆ 41243   ┆ 0.000514  │\n",
      "│ Plasma_corrected_calcium_level_4… ┆ 43505   ┆ 0.000542  │\n",
      "│ Total_bilirubin_55                ┆ 44241   ┆ 0.000552  │\n",
      "│ Plasma_LDL_cholesterol_level_104  ┆ 47291   ┆ 0.00059   │\n",
      "│ Serum_vitamin_D_86                ┆ 48442   ┆ 0.000604  │\n",
      "│ ANY_DEAFNESS_HEARING_LOSS_V2      ┆ 55438   ┆ 0.000691  │\n",
      "│ Non_HDL_cholesterol_level_108     ┆ 60638   ┆ 0.000756  │\n",
      "│ Plasma_TSH_level_73               ┆ 64049   ┆ 0.000799  │\n",
      "│ OSTEOARTHRITIS                    ┆ 66451   ┆ 0.000829  │\n",
      "│ Plasma_calcium_level_40           ┆ 67621   ┆ 0.000843  │\n",
      "│ ALLERGICRHINITISCONJ              ┆ 71148   ┆ 0.000887  │\n",
      "│ Plasma_triglyceride_level_106     ┆ 76153   ┆ 0.000949  │\n",
      "│ ANXIETY                           ┆ 83850   ┆ 0.001045  │\n",
      "│ HYPERTENSION                      ┆ 85566   ┆ 0.001067  │\n",
      "│ ASTHMA_PUSHASTHMA                 ┆ 88352   ┆ 0.001102  │\n",
      "│ Plasma_urea_level_30              ┆ 91369   ┆ 0.001139  │\n",
      "│ Haemoglobin_A1c_level_8           ┆ 96109   ┆ 0.001198  │\n",
      "│ DEPRESSION                        ┆ 96216   ┆ 0.0012    │\n",
      "│ ATOPICECZEMA                      ┆ 101828  ┆ 0.00127   │\n",
      "│ Plasma_HDL_cholesterol_level_101  ┆ 101969  ┆ 0.001271  │\n",
      "│ TSH___thyroid_stim_hormone_72     ┆ 102353  ┆ 0.001276  │\n",
      "│ Plasma_total_cholesterol_level_9… ┆ 109526  ┆ 0.001366  │\n",
      "│ Total_alkaline_phosphatase_48     ┆ 116121  ┆ 0.001448  │\n",
      "│ Haematocrit___PCV_16              ┆ 121361  ┆ 0.001513  │\n",
      "│ Plasma_gamma_glutamyl_transferas… ┆ 123332  ┆ 0.001538  │\n",
      "│ Plasma_total_bilirubin_level_54   ┆ 151298  ┆ 0.001886  │\n",
      "│ Plasma_alanine_aminotransferase_… ┆ 155162  ┆ 0.001935  │\n",
      "│ Plasma_alkaline_phosphatase_leve… ┆ 155745  ┆ 0.001942  │\n",
      "│ Calculated_LDL_cholesterol_level… ┆ 159794  ┆ 0.001992  │\n",
      "│ AST_serum_level_47                ┆ 165378  ┆ 0.002062  │\n",
      "│ Plasma_albumin_level_52           ┆ 180982  ┆ 0.002257  │\n",
      "│ Current_smoker_83                 ┆ 192388  ┆ 0.002399  │\n",
      "│ Plasma_potassium_level_27         ┆ 193964  ┆ 0.002418  │\n",
      "│ Plasma_sodium_level_25            ┆ 194345  ┆ 0.002423  │\n",
      "│ Plasma_creatinine_level_32        ┆ 199382  ┆ 0.002486  │\n",
      "│ Serum_total_cholesterol_level_98  ┆ 199418  ┆ 0.002486  │\n",
      "│ Plasma_C_reactive_protein_60      ┆ 214780  ┆ 0.002678  │\n",
      "│ Serum_non_high_density_lipoprote… ┆ 223244  ┆ 0.002783  │\n",
      "│ HbA1c_level__DCCT_aligned__7      ┆ 297120  ┆ 0.003705  │\n",
      "│ Serum_folate_80                   ┆ 309263  ┆ 0.003856  │\n",
      "│ eGFR_using_creatinine__CKD_EPI__… ┆ 310164  ┆ 0.003867  │\n",
      "│ Urine_albumin_creatinine_ratio_3… ┆ 313919  ┆ 0.003914  │\n",
      "│ Total_cholesterol_HDL_ratio_95    ┆ 350071  ┆ 0.004365  │\n",
      "│ Corrected_serum_calcium_level_42  ┆ 350804  ┆ 0.004374  │\n",
      "│ Serum_vitamin_B12_79              ┆ 361201  ┆ 0.004503  │\n",
      "│ Serum_C_reactive_protein_level_5… ┆ 374407  ┆ 0.004668  │\n",
      "│ Serum_ferritin_63                 ┆ 433898  ┆ 0.00541   │\n",
      "│ Serum_free_T4_level_75            ┆ 449966  ┆ 0.00561   │\n",
      "│ Serum_bilirubin_level_53          ┆ 494921  ┆ 0.006171  │\n",
      "│ Serum_cholesterol_HDL_ratio_94    ┆ 531929  ┆ 0.006632  │\n",
      "│ Serum_gamma_glutamyl_transferase… ┆ 535224  ┆ 0.006673  │\n",
      "│ International_normalised_ratio_8… ┆ 575296  ┆ 0.007173  │\n",
      "│ Ex_smoker_84                      ┆ 622081  ┆ 0.007756  │\n",
      "│ Serum_LDL_cholesterol_level_102   ┆ 648760  ┆ 0.008089  │\n",
      "│ Erythrocyte_sedimentation_rate_6… ┆ 658568  ┆ 0.008211  │\n",
      "│ Serum_calcium_39                  ┆ 680070  ┆ 0.008479  │\n",
      "│ Haemoglobin_A1c_level___IFCC_sta… ┆ 732546  ┆ 0.009133  │\n",
      "│ Serum_triglycerides_105           ┆ 948736  ┆ 0.011829  │\n",
      "│ Serum_HDL_cholesterol_level_100   ┆ 1076072 ┆ 0.013417  │\n",
      "│ Red_blood_cell_distribution_widt… ┆ 1077920 ┆ 0.01344   │\n",
      "│ Serum_alanine_aminotransferase_l… ┆ 1129542 ┆ 0.014083  │\n",
      "│ Serum_cholesterol_97              ┆ 1139633 ┆ 0.014209  │\n",
      "│ Serum_TSH_level_71                ┆ 1168345 ┆ 0.014567  │\n",
      "│ Serum_total_bilirubin_level_56    ┆ 1261412 ┆ 0.015727  │\n",
      "│ Never_smoked_tobacco_85           ┆ 1266658 ┆ 0.015793  │\n",
      "│ Mean_corpusc_Hb_conc__MCHC__14    ┆ 1272626 ┆ 0.015867  │\n",
      "│ O_E___height_1                    ┆ 1385957 ┆ 0.01728   │\n",
      "│ Haematocrit_15                    ┆ 1541049 ┆ 0.019214  │\n",
      "│ GFR_calculated_abbreviated_MDRD_… ┆ 1712368 ┆ 0.02135   │\n",
      "│ Serum_alkaline_phosphatase_50     ┆ 1772058 ┆ 0.022094  │\n",
      "│ Serum_albumin_51                  ┆ 1828822 ┆ 0.022802  │\n",
      "│ Serum_urea_level_29               ┆ 1881258 ┆ 0.023456  │\n",
      "│ Basophil_count_22                 ┆ 1936914 ┆ 0.02415   │\n",
      "│ Mean_corpusc_haemoglobin_MCH__13  ┆ 1970759 ┆ 0.024572  │\n",
      "│ Eosinophil_count_21               ┆ 2060523 ┆ 0.025691  │\n",
      "│ Monocyte_count_23                 ┆ 2076890 ┆ 0.025895  │\n",
      "│ Lymphocyte_count_20               ┆ 2080373 ┆ 0.025938  │\n",
      "│ Red_blood_cell__RBC__count_10     ┆ 2083063 ┆ 0.025972  │\n",
      "│ Neutrophil_count_19               ┆ 2090490 ┆ 0.026064  │\n",
      "│ Mean_corpuscular_volume__MCV__11  ┆ 2123197 ┆ 0.026472  │\n",
      "│ Platelet_count_12                 ┆ 2153718 ┆ 0.026853  │\n",
      "│ Total_white_cell_count_18         ┆ 2156773 ┆ 0.026891  │\n",
      "│ Body_mass_index_3                 ┆ 2176838 ┆ 0.027141  │\n",
      "│ Serum_potassium_26                ┆ 2180533 ┆ 0.027187  │\n",
      "│ Haemoglobin_estimation_9          ┆ 2202398 ┆ 0.02746   │\n",
      "│ Serum_sodium_24                   ┆ 2219355 ┆ 0.027671  │\n",
      "│ Serum_creatinine_31               ┆ 2285768 ┆ 0.028499  │\n",
      "│ O_E___weight_2                    ┆ 2468235 ┆ 0.030774  │\n",
      "│ Systolic_blood_pressure_4         ┆ 5591149 ┆ 0.069711  │\n",
      "│ Diastolic_blood_pressure_5        ┆ 5610826 ┆ 0.069956  │\n",
      "└───────────────────────────────────┴─────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "pl.Config.set_tbl_rows(vocab_size + 1)\n",
    "print(dm.tokenizer._event_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create models and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using Temporal Positional Encoding. This module uses the patient's age at an event within their time series.\n",
      "INFO:root:Using ExponentialTTELayer. This module predicts the time until next event as an exponential distribution\n",
      "INFO:root:Using Temporal Positional Encoding. This module uses the patient's age at an event within their time series.\n",
      "INFO:root:Using GeometricTTELayer. This module predicts the time until next event as a geometric distribution, supported on the set {0,1,...}\n"
     ]
    }
   ],
   "source": [
    "models, m_names = [], []\n",
    "\n",
    "# My development model\n",
    "for tte_layer in [\"Exponential\", \"Geometric\"]:\n",
    "    config = DemoConfig()\n",
    "    config.TTELayer = tte_layer\n",
    "    models.append(TTETransformerForCausalSequenceModelling(config, vocab_size).to(device))\n",
    "    m_names.append(f\"TPPTransformerForCausalSequenceModelling: {tte_layer} TTE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_curves_train = [[] for _ in models]\n",
    "loss_curves_train_clf = [[] for _ in models]\n",
    "loss_curves_train_tte = [[] for _ in models]\n",
    "\n",
    "loss_curves_val = [[] for _ in models]\n",
    "loss_curves_val_clf = [[] for _ in models]\n",
    "loss_curves_val_tte = [[] for _ in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hoelkOrFY8bN",
    "outputId": "961304cd-e379-40d4-dd56-8de0b91d2861"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model `TPPTransformerForCausalSequenceModelling: Exponential TTE`, with 10.853185 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   0%|          | 0/7162 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 12.829303979873657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   0%|          | 2/7162 [00:15<13:09:10,  6.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.011183738708496094\n",
      "Time to load batch 0.011097192764282227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   0%|          | 4/7162 [00:16<4:28:07,  2.25s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.002125978469848633\n",
      "Time to load batch 0.0014393329620361328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   0%|          | 6/7162 [00:16<2:02:12,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.0020134449005126953\n",
      "Time to load batch 0.002160787582397461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   0%|          | 8/7162 [00:16<1:03:45,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.0020194053649902344\n",
      "Time to load batch 0.0021080970764160156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   0%|          | 10/7162 [00:16<37:11,  3.21it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.0012030601501464844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   0%|          | 11/7162 [00:26<6:16:04,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 9.427608489990234\n",
      "Time to load batch 0.004099607467651367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   0%|          | 13/7162 [00:26<3:09:53,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.01113438606262207\n",
      "Time to load batch 0.00103759765625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   0%|          | 15/7162 [00:27<1:40:53,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.0010645389556884766\n",
      "Time to load batch 0.0009520053863525391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   0%|          | 17/7162 [00:27<57:12,  2.08it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.0010423660278320312\n",
      "Time to load batch 0.0009596347808837891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   0%|          | 20/7162 [00:27<28:37,  4.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.009146451950073242\n",
      "Time to load batch 0.0010325908660888672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   0%|          | 21/7162 [00:39<7:30:08,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 11.906419277191162\n",
      "Time to load batch 0.011122941970825195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   0%|          | 23/7162 [00:39<3:48:08,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.011175394058227539\n",
      "Time to load batch 0.0011758804321289062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   0%|          | 25/7162 [00:40<1:58:59,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.0008556842803955078\n",
      "Time to load batch 0.0009136199951171875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   0%|          | 28/7162 [00:40<48:59,  2.43it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.002193450927734375\n",
      "Time to load batch 0.0009546279907226562\n",
      "Time to load batch 0.0009748935699462891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   0%|          | 30/7162 [00:40<30:41,  3.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.0009722709655761719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   0%|          | 31/7162 [00:51<6:49:05,  3.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 10.715041875839233\n",
      "Time to load batch 0.011060476303100586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   0%|          | 33/7162 [00:52<3:38:20,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.27408289909362793\n",
      "Time to load batch 0.011074304580688477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   0%|          | 35/7162 [00:52<1:53:54,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.0008938312530517578\n",
      "Time to load batch 0.0008678436279296875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   1%|          | 37/7162 [00:52<1:03:04,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.0008814334869384766\n",
      "Time to load batch 0.0013027191162109375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   1%|          | 39/7162 [00:52<38:07,  3.11it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.0008828639984130859\n",
      "Time to load batch 0.0008823871612548828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   1%|          | 41/7162 [01:04<7:18:19,  3.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 11.555003881454468\n",
      "Time to load batch 0.011188983917236328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   1%|          | 43/7162 [01:04<3:42:25,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.0009324550628662109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   1%|          | 44/7162 [01:05<3:03:08,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.6487607955932617\n",
      "Time to load batch 0.008224010467529297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   1%|          | 46/7162 [01:06<1:38:00,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.011124134063720703\n",
      "Time to load batch 0.011170387268066406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   1%|          | 48/7162 [01:06<55:10,  2.15it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.011152029037475586\n",
      "Time to load batch 0.011102676391601562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   1%|          | 50/7162 [01:06<33:41,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.0008754730224609375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   1%|          | 51/7162 [01:19<7:49:57,  3.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 12.420634508132935\n",
      "Time to load batch 0.0008759498596191406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   1%|          | 54/7162 [01:19<2:49:37,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.0013346672058105469\n",
      "Time to load batch 0.002171754837036133\n",
      "Time to load batch 0.002204418182373047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   1%|          | 57/7162 [01:19<1:06:47,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.002172708511352539\n",
      "Time to load batch 0.0009388923645019531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   1%|          | 59/7162 [01:19<38:52,  3.05it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.0010182857513427734\n",
      "Time to load batch 0.0008676052093505859\n",
      "Time to load batch 0.0008306503295898438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   1%|          | 61/7162 [01:31<7:20:47,  3.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 11.673893928527832\n",
      "Time to load batch 0.0008962154388427734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   1%|          | 63/7162 [01:32<3:42:49,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.0009682178497314453\n",
      "Time to load batch 0.0009853839874267578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   1%|          | 66/7162 [01:32<1:24:30,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.000982522964477539\n",
      "Time to load batch 0.0009758472442626953\n",
      "Time to load batch 0.000982522964477539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   1%|          | 68/7162 [01:32<48:12,  2.45it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.0009586811065673828\n",
      "Time to load batch 0.0009136199951171875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   1%|          | 70/7162 [01:32<31:20,  3.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.0009670257568359375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   1%|          | 71/7162 [01:43<6:47:39,  3.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 10.73011302947998\n",
      "Time to load batch 0.001009225845336914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   1%|          | 73/7162 [01:44<3:35:41,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.21232819557189941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   1%|          | 74/7162 [01:47<4:11:10,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 2.679110050201416\n",
      "Time to load batch 0.011190652847290039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   1%|          | 76/7162 [01:47<2:10:53,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.011202335357666016\n",
      "Time to load batch 0.011143207550048828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   1%|          | 78/7162 [01:47<1:11:54,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.0029489994049072266\n",
      "Time to load batch 0.0022025108337402344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   1%|          | 80/7162 [01:47<42:08,  2.80it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.0018265247344970703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   1%|          | 81/7162 [01:56<5:41:46,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 8.687105417251587\n",
      "Time to load batch 0.0009522438049316406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   1%|          | 83/7162 [01:56<2:55:28,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.0009829998016357422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   1%|          | 84/7162 [01:59<3:23:27,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 2.126112461090088\n",
      "Time to load batch 0.011151552200317383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   1%|          | 86/7162 [01:59<1:47:50,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.011116981506347656\n",
      "Time to load batch 0.006203174591064453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   1%|          | 88/7162 [01:59<59:59,  1.97it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.0022420883178710938\n",
      "Time to load batch 0.001630544662475586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   1%|▏         | 90/7162 [01:59<36:16,  3.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.0009443759918212891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   1%|▏         | 91/7162 [02:08<5:37:38,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 8.697573900222778\n",
      "Time to load batch 0.011085271835327148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   1%|▏         | 93/7162 [02:09<2:54:04,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.0008785724639892578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   1%|▏         | 94/7162 [02:11<3:11:33,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 1.8437871932983398\n",
      "Time to load batch 0.013039112091064453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   1%|▏         | 96/7162 [02:11<1:42:18,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.011188507080078125\n",
      "Time to load batch 0.0021576881408691406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   1%|▏         | 99/7162 [02:11<43:25,  2.71it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.0019481182098388672\n",
      "Time to load batch 0.002140522003173828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   1%|▏         | 100/7162 [02:11<34:06,  3.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.0039017200469970703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   1%|▏         | 101/7162 [02:20<5:50:46,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 9.123026371002197\n",
      "Time to load batch 0.002267599105834961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   1%|▏         | 103/7162 [02:21<3:21:16,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.6441376209259033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   1%|▏         | 104/7162 [02:24<3:37:25,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 2.025434732437134\n",
      "Time to load batch 0.011183738708496094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   1%|▏         | 107/7162 [02:24<1:23:20,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.0008568763732910156\n",
      "Time to load batch 0.002160310745239258\n",
      "Time to load batch 0.002149820327758789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   2%|▏         | 109/7162 [02:24<47:44,  2.46it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.0021750926971435547\n",
      "Time to load batch 0.003252267837524414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   2%|▏         | 111/7162 [02:33<5:27:27,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 8.403613805770874\n",
      "Time to load batch 0.011160850524902344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   2%|▏         | 113/7162 [02:34<3:09:34,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.5921590328216553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   2%|▏         | 114/7162 [02:37<3:56:23,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 2.7698140144348145\n",
      "Time to load batch 0.011161565780639648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   2%|▏         | 116/7162 [02:37<2:04:20,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.011176109313964844\n",
      "Time to load batch 0.0015871524810791016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   2%|▏         | 119/7162 [02:37<50:59,  2.30it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.0118560791015625\n",
      "Time to load batch 0.00093841552734375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   2%|▏         | 120/7162 [02:37<39:17,  2.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.0010635852813720703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   2%|▏         | 121/7162 [02:46<5:25:31,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 8.328293085098267\n",
      "Time to load batch 0.011215448379516602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   2%|▏         | 123/7162 [02:47<3:23:35,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 1.0537564754486084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   2%|▏         | 124/7162 [02:50<4:20:04,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 3.207258701324463\n",
      "Time to load batch 0.0009429454803466797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   2%|▏         | 127/7162 [02:51<1:37:28,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.0008916854858398438\n",
      "Time to load batch 0.0009694099426269531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   2%|▏         | 128/7162 [02:51<1:18:55,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.1824812889099121\n",
      "Time to load batch 0.0022122859954833984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   2%|▏         | 130/7162 [02:51<44:51,  2.61it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.0020160675048828125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   2%|▏         | 131/7162 [02:58<4:36:52,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 6.837976932525635\n",
      "Time to load batch 0.011117935180664062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   2%|▏         | 133/7162 [03:01<4:03:46,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 2.8154335021972656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   2%|▏         | 134/7162 [03:03<3:33:50,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 1.1070115566253662\n",
      "Time to load batch 0.011150598526000977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   2%|▏         | 136/7162 [03:03<1:53:24,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.011122941970825195\n",
      "Time to load batch 0.0023534297943115234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   2%|▏         | 138/7162 [03:05<2:04:25,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 1.709071159362793\n",
      "Time to load batch 0.011184930801391602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   2%|▏         | 140/7162 [03:05<1:09:37,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.011078596115112305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   2%|▏         | 141/7162 [03:10<3:54:49,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 5.153005838394165\n",
      "Time to load batch 0.011121988296508789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   2%|▏         | 143/7162 [03:14<3:49:24,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 3.0020358562469482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   2%|▏         | 144/7162 [03:16<3:41:10,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 1.604011058807373\n",
      "Time to load batch 0.010957956314086914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   2%|▏         | 146/7162 [03:16<1:55:17,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.0007851123809814453\n",
      "Time to load batch 0.0007920265197753906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   2%|▏         | 148/7162 [03:18<2:23:33,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 2.2406063079833984\n",
      "Time to load batch 0.010959863662719727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   2%|▏         | 150/7162 [03:19<1:18:33,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.011229515075683594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   2%|▏         | 151/7162 [03:23<3:44:26,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 4.701861381530762\n",
      "Time to load batch 0.0008213520050048828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   2%|▏         | 153/7162 [03:27<4:09:57,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 3.7636775970458984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   2%|▏         | 154/7162 [03:28<3:19:55,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.5910639762878418\n",
      "Time to load batch 0.0014684200286865234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   2%|▏         | 156/7162 [03:28<1:44:30,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.0009388923645019531\n",
      "Time to load batch 0.0008788108825683594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   2%|▏         | 158/7162 [03:31<2:23:13,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 2.399024486541748\n",
      "Time to load batch 0.011173486709594727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   2%|▏         | 160/7162 [03:31<1:17:47,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.0007991790771484375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   2%|▏         | 161/7162 [03:38<4:41:46,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 6.352023601531982\n",
      "Time to load batch 0.0008747577667236328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   2%|▏         | 163/7162 [03:41<3:57:07,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 2.5930628776550293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   2%|▏         | 164/7162 [03:41<3:15:52,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.7443728446960449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   2%|▏         | 165/7162 [03:42<2:44:32,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.6824781894683838\n",
      "Time to load batch 0.0008502006530761719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   2%|▏         | 167/7162 [03:42<1:27:35,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.011080265045166016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   2%|▏         | 168/7162 [03:44<1:46:03,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 1.1668801307678223\n",
      "Time to load batch 0.003968000411987305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   2%|▏         | 170/7162 [03:44<1:00:31,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.011058807373046875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   2%|▏         | 171/7162 [03:51<4:29:53,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 6.3120129108428955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   2%|▏         | 172/7162 [03:51<3:13:47,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.009000778198242188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   2%|▏         | 173/7162 [03:53<3:35:07,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 2.1189963817596436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   2%|▏         | 174/7162 [03:55<3:33:02,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 1.6530673503875732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   2%|▏         | 176/7162 [03:56<2:11:12,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.8512420654296875\n",
      "Time to load batch 0.0008246898651123047\n",
      "Time to load batch 0.0008726119995117188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   2%|▏         | 178/7162 [03:57<1:52:19,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 1.1609904766082764\n",
      "Time to load batch 0.011106491088867188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   3%|▎         | 180/7162 [03:58<1:02:46,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.011111736297607422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   3%|▎         | 181/7162 [04:04<4:26:43,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 6.239028215408325\n",
      "Time to load batch 0.01104593276977539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   3%|▎         | 183/7162 [04:06<3:39:32,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 2.303036689758301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   3%|▎         | 184/7162 [04:09<3:49:39,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 2.054827928543091\n",
      "Time to load batch 0.0008058547973632812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   3%|▎         | 187/7162 [04:09<1:26:58,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.0007984638214111328\n",
      "Time to load batch 0.0007636547088623047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   3%|▎         | 188/7162 [04:11<2:27:47,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 2.3499643802642822\n",
      "Time to load batch 0.0008666515350341797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   3%|▎         | 190/7162 [04:12<1:21:52,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.01287531852722168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   3%|▎         | 191/7162 [04:16<3:23:14,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 4.0509934425354\n",
      "Time to load batch 0.011053323745727539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   3%|▎         | 193/7162 [04:21<4:23:30,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 4.475890398025513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   3%|▎         | 194/7162 [04:22<3:41:58,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.9543466567993164\n",
      "Time to load batch 0.0009021759033203125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   3%|▎         | 196/7162 [04:22<1:56:36,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.0008187294006347656\n",
      "Time to load batch 0.0008349418640136719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   3%|▎         | 198/7162 [04:26<2:59:05,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 3.27091908454895\n",
      "Time to load batch 0.0008308887481689453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   3%|▎         | 200/7162 [04:26<1:35:45,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.00510859489440918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   3%|▎         | 201/7162 [04:29<2:46:20,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 2.7187700271606445\n",
      "Time to load batch 0.0008521080017089844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   3%|▎         | 203/7162 [04:35<4:57:22,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 5.967419385910034\n",
      "Time to load batch 0.011674880981445312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   3%|▎         | 205/7162 [04:35<2:34:03,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.0008358955383300781\n",
      "Time to load batch 0.0008218288421630859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   3%|▎         | 207/7162 [04:36<1:23:10,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.0008082389831542969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   3%|▎         | 208/7162 [04:39<2:55:35,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 3.2420828342437744\n",
      "Time to load batch 0.0008349418640136719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   3%|▎         | 210/7162 [04:39<1:34:18,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.0010807514190673828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   3%|▎         | 211/7162 [04:43<3:18:02,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 3.6540701389312744\n",
      "Time to load batch 0.01111459732055664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   3%|▎         | 213/7162 [04:49<4:49:46,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 5.314417123794556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   3%|▎         | 214/7162 [04:49<3:33:22,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.19888687133789062\n",
      "Time to load batch 0.0015797615051269531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   3%|▎         | 216/7162 [04:49<1:50:43,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.0008516311645507812\n",
      "Time to load batch 0.0007927417755126953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   3%|▎         | 218/7162 [04:52<2:40:03,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 2.851625442504883\n",
      "Time to load batch 0.0009908676147460938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   3%|▎         | 220/7162 [04:52<1:24:32,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.0007953643798828125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   3%|▎         | 221/7162 [04:56<3:03:12,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 3.450319290161133\n",
      "Time to load batch 0.010961294174194336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   3%|▎         | 223/7162 [05:02<4:40:43,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 5.273319959640503\n",
      "Time to load batch 0.0009243488311767578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   3%|▎         | 226/7162 [05:02<1:44:08,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.000762939453125\n",
      "Time to load batch 0.0008096694946289062\n",
      "Time to load batch 0.0008087158203125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   3%|▎         | 228/7162 [05:06<3:23:28,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 4.178831100463867\n",
      "Time to load batch 0.008320331573486328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   3%|▎         | 230/7162 [05:07<1:47:27,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.0008149147033691406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   3%|▎         | 231/7162 [05:08<2:07:35,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 1.3878977298736572\n",
      "Time to load batch 0.011075258255004883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   3%|▎         | 233/7162 [05:15<4:58:39,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 6.583277702331543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   3%|▎         | 234/7162 [05:15<3:43:30,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.2730898857116699\n",
      "Time to load batch 0.015057802200317383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   3%|▎         | 236/7162 [05:16<1:57:58,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.0008318424224853516\n",
      "Time to load batch 0.0008246898651123047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   3%|▎         | 238/7162 [05:17<1:53:50,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 1.4059770107269287\n",
      "Time to load batch 0.010954856872558594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   3%|▎         | 240/7162 [05:18<1:03:34,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.009050846099853516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   3%|▎         | 241/7162 [05:22<3:08:48,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 4.02508544921875\n",
      "Time to load batch 0.011055946350097656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   3%|▎         | 243/7162 [05:28<5:09:47,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 6.025081157684326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   3%|▎         | 244/7162 [05:29<3:53:36,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.3609330654144287\n",
      "Time to load batch 0.0007429122924804688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   3%|▎         | 246/7162 [05:29<2:00:41,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.0007398128509521484\n",
      "Time to load batch 0.0008630752563476562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   3%|▎         | 248/7162 [05:31<2:04:12,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 1.6772682666778564\n",
      "Time to load batch 0.0008444786071777344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   3%|▎         | 250/7162 [05:31<1:09:23,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.0008487701416015625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   4%|▎         | 251/7162 [05:34<2:29:16,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 2.7720608711242676\n",
      "Time to load batch 0.0021164417266845703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   4%|▎         | 253/7162 [05:41<5:19:57,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 6.930565595626831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   4%|▎         | 255/7162 [05:42<3:04:14,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.8904118537902832\n",
      "Time to load batch 0.0008528232574462891\n",
      "Time to load batch 0.0008039474487304688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   4%|▎         | 257/7162 [05:42<1:36:13,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.0008256435394287109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   4%|▎         | 258/7162 [05:44<2:06:27,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 1.5832939147949219\n",
      "Time to load batch 0.009287118911743164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   4%|▎         | 260/7162 [05:44<1:09:57,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.011110782623291016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   4%|▎         | 261/7162 [05:46<1:45:02,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 1.5020818710327148\n",
      "Time to load batch 0.011085033416748047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   4%|▎         | 263/7162 [05:53<5:05:15,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 7.119077920913696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   4%|▎         | 264/7162 [05:55<4:41:52,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 1.8573341369628906\n",
      "Time to load batch 0.0007660388946533203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   4%|▎         | 267/7162 [05:56<1:44:40,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.0008165836334228516\n",
      "Time to load batch 0.0008294582366943359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   4%|▎         | 268/7162 [05:58<2:18:19,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 1.7603390216827393\n",
      "Time to load batch 0.0011942386627197266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   4%|▍         | 270/7162 [05:58<1:15:14,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.0008556842803955078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   4%|▍         | 271/7162 [05:59<1:28:28,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load batch 0.9316871166229248\n",
      "Time to load batch 0.0009012222290039062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   4%|▍         | 272/7162 [06:04<2:33:58,  1.34s/it]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for m_idx, (model, m_name) in enumerate(zip(models, m_names)):\n",
    "    \n",
    "    print(f\"Training model `{m_name}`, with {sum(p.numel() for p in model.parameters())/1e6} M parameters\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # create a PyTorch optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=opt.learning_rate)\n",
    "\n",
    "    best_val, epochs_since_best = np.inf, 0\n",
    "    for epoch in range(opt.epochs):\n",
    "        epoch_loss, epoch_clf_loss, epoch_tte_loss = 0, 0, 0\n",
    "        model.train()\n",
    "        start = time.time()\n",
    "        for i, batch in tqdm(enumerate(dm.train_dataloader()), desc=f\"Training epoch {epoch}\", total=len(dm.train_dataloader())):\n",
    "            print(f\"Time to load batch {time.time()-start}\")\n",
    "            # if i > 50:\n",
    "            #     break\n",
    "                \n",
    "            # evaluate the loss\n",
    "            _, (loss_clf, loss_tte), loss = model(batch['tokens'].to(device), \n",
    "                                                  ages=batch['ages'].to(device), \n",
    "                                                  attention_mask=batch['attention_mask'].to(device)  \n",
    "                                                  )\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            # record\n",
    "            epoch_clf_loss += loss_clf.item()\n",
    "            epoch_tte_loss += loss_tte.item()\n",
    "            start = time.time()\n",
    "        epoch_loss /= i\n",
    "        epoch_clf_loss /= i\n",
    "        epoch_tte_loss /= i\n",
    "        loss_curves_train[m_idx].append(epoch_loss)\n",
    "        loss_curves_train_clf[m_idx].append(epoch_clf_loss)\n",
    "        loss_curves_train_tte[m_idx].append(epoch_tte_loss)\n",
    "\n",
    "        # evaluate the loss on val set\n",
    "        with torch.no_grad(): \n",
    "            model.eval()\n",
    "            if epoch % opt.eval_interval == 0 or epoch == opt.epochs - 1:\n",
    "                val_loss, val_clf_loss, val_tte_loss = 0, 0, 0\n",
    "                for j, batch in tqdm(enumerate(dm.val_dataloader()), desc=f\"Validation epoch {epoch}\", total=len(dm.val_dataloader())):\n",
    "                    # if j > 20:\n",
    "                    #     break\n",
    "                    _, (loss_clf, loss_tte), loss = model(batch['tokens'].to(device), \n",
    "                                                          ages=batch['ages'].to(device),\n",
    "                                                          attention_mask=batch['attention_mask'].to(device) \n",
    "                                                          )\n",
    "                    val_loss += loss.item()\n",
    "                    # record\n",
    "                    val_clf_loss += loss_clf.item()\n",
    "                    val_tte_loss += loss_tte.item()\n",
    "                val_loss /= j\n",
    "                val_clf_loss /= j\n",
    "                val_tte_loss /= j\n",
    "                loss_curves_val[m_idx].append(val_loss)\n",
    "                loss_curves_val_clf[m_idx].append(val_clf_loss)\n",
    "                loss_curves_val_tte[m_idx].append(val_tte_loss)\n",
    "                print(f\"Epoch {epoch}:\\tTrain loss {epoch_loss:.2f}  ({epoch_clf_loss:.2f}, {epoch_tte_loss:.2f}). Val loss {val_loss:.2f} ({val_clf_loss:.2f}, {val_tte_loss:.2f})\")          \n",
    "                # TODO: Note not fully accurate as last batch is likely not the same size, will be fixed with lightning\n",
    "        \n",
    "            if val_loss >= best_val:\n",
    "                epochs_since_best += 1\n",
    "                if epochs_since_best >= 5:\n",
    "                    break\n",
    "            else:\n",
    "                best_val = val_loss\n",
    "                epochs_since_best = 0\n",
    "\n",
    "    # Test trained model with a prompt\n",
    "    # ----------------    \n",
    "    # set context: diagnosis of depression at 20 years old\n",
    "    tokens = torch.from_numpy(np.array(dm.encode([\"DEPRESSION\"])).reshape((1,-1))).to(device)\n",
    "    ages = torch.tensor([[20*365]], device=device)\n",
    "    # values = torch.tensor([[torch.nan]], device=device)\n",
    "    # generate: sample the next 10 tokens\n",
    "    new_tokens, new_ages = model.generate(tokens, ages, max_new_tokens=10)\n",
    "    generated = dm.decode(new_tokens[0].tolist())\n",
    "    # report:\n",
    "    #    note, Not considering value yet.\n",
    "    for _cat, _age in zip(generated.split(\" \"), new_ages[0, :]):\n",
    "        print(f\"\\t {_cat} at age {_age/365:.0f} ({_age:.1f} days)\")    # with value {_value}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing output to real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diastolic_blood_pressure_5, at age 29 (10762.0 days)\n",
      "Systolic_blood_pressure_4, at age 29 (10762.0 days)\n",
      "ANXIETY, at age 30 (11068.0 days)\n",
      "Diastolic_blood_pressure_5, at age 30 (11109.0 days)\n",
      "Systolic_blood_pressure_4, at age 30 (11109.0 days)\n"
     ]
    }
   ],
   "source": [
    "for batch in dm.train_dataloader():\n",
    "    break\n",
    "conditions = batch[\"tokens\"].numpy().tolist()\n",
    "# delta_ages = batch[\"ages\"][:, 1:] - batch[\"ages\"][:, :-1]\n",
    "for idx, (token, age) in enumerate(zip(conditions[0], batch[\"ages\"][0,:])):\n",
    "    if token == 0 or idx >= 10:\n",
    "        break\n",
    "    print(f\"{dm.decode([token])}, at age {age/365:.0f} ({age:.1f} days)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1392332/3200477305.py:38: UserWarning: Data has no positive values, and therefore cannot be log-scaled.\n",
      "  plt.yscale(\"log\")\n"
     ]
    }
   ],
   "source": [
    "cols = [\"k\", \"r\", \"b\", \"y\"]\n",
    "\n",
    "# Plot loss\n",
    "plt.figure()\n",
    "for m_idx, _ in enumerate(models):\n",
    "    # Training\n",
    "    iterations = np.linspace(0, len(loss_curves_train[m_idx]), len(loss_curves_train[m_idx])) * opt.eval_interval\n",
    "    plt.plot(iterations, loss_curves_train[m_idx], label=f\"{m_names[m_idx]}-train\", c=cols[m_idx], linestyle='dashed')\n",
    "    # Validation\n",
    "    iterations = np.linspace(0, len(loss_curves_val[m_idx]), len(loss_curves_val[m_idx])) * opt.eval_interval\n",
    "    plt.plot(iterations, loss_curves_val[m_idx], label=f\"{m_names[m_idx]}-val\", c=cols[m_idx])\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"figs/TTE/logloss.png\")\n",
    "\n",
    "# Plot classifier loss\n",
    "plt.figure()\n",
    "for m_idx, _ in enumerate(models):\n",
    "    # Training\n",
    "    iterations = np.linspace(0, len(loss_curves_train_clf[m_idx]), len(loss_curves_train_clf[m_idx])) * opt.eval_interval\n",
    "    plt.plot(iterations, loss_curves_train_clf[m_idx], label=f\"{m_names[m_idx]}-train\", c=cols[m_idx], linestyle='dashed')\n",
    "    # Validation\n",
    "    iterations = np.linspace(0, len(loss_curves_val_clf[m_idx]), len(loss_curves_val_clf[m_idx])) * opt.eval_interval\n",
    "    plt.plot(iterations, loss_curves_val_clf[m_idx], label=f\"{m_names[m_idx]}-val\", c=cols[m_idx])\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"figs/TTE/logloss_clf.png\")\n",
    "\n",
    "# Plot tte loss\n",
    "plt.figure()\n",
    "for m_idx, _ in enumerate(models):\n",
    "    # Training\n",
    "    iterations = np.linspace(0, len(loss_curves_train_tte[m_idx]), len(loss_curves_train_tte[m_idx])) * opt.eval_interval\n",
    "    plt.plot(iterations, loss_curves_train_tte[m_idx], label=f\"{m_names[m_idx]}-train\", c=cols[m_idx], linestyle='dashed')\n",
    "    # Validation\n",
    "    iterations = np.linspace(0, len(loss_curves_val_tte[m_idx]), len(loss_curves_val_tte[m_idx])) * opt.eval_interval\n",
    "    plt.plot(iterations, loss_curves_val_tte[m_idx], label=f\"{m_names[m_idx]}-val\", c=cols[m_idx])\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"figs/TTE/logloss_tte.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diabetes: How related conditions are impacted by each other\n",
    "Probability of type II diabetes before and after a type I diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_token = dm.tokenizer._stoi[\"TYPE1DM\"]\n",
    "t2_token = dm.tokenizer._stoi[\"TYPE2DIABETES\"]\n",
    "\n",
    "\n",
    "base_prompt = [\"DEPRESSION\"]\n",
    "ages_in_years = [20]\n",
    "\n",
    "to_days = lambda a_list: torch.FloatTensor([365 * _a for _a in a_list]).reshape((1,-1)).to(device)\n",
    "\n",
    "# Create a set of prompts\n",
    "prompts, ages, values, desc = [], [], [], []\n",
    "desc.append(\"Control\")\n",
    "prompts.append(base_prompt)\n",
    "ages.append(ages_in_years)\n",
    "desc.append(\"Type 1\")\n",
    "prompts.append(base_prompt + [\"TYPE1DM\"])\n",
    "ages.append(ages_in_years + [21])\n",
    "desc.append(\"Type 2\")\n",
    "prompts.append(base_prompt + [\"TYPE2DIABETES\"])\n",
    "ages.append(ages_in_years + [21])\n",
    "\n",
    "for model_idx, model in enumerate(models):\n",
    "    print(f\"\\n\\n{m_names[model_idx]}\\n--------------------------------------\")\n",
    "    with torch.no_grad(): \n",
    "            model.eval()\n",
    "    \n",
    "            for p_idx, (prompt, age) in enumerate(zip(prompts, ages)):\n",
    "                print(f\"\\n{desc[p_idx]}: \\t ({','.join(prompt)}): \")\n",
    "                encoded_prompt = torch.from_numpy(np.array(dm.encode(prompt)).reshape((1,-1))).to(device)\n",
    "                (lgts, tte_dist), _, _ = model(encoded_prompt,\n",
    "                                                       # values=torch.tensor(value).to(device),\n",
    "                                                       ages=to_days(age),\n",
    "                                                       is_generation=True)\n",
    "                probs = torch.nn.functional.softmax(lgts, dim=2)\n",
    "                print(f\"\\tprobability of type I diabetes: {100*float(probs[0, 0, t1_token].cpu().detach().numpy()):.4f}%\")\n",
    "                print(f\"\\tprobability of type II diabetes: {100*float(probs[0, 0, t2_token].cpu().detach().numpy()):.4f}%\")\n",
    "\n",
    "# Note: adding a diagnosis (even if potentially orthogonal) at the beginning of the prompt increases probability of either type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Age: How increasing prompt age affects likelihood of age related diagnoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\"ALLERGICRHINITISCONJ\"]\n",
    "ages = [[4],[8],[20],[30],[60],[80],[90]]\n",
    "\n",
    "# target_conditions=[\"TYPE1DM\"]#, \"TYPE2DIABETES\", \"OSTEOARTHRITIS\", \"ANY_DEAFNESS_HEARING_LOSS\"]\n",
    "\n",
    "for model_idx, model in enumerate(models):\n",
    "    print(f\"\\n\\n{m_names[model_idx]}\\n--------------------------------------\")\n",
    "\n",
    "    # for condition in target_conditions:\n",
    "    #     print(f\"Probability of {condition}\")\n",
    "    #     target_token = dm.tokenizer._stoi[condition]\n",
    "\n",
    "    for p_idx, age in enumerate(ages):\n",
    "        print(f\"\\nAge {age[-1]}\\n======\")\n",
    "        encoded_prompt = torch.from_numpy(np.array(dm.encode(prompt)).reshape((1,-1))).to(device)\n",
    "        (lgts, tte_dist), _, _ = model(encoded_prompt,\n",
    "                                       ages=to_days(age),\n",
    "                                       is_generation=True)\n",
    "        probs = torch.nn.functional.softmax(lgts, dim=2) * 100\n",
    "\n",
    "        # top K\n",
    "        k = 10\n",
    "        print(f\"Top {k}\")\n",
    "        topk_prob, topk_ind = torch.topk(probs[0,0,:], k)\n",
    "        for i, j in zip(dm.decode(topk_ind.tolist()).split(\" \"), topk_prob):\n",
    "            print(f\"\\t{i}: {j:.2f}%\")\n",
    "\n",
    "        # bottom K\n",
    "        k = 30\n",
    "        print(f\"Bottom {k}\")\n",
    "        topk_prob, topk_ind = torch.topk(-probs[0,0,:], k)\n",
    "        for i, j in zip(dm.decode(topk_ind.tolist()).split(\" \"), topk_prob):\n",
    "            print(f\"\\t{i}: {-j:.2f}%\")\n",
    "        \n",
    "            # print(f\"Age: {age[-1]} years old:  {100*float(probs[0, 0, target_token].cpu().detach().numpy()):.4f}%\")\n",
    "\n",
    "# Note: adding a diagnosis (even if potentially orthogonal) at the beginning of the prompt increases probability of either type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: model architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_idx, model in enumerate(models):\n",
    "    print(f\"\\n\\n{m_names[model_idx]}\\n\" + \"=\"*len(m_names[model_idx]))\n",
    "    print(f\"\\n\\n{model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to html --no-input TTE.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "sys_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
