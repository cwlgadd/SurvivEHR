{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJpXpmjEYC_T"
   },
   "source": [
    "# Demo Notebook:\n",
    "## Time to Event Transformer For Causal Sequence Modelling \n",
    "\n",
    "Including time, and excluding values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "/rds/homes/g/gaddcz/Projects/CPRD/examples/modelling/tteGPT\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning \n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sqlite3\n",
    "from dataclasses import dataclass\n",
    "import logging\n",
    "from CPRD.data.foundational_loader import FoundationalDataModule\n",
    "from CPRD.src.models.TTE.task_heads.causal import TTETransformerForCausalSequenceModelling\n",
    "from tqdm import tqdm\n",
    "\n",
    "# TODO:\n",
    "# replace experiment boilerplate with pytorch lightning\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "# device = \"cpu\"    # if more informative debugging statements are needed\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set config to be equivalent architecture of kaparthy benchmark, however they are not comparable tasks.\n",
    "@dataclass\n",
    "class DemoConfig:\n",
    "    block_size: int = 256        # what is the maximum context length for predictions?\n",
    "    n_layer: int = 6\n",
    "    n_head: int = 6\n",
    "    n_embd: int = 384\n",
    "    bias: bool = True\n",
    "    attention_type: str = \"global\"    \n",
    "    dropout: float = 0.0\n",
    "    unk_freq_threshold: float = 0.0\n",
    "    TTELayer = \"Exponential\"\n",
    "\n",
    "config = DemoConfig()\n",
    "\n",
    "@dataclass\n",
    "class OptConfig:\n",
    "    batch_size: int = 16\n",
    "    eval_interval: int = 1\n",
    "    learning_rate: float = 3e-4\n",
    "    epochs: int = 50\n",
    "    \n",
    "opt = OptConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data loader on a reduced cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading Polars dataset from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/archive/Version2/polars/\n",
      "INFO:root:Using tokenizer tabular\n",
      "INFO:root:Tokenzier created based on 74.82M tokens\n",
      "INFO:root:Creating dataset\n",
      "INFO:root:Creating hash map\n",
      "Calculating chunk index splits : 27it [00:05,  4.63it/s]\n",
      "INFO:root:Creating dataset\n",
      "INFO:root:Creating hash map\n",
      "Calculating chunk index splits : 2it [00:00,  9.06it/s]\n",
      "INFO:root:Creating dataset\n",
      "INFO:root:Creating hash map\n",
      "Calculating chunk index splits : 2it [00:00, 13.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466364 training patients\n",
      "17841 validation patients\n",
      "22034 test patients\n",
      "184 vocab elements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Get a list of patients which fit a reduced set of criterion\n",
    "path_to_db = \"/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/archive/Version2/\"\n",
    "\n",
    "# Build \n",
    "dm = FoundationalDataModule(path_to_db=path_to_db,\n",
    "                            load=True,\n",
    "                            # include_measurements=True,\n",
    "                            # drop_missing_data=True,\n",
    "                            # include_diagnoses=True,\n",
    "                            # drop_empty_dynamic=True,\n",
    "                            tokenizer=\"tabular\",\n",
    "                            batch_size=opt.batch_size,\n",
    "                            max_seq_length=config.block_size,\n",
    "                            unk_freq_threshold=config.unk_freq_threshold,\n",
    "                            min_workers=4,\n",
    "                           )\n",
    "\n",
    "vocab_size = dm.train_set.tokenizer.vocab_size\n",
    "\n",
    "print(f\"{len(dm.train_set)} training patients\")\n",
    "print(f\"{len(dm.val_set)} validation patients\")\n",
    "print(f\"{len(dm.test_set)} test patients\")\n",
    "print(f\"{vocab_size} vocab elements\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07556700706481934\n",
      "0.06747794151306152\n",
      "0.061283111572265625\n",
      "0.05940818786621094\n",
      "0.060183048248291016\n",
      "0.058316946029663086\n",
      "0.05803680419921875\n",
      "0.056623220443725586\n",
      "0.05976676940917969\n",
      "0.05840110778808594\n",
      "0.0591280460357666\n",
      "0.05927777290344238\n",
      "0.05933260917663574\n",
      "0.057816267013549805\n",
      "0.05925178527832031\n",
      "0.05769681930541992\n",
      "0.059206485748291016\n",
      "{'identifier': 'p20695_1668784120695', 'tokens': tensor([108, 160, 181, 183, 174, 182, 181, 174, 181, 183, 174, 182, 166, 176,\n",
      "        172, 175, 178, 171, 167, 161, 170, 169, 183, 182, 183, 166, 176, 172,\n",
      "        175, 178, 171, 167, 173, 161, 168, 170, 169, 183, 182, 181, 174, 183,\n",
      "        182, 181, 174, 183, 182, 160, 181, 183, 174, 182, 183, 182, 183, 182,\n",
      "        103, 183, 182, 183, 182, 109, 181, 174, 147, 166, 163, 162, 176, 148,\n",
      "        152, 172, 154, 175, 156, 178, 179, 171, 137, 138, 167, 180, 173, 161,\n",
      "        168, 153,  97, 151, 149, 165, 164, 170, 169, 177, 183, 182, 183, 182,\n",
      "        183, 182, 183, 182, 181, 174, 147, 132, 154, 156, 113, 179, 137, 138,\n",
      "        180, 153,  97, 151, 149, 165, 164, 177, 183, 182, 181, 183, 182, 183,\n",
      "        182, 183, 182, 183, 182, 163, 162, 142, 176, 152, 172, 154, 175, 156,\n",
      "        178, 179, 171, 157, 155, 167, 180, 173, 150, 168, 153, 119, 151, 165,\n",
      "        164, 170, 169, 177, 106, 183, 182, 166, 163, 162, 142, 176, 152, 172,\n",
      "        154, 175, 156, 143, 178, 179, 171, 157, 130, 167, 180, 173, 150, 168,\n",
      "        153, 119, 151, 165, 164, 170, 169, 177, 183, 182]), 'ages': tensor([ 8125, 11722, 11722, 11722, 11722, 11722, 11881, 11881, 11995, 11995,\n",
      "        11995, 11995, 12043, 12043, 12043, 12043, 12043, 12043, 12043, 12043,\n",
      "        12043, 12043, 12051, 12051, 12062, 12189, 12189, 12189, 12189, 12189,\n",
      "        12189, 12189, 12189, 12189, 12189, 12189, 12189, 12443, 12443, 12518,\n",
      "        12518, 12604, 12604, 12783, 12783, 12860, 12860, 12917, 12917, 12917,\n",
      "        12917, 12917, 13182, 13182, 13392, 13392, 13412, 13482, 13482, 13518,\n",
      "        13518, 13518, 13745, 13745, 13748, 13748, 13748, 13748, 13748, 13748,\n",
      "        13748, 13748, 13748, 13748, 13748, 13748, 13748, 13748, 13748, 13748,\n",
      "        13748, 13748, 13748, 13748, 13748, 13748, 13748, 13748, 13748, 13748,\n",
      "        13748, 13748, 13748, 13748, 13980, 13980, 14064, 14064, 14371, 14371,\n",
      "        14682, 14682, 14708, 14708, 14710, 14710, 14710, 14710, 14710, 14710,\n",
      "        14710, 14710, 14710, 14710, 14710, 14710, 14710, 14710, 14710, 14710,\n",
      "        15037, 15037, 15127, 15212, 15212, 15450, 15450, 15570, 15570, 15849,\n",
      "        15849, 16096, 16096, 16096, 16096, 16096, 16096, 16096, 16096, 16096,\n",
      "        16096, 16096, 16096, 16096, 16096, 16096, 16096, 16096, 16096, 16096,\n",
      "        16096, 16096, 16096, 16096, 16096, 16096, 16096, 16096, 16725, 17371,\n",
      "        17371, 17381, 17381, 17381, 17381, 17381, 17381, 17381, 17381, 17381,\n",
      "        17381, 17381, 17381, 17381, 17381, 17381, 17381, 17381, 17381, 17381,\n",
      "        17381, 17381, 17381, 17381, 17381, 17381, 17381, 17381, 17381, 17381,\n",
      "        17392, 17392]), 'values': tensor([       nan, 1.4500e+02, 6.2000e+01, 7.8000e+01, 2.9400e+01, 1.0100e+02,\n",
      "        6.2000e+01, 2.9400e+01, 6.0000e+01, 8.2000e+01, 2.8500e+01, 1.1000e+02,\n",
      "        1.0000e-01, 1.1600e+01, 8.4000e+00, 2.4300e+02, 1.3200e+01, 2.1000e+00,\n",
      "        2.9100e+01, 3.8100e-01, 4.5500e+00, 6.0000e-01, 7.8000e+01, 1.1000e+02,\n",
      "        7.9000e+01, 0.0000e+00, 1.0500e+01, 8.5000e+00, 2.3300e+02, 1.1200e+01,\n",
      "        1.4000e+00, 2.6600e+01, 7.6200e+01, 3.2200e-01, 5.0000e-01, 4.2300e+00,\n",
      "        1.0000e-01, 7.0000e+01, 1.0000e+02, 6.5000e+01, 3.0900e+01, 8.0000e+01,\n",
      "        1.2000e+02, 6.8000e+01, 3.2300e+01, 7.8000e+01, 1.3000e+02, 1.4500e+02,\n",
      "        6.5000e+01, 8.7000e+01, 3.0900e+01, 1.1300e+02, 7.4000e+01, 1.1300e+02,\n",
      "        7.5000e+01, 1.1400e+02,        nan, 9.1000e+01, 1.2400e+02, 8.9000e+01,\n",
      "        1.3700e+02,        nan, 6.6000e+01, 3.1300e+01, 2.4000e+00, 1.0000e-01,\n",
      "        9.8000e+01, 9.0000e+01, 5.6000e+00, 5.0000e+00, 1.4000e+01, 3.0000e+00,\n",
      "        3.9000e+00, 2.1100e+02, 2.1400e+00, 1.5400e+01, 1.3700e+02, 2.0000e+00,\n",
      "        3.9000e+00, 2.2500e+00, 2.8700e+01, 6.5000e+01, 8.2900e+01, 4.4500e-01,\n",
      "        5.0000e-01, 1.0000e+00, 9.0000e+00, 1.2000e+00, 2.1700e+00, 3.4000e+00,\n",
      "        3.5000e+01, 5.3600e+00, 1.0000e-01, 5.1000e+00, 9.0000e+01, 1.3400e+02,\n",
      "        9.0000e+01, 1.2800e+02, 8.4000e+01, 1.1900e+02, 8.0000e+01, 1.1900e+02,\n",
      "        6.6000e+01, 3.1300e+01, 2.4000e+00, 9.2000e+00, 3.9000e+00, 1.1300e+00,\n",
      "        6.5000e+01, 1.4100e+02, 3.0000e+00, 2.3300e+00, 7.1000e+01, 1.3000e+00,\n",
      "        2.2000e+01, 5.0000e-01, 2.3300e+00, 3.7000e+00, 4.0000e+01, 4.9000e+00,\n",
      "        8.0000e+01, 1.3900e+02, 7.3000e+01, 6.8000e+01, 1.0200e+02, 1.0000e+02,\n",
      "        1.3200e+02, 8.0000e+01, 1.3000e+02, 8.8000e+01, 1.2200e+02, 6.0000e+01,\n",
      "        6.0000e+01, 1.4000e+01, 7.6000e+00, 1.4000e+01, 4.7000e+00, 3.7000e+00,\n",
      "        2.5300e+02, 1.3400e+00, 1.4100e+02, 1.4200e+02, 2.4000e+00, 3.3300e+02,\n",
      "        1.7000e+01, 2.8000e+01, 7.6000e+01, 8.4100e+01, 2.8000e+01, 3.0000e-01,\n",
      "        1.2000e+00, 2.2000e+00, 8.3000e-01, 4.6000e+00, 3.7000e+01, 5.0300e+00,\n",
      "        2.0000e-01, 3.9000e+00,        nan, 9.5000e+01, 1.2700e+02, 1.0000e-01,\n",
      "        6.4000e+01, 9.0000e+01, 8.0000e+00, 8.7000e+00, 1.4800e+01, 5.6000e+00,\n",
      "        4.1000e+00, 2.3400e+02, 1.4100e+00, 3.2000e+00, 1.3500e+02, 1.4000e+02,\n",
      "        1.8000e+00, 3.2600e+02, 2.8000e+00, 2.6600e+01, 6.4000e+01, 8.1400e+01,\n",
      "        3.1000e+01, 6.0000e-01, 1.2900e+00, 2.5000e+00, 7.8000e-01, 4.7000e+00,\n",
      "        3.8000e+01, 5.0700e+00, 5.0000e-01, 4.8000e+00, 8.8000e+01, 1.2200e+02])} loaded in 0.003571033477783203 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()   # starting time\n",
    "for row_idx, row in enumerate(dm.train_set):\n",
    "    print(time.time() - start)\n",
    "    start = time.time()\n",
    "    if row_idx > opt.batch_size - 1:\n",
    "        break\n",
    "# print(f\"{row} loaded in {time.time()-start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (183, 3)\n",
      "┌───────────────────────────────────┬─────────┬───────────┐\n",
      "│ EVENT                             ┆ COUNT   ┆ FREQUENCY │\n",
      "│ ---                               ┆ ---     ┆ ---       │\n",
      "│ str                               ┆ u32     ┆ f64       │\n",
      "╞═══════════════════════════════════╪═════════╪═══════════╡\n",
      "│ UNK                               ┆ 0       ┆ 0.0       │\n",
      "│ Plasma_N-terminal_pro_B-type_nat… ┆ 23      ┆ 3.0742e-7 │\n",
      "│ SICKLE_CELL_DISEASE_V2            ┆ 123     ┆ 0.000002  │\n",
      "│ CYSTICFIBROSIS                    ┆ 127     ┆ 0.000002  │\n",
      "│ SYSTEMIC_SCLEROSIS                ┆ 199     ┆ 0.000003  │\n",
      "│ ADDISON_DISEASE                   ┆ 239     ┆ 0.000003  │\n",
      "│ DOWNSSYNDROME                     ┆ 361     ┆ 0.000005  │\n",
      "│ PLASMACELL_NEOPLASM               ┆ 399     ┆ 0.000005  │\n",
      "│ HAEMOCHROMATOSIS_V2               ┆ 515     ┆ 0.000007  │\n",
      "│ SJOGRENSSYNDROME                  ┆ 530     ┆ 0.000007  │\n",
      "│ SYSTEMIC_LUPUS_ERYTHEMATOSUS      ┆ 560     ┆ 0.000007  │\n",
      "│ N_terminal_pro-brain_natriuretic… ┆ 671     ┆ 0.000009  │\n",
      "│ Blood_calcium_level_38            ┆ 844     ┆ 0.000011  │\n",
      "│ HIVAIDS                           ┆ 931     ┆ 0.000012  │\n",
      "│ Brain_natriuretic_peptide_level_… ┆ 941     ┆ 0.000013  │\n",
      "│ MS                                ┆ 1084    ┆ 0.000014  │\n",
      "│ LEUKAEMIA_PREVALENCE              ┆ 1201    ┆ 0.000016  │\n",
      "│ PSORIATICARTHRITIS2021            ┆ 1212    ┆ 0.000016  │\n",
      "│ Plasma_B_natriuretic_peptide_lev… ┆ 1285    ┆ 0.000017  │\n",
      "│ ILD_SH                            ┆ 1376    ┆ 0.000018  │\n",
      "│ PERNICIOUSANAEMIA                 ┆ 1601    ┆ 0.000021  │\n",
      "│ CHRONIC_LIVER_DISEASE_ALCOHOL     ┆ 1617    ┆ 0.000022  │\n",
      "│ CHRONICFATIGUESYNDROMEMM          ┆ 1682    ┆ 0.000022  │\n",
      "│ CROHNS_DISEASE                    ┆ 1686    ┆ 0.000023  │\n",
      "│ MENIERESDISEASE                   ┆ 1713    ┆ 0.000023  │\n",
      "│ LYMPHOMA_PREVALENCE               ┆ 1740    ┆ 0.000023  │\n",
      "│ STROKE_HAEMRGIC                   ┆ 1797    ┆ 0.000024  │\n",
      "│ PARKINSONS                        ┆ 1932    ┆ 0.000026  │\n",
      "│ BIPOLAR                           ┆ 2092    ┆ 0.000028  │\n",
      "│ ULCERATIVE_COLITIS                ┆ 2498    ┆ 0.000033  │\n",
      "│ AORTICANEURYSM                    ┆ 2508    ┆ 0.000034  │\n",
      "│ SCHIZOPHRENIAMM                   ┆ 2611    ┆ 0.000035  │\n",
      "│ Plasma_pro-brain_natriuretic_pep… ┆ 2616    ┆ 0.000035  │\n",
      "│ PTSDDIAGNOSIS                     ┆ 2670    ┆ 0.000036  │\n",
      "│ BRONCHIECTASIS                    ┆ 2678    ┆ 0.000036  │\n",
      "│ Total_25-hydroxyvitamin_D_level_… ┆ 2684    ┆ 0.000036  │\n",
      "│ TYPE1DM                           ┆ 2983    ┆ 0.00004   │\n",
      "│ LEARNINGDISABILITY_V2             ┆ 3023    ┆ 0.00004   │\n",
      "│ AUTISM                            ┆ 3060    ┆ 0.000041  │\n",
      "│ VISUAL_IMPAIRMENT                 ┆ 3299    ┆ 0.000044  │\n",
      "│ ISCHAEMICSTROKE                   ┆ 3470    ┆ 0.000046  │\n",
      "│ FIBROMYALGIA                      ┆ 3657    ┆ 0.000049  │\n",
      "│ NAFLD                             ┆ 3877    ┆ 0.000052  │\n",
      "│ 25-Hydroxyvitamin_D3_level_90     ┆ 3894    ┆ 0.000052  │\n",
      "│ 25-Hydroxyvitamin_D2_level_92     ┆ 3991    ┆ 0.000053  │\n",
      "│ EATINGDISORDERS                   ┆ 4055    ┆ 0.000054  │\n",
      "│ ENDOMETRIOSIS_ADENOMYOSIS         ┆ 4262    ┆ 0.000057  │\n",
      "│ ENDOMETRIOSIS_ADENOMYOSIS_V2      ┆ 4262    ┆ 0.000057  │\n",
      "│ Urine_microalbumin_creatinine_ra… ┆ 4490    ┆ 0.00006   │\n",
      "│ HYPERTHYROIDISM_V2                ┆ 4622    ┆ 0.000062  │\n",
      "│ RHEUMATOIDARTHRITIS               ┆ 4669    ┆ 0.000062  │\n",
      "│ PMRANDGCA                         ┆ 4794    ┆ 0.000064  │\n",
      "│ OSA                               ┆ 4910    ┆ 0.000066  │\n",
      "│ Plasma_ferritin_level_62          ┆ 4963    ┆ 0.000066  │\n",
      "│ Serum_pro-brain_natriuretic_pept… ┆ 5018    ┆ 0.000067  │\n",
      "│ POLYCYSTIC_OVARIAN_SYNDROME_PCOS… ┆ 5902    ┆ 0.000079  │\n",
      "│ OTHER_CHRONIC_LIVER_DISEASE_OPTI… ┆ 5972    ┆ 0.00008   │\n",
      "│ Serum_vitamin_D2_level_89         ┆ 6433    ┆ 0.000086  │\n",
      "│ Albumin___creatinine_ratio_37     ┆ 6542    ┆ 0.000087  │\n",
      "│ PAD_STRICT                        ┆ 6564    ┆ 0.000088  │\n",
      "│ INR_-_international_normalised_r… ┆ 7716    ┆ 0.000103  │\n",
      "│ EPILEPSY                          ┆ 7834    ┆ 0.000105  │\n",
      "│ Serum_N-terminal_pro_B-type_natr… ┆ 8737    ┆ 0.000117  │\n",
      "│ VALVULARDISEASES                  ┆ 8755    ┆ 0.000117  │\n",
      "│ SUBSTANCEMISUSE                   ┆ 9671    ┆ 0.000129  │\n",
      "│ STROKEUNSPECIFIED                 ┆ 11033   ┆ 0.000147  │\n",
      "│ MINFARCTION                       ┆ 11213   ┆ 0.00015   │\n",
      "│ ALL_DEMENTIA                      ┆ 11429   ┆ 0.000153  │\n",
      "│ TSH_level_74                      ┆ 12475   ┆ 0.000167  │\n",
      "│ HF                                ┆ 13208   ┆ 0.000177  │\n",
      "│ Combined_total_vitamin_D2_and_D3… ┆ 13227   ┆ 0.000177  │\n",
      "│ Calcium_adjusted_level_41         ┆ 13962   ┆ 0.000187  │\n",
      "│ GOUT                              ┆ 14173   ┆ 0.000189  │\n",
      "│ OSTEOPOROSIS                      ┆ 14954   ┆ 0.0002    │\n",
      "│ Plasma_free_T4_level_77           ┆ 14970   ┆ 0.0002    │\n",
      "│ PSORIASIS                         ┆ 16091   ┆ 0.000215  │\n",
      "│ AF                                ┆ 16612   ┆ 0.000222  │\n",
      "│ COPD                              ┆ 17248   ┆ 0.000231  │\n",
      "│ PERIPHERAL_NEUROPATHY             ┆ 18080   ┆ 0.000242  │\n",
      "│ Blood_urea_28                     ┆ 19085   ┆ 0.000255  │\n",
      "│ HYPOTHYROIDISM_DRAFT_V1           ┆ 19255   ┆ 0.000257  │\n",
      "│ Serum_total_25-hydroxy_vitamin_D… ┆ 20424   ┆ 0.000273  │\n",
      "│ CKDSTAGE3TO5                      ┆ 23386   ┆ 0.000313  │\n",
      "│ Plasma_cholesterol_HDL_ratio_96   ┆ 23849   ┆ 0.000319  │\n",
      "│ IHD_NOMI                          ┆ 25318   ┆ 0.000338  │\n",
      "│ ALCOHOLMISUSE                     ┆ 25803   ┆ 0.000345  │\n",
      "│ Serum_T4_level_78                 ┆ 26722   ┆ 0.000357  │\n",
      "│ PREVALENT_IBS_V2                  ┆ 27540   ┆ 0.000368  │\n",
      "│ TYPE2DIABETES                     ┆ 27873   ┆ 0.000373  │\n",
      "│ Serum_25-Hydroxy_vitamin_D3_leve… ┆ 29824   ┆ 0.000399  │\n",
      "│ ALLCA_NOBCC_VFINAL                ┆ 33222   ┆ 0.000444  │\n",
      "│ Free_T4_level_76                  ┆ 34651   ┆ 0.000463  │\n",
      "│ DEATH                             ┆ 36448   ┆ 0.000487  │\n",
      "│ AST_-_aspartate_transam._SGOT__4… ┆ 41058   ┆ 0.000549  │\n",
      "│ Total_bilirubin_55                ┆ 41985   ┆ 0.000561  │\n",
      "│ Plasma_corrected_calcium_level_4… ┆ 43463   ┆ 0.000581  │\n",
      "│ Serum_vitamin_D_86                ┆ 43799   ┆ 0.000585  │\n",
      "│ Plasma_LDL_cholesterol_level_104  ┆ 47259   ┆ 0.000632  │\n",
      "│ ANY_DEAFNESS_HEARING_LOSS_V2      ┆ 52275   ┆ 0.000699  │\n",
      "│ Non_HDL_cholesterol_level_108     ┆ 58166   ┆ 0.000777  │\n",
      "│ OSTEOARTHRITIS                    ┆ 62625   ┆ 0.000837  │\n",
      "│ Plasma_TSH_level_73               ┆ 63886   ┆ 0.000854  │\n",
      "│ ALLERGICRHINITISCONJ              ┆ 65975   ┆ 0.000882  │\n",
      "│ Plasma_calcium_level_40           ┆ 67579   ┆ 0.000903  │\n",
      "│ Plasma_triglyceride_level_106     ┆ 76103   ┆ 0.001017  │\n",
      "│ ANXIETY                           ┆ 79085   ┆ 0.001057  │\n",
      "│ HYPERTENSION                      ┆ 79270   ┆ 0.00106   │\n",
      "│ ASTHMA_PUSHASTHMA                 ┆ 82197   ┆ 0.001099  │\n",
      "│ DEPRESSION                        ┆ 90568   ┆ 0.001211  │\n",
      "│ Haemoglobin_A1c_level_8           ┆ 90932   ┆ 0.001215  │\n",
      "│ Plasma_urea_level_30              ┆ 91294   ┆ 0.00122   │\n",
      "│ ATOPICECZEMA                      ┆ 95186   ┆ 0.001272  │\n",
      "│ Total_alkaline_phosphatase_48     ┆ 100228  ┆ 0.00134   │\n",
      "│ TSH_-_thyroid_stim._hormone_72    ┆ 100746  ┆ 0.001347  │\n",
      "│ Plasma_HDL_cholesterol_level_101  ┆ 101886  ┆ 0.001362  │\n",
      "│ Plasma_total_cholesterol_level_9… ┆ 109221  ┆ 0.00146   │\n",
      "│ Haematocrit_-_PCV_16              ┆ 120764  ┆ 0.001614  │\n",
      "│ Plasma_gamma-glutamyl_transferas… ┆ 123248  ┆ 0.001647  │\n",
      "│ Calculated_LDL_cholesterol_level… ┆ 149252  ┆ 0.001995  │\n",
      "│ Plasma_total_bilirubin_level_54   ┆ 151196  ┆ 0.002021  │\n",
      "│ Plasma_alanine_aminotransferase_… ┆ 154986  ┆ 0.002072  │\n",
      "│ Plasma_alkaline_phosphatase_leve… ┆ 155662  ┆ 0.002081  │\n",
      "│ AST_serum_level_47                ┆ 164019  ┆ 0.002192  │\n",
      "│ Current_smoker_83                 ┆ 180549  ┆ 0.002413  │\n",
      "│ Plasma_albumin_level_52           ┆ 180891  ┆ 0.002418  │\n",
      "│ Plasma_potassium_level_27         ┆ 193870  ┆ 0.002591  │\n",
      "│ Plasma_sodium_level_25            ┆ 194250  ┆ 0.002596  │\n",
      "│ Plasma_C_reactive_protein_60      ┆ 195455  ┆ 0.002612  │\n",
      "│ Serum_total_cholesterol_level_98  ┆ 196909  ┆ 0.002632  │\n",
      "│ Serum_non_high_density_lipoprote… ┆ 199041  ┆ 0.00266   │\n",
      "│ Plasma_creatinine_level_32        ┆ 199277  ┆ 0.002664  │\n",
      "│ Serum_folate_80                   ┆ 280513  ┆ 0.003749  │\n",
      "│ HbA1c_level__DCCT_aligned__7      ┆ 282821  ┆ 0.00378   │\n",
      "│ eGFR_using_creatinine__CKD-EPI__… ┆ 284882  ┆ 0.003808  │\n",
      "│ Urine_albumin_creatinine_ratio_3… ┆ 288562  ┆ 0.003857  │\n",
      "│ Serum_vitamin_B12_79              ┆ 326622  ┆ 0.004366  │\n",
      "│ Total_cholesterol_HDL_ratio_95    ┆ 330782  ┆ 0.004421  │\n",
      "│ Corrected_serum_calcium_level_42  ┆ 337637  ┆ 0.004513  │\n",
      "│ Serum_C_reactive_protein_level_5… ┆ 352095  ┆ 0.004706  │\n",
      "│ Serum_ferritin_63                 ┆ 397341  ┆ 0.005311  │\n",
      "│ Serum_free_T4_level_75            ┆ 421082  ┆ 0.005628  │\n",
      "│ Serum_bilirubin_level_53          ┆ 458640  ┆ 0.00613   │\n",
      "│ Serum_cholesterol_HDL_ratio_94    ┆ 466971  ┆ 0.006242  │\n",
      "│ Serum_gamma-glutamyl_transferase… ┆ 517541  ┆ 0.006917  │\n",
      "│ International_normalised_ratio_8… ┆ 563237  ┆ 0.007528  │\n",
      "│ Ex_smoker_84                      ┆ 584869  ┆ 0.007817  │\n",
      "│ Serum_LDL_cholesterol_level_102   ┆ 589329  ┆ 0.007877  │\n",
      "│ Erythrocyte_sedimentation_rate_6… ┆ 610389  ┆ 0.008158  │\n",
      "│ Serum_calcium_39                  ┆ 645535  ┆ 0.008628  │\n",
      "│ Haemoglobin_A1c_level_-_IFCC_sta… ┆ 670222  ┆ 0.008958  │\n",
      "│ Serum_triglycerides_105           ┆ 872054  ┆ 0.011656  │\n",
      "│ Red_blood_cell_distribution_widt… ┆ 978501  ┆ 0.013079  │\n",
      "│ Serum_HDL_cholesterol_level_100   ┆ 979267  ┆ 0.013089  │\n",
      "│ Serum_cholesterol_97              ┆ 1038765 ┆ 0.013884  │\n",
      "│ Serum_alanine_aminotransferase_l… ┆ 1044872 ┆ 0.013966  │\n",
      "│ Serum_TSH_level_71                ┆ 1072044 ┆ 0.014329  │\n",
      "│ Mean_corpusc._Hb._conc.__MCHC__1… ┆ 1137895 ┆ 0.015209  │\n",
      "│ Never_smoked_tobacco_85           ┆ 1173432 ┆ 0.015684  │\n",
      "│ Serum_total_bilirubin_level_56    ┆ 1187675 ┆ 0.015874  │\n",
      "│ O_E_-_height_1                    ┆ 1278970 ┆ 0.017095  │\n",
      "│ Haematocrit_15                    ┆ 1409203 ┆ 0.018835  │\n",
      "│ GFR_calculated_abbreviated_MDRD_… ┆ 1603129 ┆ 0.021427  │\n",
      "│ Serum_alkaline_phosphatase_50     ┆ 1665537 ┆ 0.022262  │\n",
      "│ Serum_albumin_51                  ┆ 1682434 ┆ 0.022487  │\n",
      "│ Serum_urea_level_29               ┆ 1792360 ┆ 0.023957  │\n",
      "│ Basophil_count_22                 ┆ 1814981 ┆ 0.024259  │\n",
      "│ Mean_corpusc._haemoglobin_MCH__1… ┆ 1834840 ┆ 0.024524  │\n",
      "│ Monocyte_count_23                 ┆ 1907620 ┆ 0.025497  │\n",
      "│ Eosinophil_count_21               ┆ 1928177 ┆ 0.025772  │\n",
      "│ Red_blood_cell__RBC__count_10     ┆ 1935962 ┆ 0.025876  │\n",
      "│ Lymphocyte_count_20               ┆ 1945738 ┆ 0.026007  │\n",
      "│ Neutrophil_count_19               ┆ 1956057 ┆ 0.026145  │\n",
      "│ Mean_corpuscular_volume__MCV__11  ┆ 1986695 ┆ 0.026554  │\n",
      "│ Body_mass_index_3                 ┆ 1988101 ┆ 0.026573  │\n",
      "│ Platelet_count_12                 ┆ 2017132 ┆ 0.026961  │\n",
      "│ Total_white_cell_count_18         ┆ 2018520 ┆ 0.02698   │\n",
      "│ Serum_potassium_26                ┆ 2040231 ┆ 0.02727   │\n",
      "│ Haemoglobin_estimation_9          ┆ 2061258 ┆ 0.027551  │\n",
      "│ Serum_sodium_24                   ┆ 2077867 ┆ 0.027773  │\n",
      "│ Serum_creatinine_31               ┆ 2138578 ┆ 0.028584  │\n",
      "│ O_E_-_weight_2                    ┆ 2283852 ┆ 0.030526  │\n",
      "│ Systolic_blood_pressure_4         ┆ 5269951 ┆ 0.070438  │\n",
      "│ Diastolic_blood_pressure_5        ┆ 5271477 ┆ 0.070459  │\n",
      "└───────────────────────────────────┴─────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "pl.Config.set_tbl_rows(vocab_size + 1)\n",
    "print(dm.tokenizer._event_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create models and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using Temporal Positional Encoding. This module uses the patient's age at an event within their time series.\n",
      "INFO:root:Using ExponentialTTELayer. This module predicts the time until next event as an exponential distribution\n",
      "INFO:root:Using Temporal Positional Encoding. This module uses the patient's age at an event within their time series.\n",
      "INFO:root:Using GeometricTTELayer. This module predicts the time until next event as a geometric distribution, supported on the set {0,1,...}\n"
     ]
    }
   ],
   "source": [
    "models, m_names = [], []\n",
    "\n",
    "# My development model\n",
    "for tte_layer in [\"Exponential\", \"Geometric\"]:\n",
    "    config = DemoConfig()\n",
    "    config.TTELayer = tte_layer\n",
    "    models.append(TTETransformerForCausalSequenceModelling(config, vocab_size).to(device))\n",
    "    m_names.append(f\"TPPTransformerForCausalSequenceModelling: {tte_layer} TTE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_curves_train = [[] for _ in models]\n",
    "loss_curves_train_clf = [[] for _ in models]\n",
    "loss_curves_train_tte = [[] for _ in models]\n",
    "\n",
    "loss_curves_val = [[] for _ in models]\n",
    "loss_curves_val_clf = [[] for _ in models]\n",
    "loss_curves_val_tte = [[] for _ in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hoelkOrFY8bN",
    "outputId": "961304cd-e379-40d4-dd56-8de0b91d2861"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model `TPPTransformerForCausalSequenceModelling: Exponential TTE`, with 10.853185 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   0%|          | 51/29148 [00:35<5:40:24,  1.42it/s]\n",
      "Validation epoch 0:   2%|▏         | 21/1116 [00:06<05:55,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\tTrain loss 0.62  (2.57, -1.33). Val loss 0.12 (1.77, -1.53)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   0%|          | 21/29148 [00:16<6:20:47,  1.27it/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for m_idx, (model, m_name) in enumerate(zip(models, m_names)):\n",
    "    \n",
    "    print(f\"Training model `{m_name}`, with {sum(p.numel() for p in model.parameters())/1e6} M parameters\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # create a PyTorch optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=opt.learning_rate)\n",
    "\n",
    "    best_val, epochs_since_best = np.inf, 0\n",
    "    for epoch in range(opt.epochs):\n",
    "        epoch_loss, epoch_clf_loss, epoch_tte_loss = 0, 0, 0\n",
    "        model.train()\n",
    "        for i, batch in tqdm(enumerate(dm.train_dataloader()), desc=f\"Training epoch {epoch}\", total=len(dm.train_dataloader())):\n",
    "            if i > 50:\n",
    "                break\n",
    "                \n",
    "            # evaluate the loss\n",
    "            _, (loss_clf, loss_tte), loss = model(batch['tokens'].to(device), \n",
    "                                                  ages=batch['ages'].to(device), \n",
    "                                                  attention_mask=batch['attention_mask'].to(device)  \n",
    "                                                  )\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            # record\n",
    "            epoch_clf_loss += loss_clf.item()\n",
    "            epoch_tte_loss += loss_tte.item()\n",
    "        epoch_loss /= i\n",
    "        epoch_clf_loss /= i\n",
    "        epoch_tte_loss /= i\n",
    "        loss_curves_train[m_idx].append(epoch_loss)\n",
    "        loss_curves_train_clf[m_idx].append(epoch_clf_loss)\n",
    "        loss_curves_train_tte[m_idx].append(epoch_tte_loss)\n",
    "\n",
    "        # evaluate the loss on val set\n",
    "        with torch.no_grad(): \n",
    "            model.eval()\n",
    "            if epoch % opt.eval_interval == 0 or epoch == opt.epochs - 1:\n",
    "                val_loss, val_clf_loss, val_tte_loss = 0, 0, 0\n",
    "                for j, batch in tqdm(enumerate(dm.val_dataloader()), desc=f\"Validation epoch {epoch}\", total=len(dm.val_dataloader())):\n",
    "                    if j > 20:\n",
    "                        break\n",
    "                    _, (loss_clf, loss_tte), loss = model(batch['tokens'].to(device), \n",
    "                                                          ages=batch['ages'].to(device),\n",
    "                                                          attention_mask=batch['attention_mask'].to(device) \n",
    "                                                          )\n",
    "                    val_loss += loss.item()\n",
    "                    # record\n",
    "                    val_clf_loss += loss_clf.item()\n",
    "                    val_tte_loss += loss_tte.item()\n",
    "                val_loss /= j\n",
    "                val_clf_loss /= j\n",
    "                val_tte_loss /= j\n",
    "                loss_curves_val[m_idx].append(val_loss)\n",
    "                loss_curves_val_clf[m_idx].append(val_clf_loss)\n",
    "                loss_curves_val_tte[m_idx].append(val_tte_loss)\n",
    "                print(f\"Epoch {epoch}:\\tTrain loss {epoch_loss:.2f}  ({epoch_clf_loss:.2f}, {epoch_tte_loss:.2f}). Val loss {val_loss:.2f} ({val_clf_loss:.2f}, {val_tte_loss:.2f})\")          \n",
    "                # TODO: Note not fully accurate as last batch is likely not the same size, will be fixed with lightning\n",
    "        \n",
    "            if val_loss >= best_val:\n",
    "                epochs_since_best += 1\n",
    "                if epochs_since_best >= 5:\n",
    "                    break\n",
    "            else:\n",
    "                best_val = val_loss\n",
    "                epochs_since_best = 0\n",
    "\n",
    "    # Test trained model with a prompt\n",
    "    # ----------------    \n",
    "    # set context: diagnosis of depression at 20 years old\n",
    "    tokens = torch.from_numpy(np.array(dm.encode([\"DEPRESSION\"])).reshape((1,-1))).to(device)\n",
    "    ages = torch.tensor([[20*365]], device=device)\n",
    "    # values = torch.tensor([[torch.nan]], device=device)\n",
    "    # generate: sample the next 10 tokens\n",
    "    new_tokens, new_ages = model.generate(tokens, ages, max_new_tokens=10)\n",
    "    generated = dm.decode(new_tokens[0].tolist())\n",
    "    # report:\n",
    "    #    note, Not considering value yet.\n",
    "    for _cat, _age in zip(generated.split(\" \"), new_ages[0, :]):\n",
    "        print(f\"\\t {_cat} at age {_age/365:.0f} ({_age:.1f} days)\")    # with value {_value}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing output to real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dm.train_dataloader():\n",
    "    break\n",
    "conditions = batch[\"tokens\"].numpy().tolist()\n",
    "# delta_ages = batch[\"ages\"][:, 1:] - batch[\"ages\"][:, :-1]\n",
    "for idx, (token, age) in enumerate(zip(conditions[0], batch[\"ages\"][0,:])):\n",
    "    if token == 0 or idx >= 10:\n",
    "        break\n",
    "    print(f\"{dm.decode([token])}, at age {age/365:.0f} ({age:.1f} days)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"k\", \"r\", \"b\", \"y\"]\n",
    "\n",
    "# Plot loss\n",
    "plt.figure()\n",
    "for m_idx, _ in enumerate(models):\n",
    "    # Training\n",
    "    iterations = np.linspace(0, len(loss_curves_train[m_idx]), len(loss_curves_train[m_idx])) * opt.eval_interval\n",
    "    plt.plot(iterations, loss_curves_train[m_idx], label=f\"{m_names[m_idx]}-train\", c=cols[m_idx], linestyle='dashed')\n",
    "    # Validation\n",
    "    iterations = np.linspace(0, len(loss_curves_val[m_idx]), len(loss_curves_val[m_idx])) * opt.eval_interval\n",
    "    plt.plot(iterations, loss_curves_val[m_idx], label=f\"{m_names[m_idx]}-val\", c=cols[m_idx])\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"figs/TTE/logloss.png\")\n",
    "\n",
    "# Plot classifier loss\n",
    "plt.figure()\n",
    "for m_idx, _ in enumerate(models):\n",
    "    # Training\n",
    "    iterations = np.linspace(0, len(loss_curves_train_clf[m_idx]), len(loss_curves_train_clf[m_idx])) * opt.eval_interval\n",
    "    plt.plot(iterations, loss_curves_train_clf[m_idx], label=f\"{m_names[m_idx]}-train\", c=cols[m_idx], linestyle='dashed')\n",
    "    # Validation\n",
    "    iterations = np.linspace(0, len(loss_curves_val_clf[m_idx]), len(loss_curves_val_clf[m_idx])) * opt.eval_interval\n",
    "    plt.plot(iterations, loss_curves_val_clf[m_idx], label=f\"{m_names[m_idx]}-val\", c=cols[m_idx])\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"figs/TTE/logloss_clf.png\")\n",
    "\n",
    "# Plot tte loss\n",
    "plt.figure()\n",
    "for m_idx, _ in enumerate(models):\n",
    "    # Training\n",
    "    iterations = np.linspace(0, len(loss_curves_train_tte[m_idx]), len(loss_curves_train_tte[m_idx])) * opt.eval_interval\n",
    "    plt.plot(iterations, loss_curves_train_tte[m_idx], label=f\"{m_names[m_idx]}-train\", c=cols[m_idx], linestyle='dashed')\n",
    "    # Validation\n",
    "    iterations = np.linspace(0, len(loss_curves_val_tte[m_idx]), len(loss_curves_val_tte[m_idx])) * opt.eval_interval\n",
    "    plt.plot(iterations, loss_curves_val_tte[m_idx], label=f\"{m_names[m_idx]}-val\", c=cols[m_idx])\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"figs/TTE/logloss_tte.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diabetes: How related conditions are impacted by each other\n",
    "Probability of type II diabetes before and after a type I diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_token = dm.tokenizer._stoi[\"TYPE1DM\"]\n",
    "t2_token = dm.tokenizer._stoi[\"TYPE2DIABETES\"]\n",
    "\n",
    "\n",
    "base_prompt = [\"DEPRESSION\"]\n",
    "ages_in_years = [20]\n",
    "\n",
    "to_days = lambda a_list: torch.FloatTensor([365 * _a for _a in a_list]).reshape((1,-1)).to(device)\n",
    "\n",
    "# Create a set of prompts\n",
    "prompts, ages, values, desc = [], [], [], []\n",
    "desc.append(\"Control\")\n",
    "prompts.append(base_prompt)\n",
    "ages.append(ages_in_years)\n",
    "desc.append(\"Type 1\")\n",
    "prompts.append(base_prompt + [\"TYPE1DM\"])\n",
    "ages.append(ages_in_years + [21])\n",
    "desc.append(\"Type 2\")\n",
    "prompts.append(base_prompt + [\"TYPE2DIABETES\"])\n",
    "ages.append(ages_in_years + [21])\n",
    "\n",
    "for model_idx, model in enumerate(models):\n",
    "    print(f\"\\n\\n{m_names[model_idx]}\\n--------------------------------------\")\n",
    "    with torch.no_grad(): \n",
    "            model.eval()\n",
    "    \n",
    "            for p_idx, (prompt, age) in enumerate(zip(prompts, ages)):\n",
    "                print(f\"\\n{desc[p_idx]}: \\t ({','.join(prompt)}): \")\n",
    "                encoded_prompt = torch.from_numpy(np.array(dm.encode(prompt)).reshape((1,-1))).to(device)\n",
    "                (lgts, tte_dist), _, _ = model(encoded_prompt,\n",
    "                                                       # values=torch.tensor(value).to(device),\n",
    "                                                       ages=to_days(age),\n",
    "                                                       is_generation=True)\n",
    "                probs = torch.nn.functional.softmax(lgts, dim=2)\n",
    "                print(f\"\\tprobability of type I diabetes: {100*float(probs[0, 0, t1_token].cpu().detach().numpy()):.4f}%\")\n",
    "                print(f\"\\tprobability of type II diabetes: {100*float(probs[0, 0, t2_token].cpu().detach().numpy()):.4f}%\")\n",
    "\n",
    "# Note: adding a diagnosis (even if potentially orthogonal) at the beginning of the prompt increases probability of either type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Age: How increasing prompt age affects likelihood of age related diagnoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\"ALLERGICRHINITISCONJ\"]\n",
    "ages = [[4],[8],[20],[30],[60],[80],[90]]\n",
    "\n",
    "# target_conditions=[\"TYPE1DM\"]#, \"TYPE2DIABETES\", \"OSTEOARTHRITIS\", \"ANY_DEAFNESS_HEARING_LOSS\"]\n",
    "\n",
    "for model_idx, model in enumerate(models):\n",
    "    print(f\"\\n\\n{m_names[model_idx]}\\n--------------------------------------\")\n",
    "\n",
    "    # for condition in target_conditions:\n",
    "    #     print(f\"Probability of {condition}\")\n",
    "    #     target_token = dm.tokenizer._stoi[condition]\n",
    "\n",
    "    for p_idx, age in enumerate(ages):\n",
    "        print(f\"\\nAge {age[-1]}\\n======\")\n",
    "        encoded_prompt = torch.from_numpy(np.array(dm.encode(prompt)).reshape((1,-1))).to(device)\n",
    "        (lgts, tte_dist), _, _ = model(encoded_prompt,\n",
    "                                       ages=to_days(age),\n",
    "                                       is_generation=True)\n",
    "        probs = torch.nn.functional.softmax(lgts, dim=2) * 100\n",
    "\n",
    "        # top K\n",
    "        k = 10\n",
    "        print(f\"Top {k}\")\n",
    "        topk_prob, topk_ind = torch.topk(probs[0,0,:], k)\n",
    "        for i, j in zip(dm.decode(topk_ind.tolist()).split(\" \"), topk_prob):\n",
    "            print(f\"\\t{i}: {j:.2f}%\")\n",
    "\n",
    "        # bottom K\n",
    "        k = 30\n",
    "        print(f\"Bottom {k}\")\n",
    "        topk_prob, topk_ind = torch.topk(-probs[0,0,:], k)\n",
    "        for i, j in zip(dm.decode(topk_ind.tolist()).split(\" \"), topk_prob):\n",
    "            print(f\"\\t{i}: {-j:.2f}%\")\n",
    "        \n",
    "            # print(f\"Age: {age[-1]} years old:  {100*float(probs[0, 0, target_token].cpu().detach().numpy()):.4f}%\")\n",
    "\n",
    "# Note: adding a diagnosis (even if potentially orthogonal) at the beginning of the prompt increases probability of either type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: model architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_idx, model in enumerate(models):\n",
    "    print(f\"\\n\\n{m_names[model_idx]}\\n\" + \"=\"*len(m_names[model_idx]))\n",
    "    print(f\"\\n\\n{model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to html --no-input TTE.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "sys_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
