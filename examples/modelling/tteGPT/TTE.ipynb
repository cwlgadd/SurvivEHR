{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJpXpmjEYC_T"
   },
   "source": [
    "# Demo Notebook:\n",
    "## Time to Event Transformer For Causal Sequence Modelling \n",
    "\n",
    "Including time, and excluding values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "/rds/homes/g/gaddcz/Projects/CPRD/examples/modelling/tteGPT\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning \n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sqlite3\n",
    "from dataclasses import dataclass\n",
    "import logging\n",
    "from CPRD.data.foundational_loader import FoundationalDataModule\n",
    "from CPRD.src.models.TTE.task_heads.causal import TTETransformerForCausalSequenceModelling\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# TODO:\n",
    "# replace experiment boilerplate with pytorch lightning\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "# device = \"cpu\"    # if more informative debugging statements are needed\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set config to be equivalent architecture of kaparthy benchmark, however they are not comparable tasks.\n",
    "@dataclass\n",
    "class DemoConfig:\n",
    "    block_size: int = 256        # what is the maximum context length for predictions?\n",
    "    n_layer: int = 6\n",
    "    n_head: int = 6\n",
    "    n_embd: int = 384\n",
    "    bias: bool = True\n",
    "    attention_type: str = \"global\"    \n",
    "    dropout: float = 0.0\n",
    "    unk_freq_threshold: float = 0.0\n",
    "    TTELayer = \"Exponential\"\n",
    "\n",
    "config = DemoConfig()\n",
    "\n",
    "@dataclass\n",
    "class OptConfig:\n",
    "    batch_size: int = 128\n",
    "    eval_interval: int = 1\n",
    "    learning_rate: float = 3e-4\n",
    "    epochs: int = 50\n",
    "    \n",
    "opt = OptConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n"
     ]
    }
   ],
   "source": [
    "print(os.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data loader on a reduced cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading Polars dataset from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/archive/Version2/polars/\n",
      "INFO:root:Using tokenizer tabular\n",
      "INFO:root:Tokenzier created based on 80.20M tokens\n",
      "INFO:root:Creating dataset\n",
      "Calculating chunk index splits : 27it [00:05,  4.96it/s]\n",
      "INFO:root:Creating dataset\n",
      "Calculating chunk index splits : 2it [00:00,  5.30it/s]\n",
      "INFO:root:Creating dataset\n",
      "Calculating chunk index splits : 2it [00:00,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "433410 training patients\n",
      "47449 validation patients\n",
      "31966 test patients\n",
      "184 vocab elements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Get a list of patients which fit a reduced set of criterion\n",
    "path_to_db = \"/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/archive/Version2/\"\n",
    "\n",
    "# Build \n",
    "dm = FoundationalDataModule(path_to_db=path_to_db,\n",
    "                            load=True,\n",
    "                            tokenizer=\"tabular\",\n",
    "                            batch_size=opt.batch_size,\n",
    "                            max_seq_length=config.block_size,\n",
    "                            unk_freq_threshold=config.unk_freq_threshold,\n",
    "                            min_workers=10, # os.cpu_count()\n",
    "                           )\n",
    "\n",
    "vocab_size = dm.train_set.tokenizer.vocab_size\n",
    "\n",
    "print(f\"{len(dm.train_set)} training patients\")\n",
    "print(f\"{len(dm.val_set)} validation patients\")\n",
    "print(f\"{len(dm.test_set)} test patients\")\n",
    "print(f\"{vocab_size} vocab elements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.57733154296875\n",
      "0.0002613067626953125\n",
      "0.00018334388732910156\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_1392332/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">1470451186.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">13</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_1392332/1470451186.py'</span>                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_1392332/\u001b[0m\u001b[1;33m1470451186.py\u001b[0m:\u001b[94m13\u001b[0m in \u001b[92m<module>\u001b[0m                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_1392332/1470451186.py'\u001b[0m                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# start = time.time()   # starting time\n",
    "# for row_idx, row in enumerate(dm.train_set):\n",
    "#     print(time.time() - start)\n",
    "#     start = time.time()\n",
    "#     if row_idx > opt.batch_size - 1:\n",
    "#         break\n",
    "\n",
    "start = time.time()   # starting time\n",
    "for batch_idx, batch in enumerate(dm.train_dataloader()):\n",
    "    print(time.time() - start)\n",
    "    time.sleep(np.abs(np.random.normal(10,0.5)))\n",
    "    start = time.time()\n",
    "    if batch_idx > 300:\n",
    "        break\n",
    "# print(f\"{row} loaded in {time.time()-start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': tensor([[182, 183, 182,  ..., 177, 179, 129],\n",
       "         [173, 169, 172,  ..., 152, 147, 155],\n",
       "         [158, 155, 176,  ..., 174, 171, 153],\n",
       "         ...,\n",
       "         [ 85, 183, 182,  ..., 145, 145, 145],\n",
       "         [108, 183, 182,  ...,   0,   0,   0],\n",
       "         [176,   0,   0,  ...,   0,   0,   0]]),\n",
       " 'ages': tensor([[21564, 21730, 21730,  ..., 27527, 27527, 27527],\n",
       "         [20875, 20875, 20875,  ..., 24195, 24195, 24195],\n",
       "         [26804, 26931, 26939,  ..., 29258, 29258, 29258],\n",
       "         ...,\n",
       "         [19894, 20807, 20807,  ..., 25815, 25843, 25864],\n",
       "         [ 3234,  6880,  6880,  ...,     0,     0,     0],\n",
       "         [ 1645,     0,     0,  ...,     0,     0,     0]]),\n",
       " 'values': tensor([[1.6136e-03, 2.3238e-04, 1.6418e-03,  ..., 3.3570e-04, 1.7445e-01,\n",
       "          4.2975e-03],\n",
       "         [1.7375e-02, 5.7208e-04, 7.0428e-03,  ..., 7.1763e-04, 6.1852e-03,\n",
       "          1.3343e-03],\n",
       "         [0.0000e+00, 1.3803e-03, 8.2428e-08,  ..., 4.6714e-07, 1.9003e-09,\n",
       "                 nan],\n",
       "         ...,\n",
       "         [       nan, 2.2968e-04, 1.6136e-03,  ..., 7.7922e-04, 9.4156e-04,\n",
       "          8.7662e-04],\n",
       "         [       nan, 2.1617e-04, 1.3879e-03,  ...,        nan,        nan,\n",
       "                 nan],\n",
       "         [4.8883e-04,        nan,        nan,  ...,        nan,        nan,\n",
       "                 nan]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 0, 0,  ..., 0, 0, 0]])}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (183, 3)\n",
      "┌───────────────────────────────────┬─────────┬───────────┐\n",
      "│ EVENT                             ┆ COUNT   ┆ FREQUENCY │\n",
      "│ ---                               ┆ ---     ┆ ---       │\n",
      "│ str                               ┆ u32     ┆ f64       │\n",
      "╞═══════════════════════════════════╪═════════╪═══════════╡\n",
      "│ UNK                               ┆ 0       ┆ 0.0       │\n",
      "│ Plasma_N_terminal_pro_B_type_nat… ┆ 39      ┆ 4.8626e-7 │\n",
      "│ CYSTICFIBROSIS                    ┆ 135     ┆ 0.000002  │\n",
      "│ SICKLE_CELL_DISEASE_V2            ┆ 136     ┆ 0.000002  │\n",
      "│ SYSTEMIC_SCLEROSIS                ┆ 211     ┆ 0.000003  │\n",
      "│ ADDISON_DISEASE                   ┆ 250     ┆ 0.000003  │\n",
      "│ DOWNSSYNDROME                     ┆ 383     ┆ 0.000005  │\n",
      "│ PLASMACELL_NEOPLASM               ┆ 426     ┆ 0.000005  │\n",
      "│ HAEMOCHROMATOSIS_V2               ┆ 536     ┆ 0.000007  │\n",
      "│ SJOGRENSSYNDROME                  ┆ 557     ┆ 0.000007  │\n",
      "│ SYSTEMIC_LUPUS_ERYTHEMATOSUS      ┆ 604     ┆ 0.000008  │\n",
      "│ N_terminal_pro_brain_natriuretic… ┆ 673     ┆ 0.000008  │\n",
      "│ HIVAIDS                           ┆ 982     ┆ 0.000012  │\n",
      "│ Blood_calcium_level_38            ┆ 1079    ┆ 0.000013  │\n",
      "│ MS                                ┆ 1158    ┆ 0.000014  │\n",
      "│ LEUKAEMIA_PREVALENCE              ┆ 1268    ┆ 0.000016  │\n",
      "│ PSORIATICARTHRITIS2021            ┆ 1275    ┆ 0.000016  │\n",
      "│ Plasma_B_natriuretic_peptide_lev… ┆ 1435    ┆ 0.000018  │\n",
      "│ ILD_SH                            ┆ 1437    ┆ 0.000018  │\n",
      "│ CHRONIC_LIVER_DISEASE_ALCOHOL     ┆ 1688    ┆ 0.000021  │\n",
      "│ PERNICIOUSANAEMIA                 ┆ 1707    ┆ 0.000021  │\n",
      "│ CHRONICFATIGUESYNDROMEMM          ┆ 1764    ┆ 0.000022  │\n",
      "│ CROHNS_DISEASE                    ┆ 1800    ┆ 0.000022  │\n",
      "│ MENIERESDISEASE                   ┆ 1810    ┆ 0.000023  │\n",
      "│ Brain_natriuretic_peptide_level_… ┆ 1836    ┆ 0.000023  │\n",
      "│ LYMPHOMA_PREVALENCE               ┆ 1863    ┆ 0.000023  │\n",
      "│ STROKE_HAEMRGIC                   ┆ 1932    ┆ 0.000024  │\n",
      "│ PARKINSONS                        ┆ 2062    ┆ 0.000026  │\n",
      "│ BIPOLAR                           ┆ 2249    ┆ 0.000028  │\n",
      "│ Plasma_pro_brain_natriuretic_pep… ┆ 2628    ┆ 0.000033  │\n",
      "│ AORTICANEURYSM                    ┆ 2668    ┆ 0.000033  │\n",
      "│ ULCERATIVE_COLITIS                ┆ 2697    ┆ 0.000034  │\n",
      "│ BRONCHIECTASIS                    ┆ 2816    ┆ 0.000035  │\n",
      "│ SCHIZOPHRENIAMM                   ┆ 2833    ┆ 0.000035  │\n",
      "│ PTSDDIAGNOSIS                     ┆ 2889    ┆ 0.000036  │\n",
      "│ TYPE1DM                           ┆ 3184    ┆ 0.00004   │\n",
      "│ LEARNINGDISABILITY_V2             ┆ 3267    ┆ 0.000041  │\n",
      "│ AUTISM                            ┆ 3327    ┆ 0.000041  │\n",
      "│ VISUAL_IMPAIRMENT                 ┆ 3512    ┆ 0.000044  │\n",
      "│ ISCHAEMICSTROKE                   ┆ 3774    ┆ 0.000047  │\n",
      "│ FIBROMYALGIA                      ┆ 3858    ┆ 0.000048  │\n",
      "│ NAFLD                             ┆ 4095    ┆ 0.000051  │\n",
      "│ EATINGDISORDERS                   ┆ 4289    ┆ 0.000053  │\n",
      "│ ENDOMETRIOSIS_ADENOMYOSIS         ┆ 4562    ┆ 0.000057  │\n",
      "│ ENDOMETRIOSIS_ADENOMYOSIS_V2      ┆ 4562    ┆ 0.000057  │\n",
      "│ HYPERTHYROIDISM_V2                ┆ 4963    ┆ 0.000062  │\n",
      "│ RHEUMATOIDARTHRITIS               ┆ 4992    ┆ 0.000062  │\n",
      "│ Plasma_ferritin_level_62          ┆ 5002    ┆ 0.000062  │\n",
      "│ PMRANDGCA                         ┆ 5031    ┆ 0.000063  │\n",
      "│ Serum_pro_brain_natriuretic_pept… ┆ 5092    ┆ 0.000063  │\n",
      "│ OSA                               ┆ 5353    ┆ 0.000067  │\n",
      "│ Urine_microalbumin_creatinine_ra… ┆ 5785    ┆ 0.000072  │\n",
      "│ Total_25_hydroxyvitamin_D_level_… ┆ 5862    ┆ 0.000073  │\n",
      "│ OTHER_CHRONIC_LIVER_DISEASE_OPTI… ┆ 6394    ┆ 0.00008   │\n",
      "│ POLYCYSTIC_OVARIAN_SYNDROME_PCOS… ┆ 6518    ┆ 0.000081  │\n",
      "│ Albumin___creatinine_ratio_37     ┆ 6758    ┆ 0.000084  │\n",
      "│ Serum_vitamin_D2_level_89         ┆ 6898    ┆ 0.000086  │\n",
      "│ PAD_STRICT                        ┆ 6912    ┆ 0.000086  │\n",
      "│ EPILEPSY                          ┆ 8340    ┆ 0.000104  │\n",
      "│ INR___international_normalised_r… ┆ 9021    ┆ 0.000112  │\n",
      "│ Serum_N_terminal_pro_B_type_natr… ┆ 9243    ┆ 0.000115  │\n",
      "│ VALVULARDISEASES                  ┆ 9296    ┆ 0.000116  │\n",
      "│ 25_Hydroxyvitamin_D3_level_90     ┆ 10010   ┆ 0.000125  │\n",
      "│ 25_Hydroxyvitamin_D2_level_92     ┆ 10102   ┆ 0.000126  │\n",
      "│ SUBSTANCEMISUSE                   ┆ 10385   ┆ 0.000129  │\n",
      "│ STROKEUNSPECIFIED                 ┆ 11750   ┆ 0.000147  │\n",
      "│ MINFARCTION                       ┆ 11899   ┆ 0.000148  │\n",
      "│ ALL_DEMENTIA                      ┆ 12220   ┆ 0.000152  │\n",
      "│ Combined_total_vitamin_D2_and_D3… ┆ 13241   ┆ 0.000165  │\n",
      "│ HF                                ┆ 13963   ┆ 0.000174  │\n",
      "│ Plasma_free_T4_level_77           ┆ 15085   ┆ 0.000188  │\n",
      "│ GOUT                              ┆ 15096   ┆ 0.000188  │\n",
      "│ TSH_level_74                      ┆ 15216   ┆ 0.00019   │\n",
      "│ OSTEOPOROSIS                      ┆ 16012   ┆ 0.0002    │\n",
      "│ PSORIASIS                         ┆ 17155   ┆ 0.000214  │\n",
      "│ AF                                ┆ 17696   ┆ 0.000221  │\n",
      "│ COPD                              ┆ 18381   ┆ 0.000229  │\n",
      "│ Calcium_adjusted_level_41         ┆ 18661   ┆ 0.000233  │\n",
      "│ PERIPHERAL_NEUROPATHY             ┆ 19202   ┆ 0.000239  │\n",
      "│ Blood_urea_28                     ┆ 20489   ┆ 0.000255  │\n",
      "│ HYPOTHYROIDISM_DRAFT_V1           ┆ 21456   ┆ 0.000268  │\n",
      "│ Plasma_cholesterol_HDL_ratio_96   ┆ 23878   ┆ 0.000298  │\n",
      "│ CKDSTAGE3TO5                      ┆ 25246   ┆ 0.000315  │\n",
      "│ Serum_T4_level_78                 ┆ 26956   ┆ 0.000336  │\n",
      "│ IHD_NOMI                          ┆ 27181   ┆ 0.000339  │\n",
      "│ PREVALENT_IBS_V2                  ┆ 28955   ┆ 0.000361  │\n",
      "│ TYPE2DIABETES                     ┆ 30317   ┆ 0.000378  │\n",
      "│ Serum_total_25_hydroxy_vitamin_D… ┆ 30979   ┆ 0.000386  │\n",
      "│ ALCOHOLMISUSE                     ┆ 31555   ┆ 0.000393  │\n",
      "│ Serum_25_Hydroxy_vitamin_D3_leve… ┆ 33883   ┆ 0.000422  │\n",
      "│ ALLCA_NOBCC_VFINAL                ┆ 35375   ┆ 0.000441  │\n",
      "│ Free_T4_level_76                  ┆ 35394   ┆ 0.000441  │\n",
      "│ DEATH                             ┆ 38690   ┆ 0.000482  │\n",
      "│ AST___aspartate_transam_SGOT__46  ┆ 41243   ┆ 0.000514  │\n",
      "│ Plasma_corrected_calcium_level_4… ┆ 43505   ┆ 0.000542  │\n",
      "│ Total_bilirubin_55                ┆ 44241   ┆ 0.000552  │\n",
      "│ Plasma_LDL_cholesterol_level_104  ┆ 47291   ┆ 0.00059   │\n",
      "│ Serum_vitamin_D_86                ┆ 48442   ┆ 0.000604  │\n",
      "│ ANY_DEAFNESS_HEARING_LOSS_V2      ┆ 55438   ┆ 0.000691  │\n",
      "│ Non_HDL_cholesterol_level_108     ┆ 60638   ┆ 0.000756  │\n",
      "│ Plasma_TSH_level_73               ┆ 64049   ┆ 0.000799  │\n",
      "│ OSTEOARTHRITIS                    ┆ 66451   ┆ 0.000829  │\n",
      "│ Plasma_calcium_level_40           ┆ 67621   ┆ 0.000843  │\n",
      "│ ALLERGICRHINITISCONJ              ┆ 71148   ┆ 0.000887  │\n",
      "│ Plasma_triglyceride_level_106     ┆ 76153   ┆ 0.000949  │\n",
      "│ ANXIETY                           ┆ 83850   ┆ 0.001045  │\n",
      "│ HYPERTENSION                      ┆ 85566   ┆ 0.001067  │\n",
      "│ ASTHMA_PUSHASTHMA                 ┆ 88352   ┆ 0.001102  │\n",
      "│ Plasma_urea_level_30              ┆ 91369   ┆ 0.001139  │\n",
      "│ Haemoglobin_A1c_level_8           ┆ 96109   ┆ 0.001198  │\n",
      "│ DEPRESSION                        ┆ 96216   ┆ 0.0012    │\n",
      "│ ATOPICECZEMA                      ┆ 101828  ┆ 0.00127   │\n",
      "│ Plasma_HDL_cholesterol_level_101  ┆ 101969  ┆ 0.001271  │\n",
      "│ TSH___thyroid_stim_hormone_72     ┆ 102353  ┆ 0.001276  │\n",
      "│ Plasma_total_cholesterol_level_9… ┆ 109526  ┆ 0.001366  │\n",
      "│ Total_alkaline_phosphatase_48     ┆ 116121  ┆ 0.001448  │\n",
      "│ Haematocrit___PCV_16              ┆ 121361  ┆ 0.001513  │\n",
      "│ Plasma_gamma_glutamyl_transferas… ┆ 123332  ┆ 0.001538  │\n",
      "│ Plasma_total_bilirubin_level_54   ┆ 151298  ┆ 0.001886  │\n",
      "│ Plasma_alanine_aminotransferase_… ┆ 155162  ┆ 0.001935  │\n",
      "│ Plasma_alkaline_phosphatase_leve… ┆ 155745  ┆ 0.001942  │\n",
      "│ Calculated_LDL_cholesterol_level… ┆ 159794  ┆ 0.001992  │\n",
      "│ AST_serum_level_47                ┆ 165378  ┆ 0.002062  │\n",
      "│ Plasma_albumin_level_52           ┆ 180982  ┆ 0.002257  │\n",
      "│ Current_smoker_83                 ┆ 192388  ┆ 0.002399  │\n",
      "│ Plasma_potassium_level_27         ┆ 193964  ┆ 0.002418  │\n",
      "│ Plasma_sodium_level_25            ┆ 194345  ┆ 0.002423  │\n",
      "│ Plasma_creatinine_level_32        ┆ 199382  ┆ 0.002486  │\n",
      "│ Serum_total_cholesterol_level_98  ┆ 199418  ┆ 0.002486  │\n",
      "│ Plasma_C_reactive_protein_60      ┆ 214780  ┆ 0.002678  │\n",
      "│ Serum_non_high_density_lipoprote… ┆ 223244  ┆ 0.002783  │\n",
      "│ HbA1c_level__DCCT_aligned__7      ┆ 297120  ┆ 0.003705  │\n",
      "│ Serum_folate_80                   ┆ 309263  ┆ 0.003856  │\n",
      "│ eGFR_using_creatinine__CKD_EPI__… ┆ 310164  ┆ 0.003867  │\n",
      "│ Urine_albumin_creatinine_ratio_3… ┆ 313919  ┆ 0.003914  │\n",
      "│ Total_cholesterol_HDL_ratio_95    ┆ 350071  ┆ 0.004365  │\n",
      "│ Corrected_serum_calcium_level_42  ┆ 350804  ┆ 0.004374  │\n",
      "│ Serum_vitamin_B12_79              ┆ 361201  ┆ 0.004503  │\n",
      "│ Serum_C_reactive_protein_level_5… ┆ 374407  ┆ 0.004668  │\n",
      "│ Serum_ferritin_63                 ┆ 433898  ┆ 0.00541   │\n",
      "│ Serum_free_T4_level_75            ┆ 449966  ┆ 0.00561   │\n",
      "│ Serum_bilirubin_level_53          ┆ 494921  ┆ 0.006171  │\n",
      "│ Serum_cholesterol_HDL_ratio_94    ┆ 531929  ┆ 0.006632  │\n",
      "│ Serum_gamma_glutamyl_transferase… ┆ 535224  ┆ 0.006673  │\n",
      "│ International_normalised_ratio_8… ┆ 575296  ┆ 0.007173  │\n",
      "│ Ex_smoker_84                      ┆ 622081  ┆ 0.007756  │\n",
      "│ Serum_LDL_cholesterol_level_102   ┆ 648760  ┆ 0.008089  │\n",
      "│ Erythrocyte_sedimentation_rate_6… ┆ 658568  ┆ 0.008211  │\n",
      "│ Serum_calcium_39                  ┆ 680070  ┆ 0.008479  │\n",
      "│ Haemoglobin_A1c_level___IFCC_sta… ┆ 732546  ┆ 0.009133  │\n",
      "│ Serum_triglycerides_105           ┆ 948736  ┆ 0.011829  │\n",
      "│ Serum_HDL_cholesterol_level_100   ┆ 1076072 ┆ 0.013417  │\n",
      "│ Red_blood_cell_distribution_widt… ┆ 1077920 ┆ 0.01344   │\n",
      "│ Serum_alanine_aminotransferase_l… ┆ 1129542 ┆ 0.014083  │\n",
      "│ Serum_cholesterol_97              ┆ 1139633 ┆ 0.014209  │\n",
      "│ Serum_TSH_level_71                ┆ 1168345 ┆ 0.014567  │\n",
      "│ Serum_total_bilirubin_level_56    ┆ 1261412 ┆ 0.015727  │\n",
      "│ Never_smoked_tobacco_85           ┆ 1266658 ┆ 0.015793  │\n",
      "│ Mean_corpusc_Hb_conc__MCHC__14    ┆ 1272626 ┆ 0.015867  │\n",
      "│ O_E___height_1                    ┆ 1385957 ┆ 0.01728   │\n",
      "│ Haematocrit_15                    ┆ 1541049 ┆ 0.019214  │\n",
      "│ GFR_calculated_abbreviated_MDRD_… ┆ 1712368 ┆ 0.02135   │\n",
      "│ Serum_alkaline_phosphatase_50     ┆ 1772058 ┆ 0.022094  │\n",
      "│ Serum_albumin_51                  ┆ 1828822 ┆ 0.022802  │\n",
      "│ Serum_urea_level_29               ┆ 1881258 ┆ 0.023456  │\n",
      "│ Basophil_count_22                 ┆ 1936914 ┆ 0.02415   │\n",
      "│ Mean_corpusc_haemoglobin_MCH__13  ┆ 1970759 ┆ 0.024572  │\n",
      "│ Eosinophil_count_21               ┆ 2060523 ┆ 0.025691  │\n",
      "│ Monocyte_count_23                 ┆ 2076890 ┆ 0.025895  │\n",
      "│ Lymphocyte_count_20               ┆ 2080373 ┆ 0.025938  │\n",
      "│ Red_blood_cell__RBC__count_10     ┆ 2083063 ┆ 0.025972  │\n",
      "│ Neutrophil_count_19               ┆ 2090490 ┆ 0.026064  │\n",
      "│ Mean_corpuscular_volume__MCV__11  ┆ 2123197 ┆ 0.026472  │\n",
      "│ Platelet_count_12                 ┆ 2153718 ┆ 0.026853  │\n",
      "│ Total_white_cell_count_18         ┆ 2156773 ┆ 0.026891  │\n",
      "│ Body_mass_index_3                 ┆ 2176838 ┆ 0.027141  │\n",
      "│ Serum_potassium_26                ┆ 2180533 ┆ 0.027187  │\n",
      "│ Haemoglobin_estimation_9          ┆ 2202398 ┆ 0.02746   │\n",
      "│ Serum_sodium_24                   ┆ 2219355 ┆ 0.027671  │\n",
      "│ Serum_creatinine_31               ┆ 2285768 ┆ 0.028499  │\n",
      "│ O_E___weight_2                    ┆ 2468235 ┆ 0.030774  │\n",
      "│ Systolic_blood_pressure_4         ┆ 5591149 ┆ 0.069711  │\n",
      "│ Diastolic_blood_pressure_5        ┆ 5610826 ┆ 0.069956  │\n",
      "└───────────────────────────────────┴─────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "pl.Config.set_tbl_rows(vocab_size + 1)\n",
    "print(dm.tokenizer._event_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create models and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using Temporal Positional Encoding. This module uses the patient's age at an event within their time series.\n",
      "INFO:root:Using ExponentialTTELayer. This module predicts the time until next event as an exponential distribution\n",
      "INFO:root:Using Temporal Positional Encoding. This module uses the patient's age at an event within their time series.\n",
      "INFO:root:Using GeometricTTELayer. This module predicts the time until next event as a geometric distribution, supported on the set {0,1,...}\n"
     ]
    }
   ],
   "source": [
    "models, m_names = [], []\n",
    "\n",
    "# My development model\n",
    "for tte_layer in [\"Exponential\", \"Geometric\"]:\n",
    "    config = DemoConfig()\n",
    "    config.TTELayer = tte_layer\n",
    "    models.append(TTETransformerForCausalSequenceModelling(config, vocab_size).to(device))\n",
    "    m_names.append(f\"TPPTransformerForCausalSequenceModelling: {tte_layer} TTE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_curves_train = [[] for _ in models]\n",
    "loss_curves_train_clf = [[] for _ in models]\n",
    "loss_curves_train_tte = [[] for _ in models]\n",
    "\n",
    "loss_curves_val = [[] for _ in models]\n",
    "loss_curves_val_clf = [[] for _ in models]\n",
    "loss_curves_val_tte = [[] for _ in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hoelkOrFY8bN",
    "outputId": "961304cd-e379-40d4-dd56-8de0b91d2861"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model `TPPTransformerForCausalSequenceModelling: Exponential TTE`, with 10.853185 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   2%|▏         | 51/3387 [02:40<2:54:43,  3.14s/it]\n",
      "Validation epoch 0:   2%|▏         | 6/371 [00:36<37:16,  6.13s/it]  \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for m_idx, (model, m_name) in enumerate(zip(models, m_names)):\n",
    "    \n",
    "    print(f\"Training model `{m_name}`, with {sum(p.numel() for p in model.parameters())/1e6} M parameters\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # create a PyTorch optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=opt.learning_rate)\n",
    "\n",
    "    best_val, epochs_since_best = np.inf, 0\n",
    "    for epoch in range(opt.epochs):\n",
    "        epoch_loss, epoch_clf_loss, epoch_tte_loss = 0, 0, 0\n",
    "        model.train()\n",
    "        for i, batch in tqdm(enumerate(dm.train_dataloader()), desc=f\"Training epoch {epoch}\", total=len(dm.train_dataloader())):\n",
    "            if i > 50:\n",
    "                break\n",
    "                \n",
    "            # evaluate the loss\n",
    "            _, (loss_clf, loss_tte), loss = model(batch['tokens'].to(device), \n",
    "                                                  ages=batch['ages'].to(device), \n",
    "                                                  attention_mask=batch['attention_mask'].to(device)  \n",
    "                                                  )\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            # record\n",
    "            epoch_clf_loss += loss_clf.item()\n",
    "            epoch_tte_loss += loss_tte.item()\n",
    "        epoch_loss /= i\n",
    "        epoch_clf_loss /= i\n",
    "        epoch_tte_loss /= i\n",
    "        loss_curves_train[m_idx].append(epoch_loss)\n",
    "        loss_curves_train_clf[m_idx].append(epoch_clf_loss)\n",
    "        loss_curves_train_tte[m_idx].append(epoch_tte_loss)\n",
    "\n",
    "        # evaluate the loss on val set\n",
    "        with torch.no_grad(): \n",
    "            model.eval()\n",
    "            if epoch % opt.eval_interval == 0 or epoch == opt.epochs - 1:\n",
    "                val_loss, val_clf_loss, val_tte_loss = 0, 0, 0\n",
    "                for j, batch in tqdm(enumerate(dm.val_dataloader()), desc=f\"Validation epoch {epoch}\", total=len(dm.val_dataloader())):\n",
    "                    if j > 20:\n",
    "                        break\n",
    "                    _, (loss_clf, loss_tte), loss = model(batch['tokens'].to(device), \n",
    "                                                          ages=batch['ages'].to(device),\n",
    "                                                          attention_mask=batch['attention_mask'].to(device) \n",
    "                                                          )\n",
    "                    val_loss += loss.item()\n",
    "                    # record\n",
    "                    val_clf_loss += loss_clf.item()\n",
    "                    val_tte_loss += loss_tte.item()\n",
    "                val_loss /= j\n",
    "                val_clf_loss /= j\n",
    "                val_tte_loss /= j\n",
    "                loss_curves_val[m_idx].append(val_loss)\n",
    "                loss_curves_val_clf[m_idx].append(val_clf_loss)\n",
    "                loss_curves_val_tte[m_idx].append(val_tte_loss)\n",
    "                print(f\"Epoch {epoch}:\\tTrain loss {epoch_loss:.2f}  ({epoch_clf_loss:.2f}, {epoch_tte_loss:.2f}). Val loss {val_loss:.2f} ({val_clf_loss:.2f}, {val_tte_loss:.2f})\")          \n",
    "                # TODO: Note not fully accurate as last batch is likely not the same size, will be fixed with lightning\n",
    "        \n",
    "            if val_loss >= best_val:\n",
    "                epochs_since_best += 1\n",
    "                if epochs_since_best >= 5:\n",
    "                    break\n",
    "            else:\n",
    "                best_val = val_loss\n",
    "                epochs_since_best = 0\n",
    "\n",
    "    # Test trained model with a prompt\n",
    "    # ----------------    \n",
    "    # set context: diagnosis of depression at 20 years old\n",
    "    tokens = torch.from_numpy(np.array(dm.encode([\"DEPRESSION\"])).reshape((1,-1))).to(device)\n",
    "    ages = torch.tensor([[20*365]], device=device)\n",
    "    # values = torch.tensor([[torch.nan]], device=device)\n",
    "    # generate: sample the next 10 tokens\n",
    "    new_tokens, new_ages = model.generate(tokens, ages, max_new_tokens=10)\n",
    "    generated = dm.decode(new_tokens[0].tolist())\n",
    "    # report:\n",
    "    #    note, Not considering value yet.\n",
    "    for _cat, _age in zip(generated.split(\" \"), new_ages[0, :]):\n",
    "        print(f\"\\t {_cat} at age {_age/365:.0f} ({_age:.1f} days)\")    # with value {_value}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing output to real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dm.train_dataloader():\n",
    "    break\n",
    "conditions = batch[\"tokens\"].numpy().tolist()\n",
    "# delta_ages = batch[\"ages\"][:, 1:] - batch[\"ages\"][:, :-1]\n",
    "for idx, (token, age) in enumerate(zip(conditions[0], batch[\"ages\"][0,:])):\n",
    "    if token == 0 or idx >= 10:\n",
    "        break\n",
    "    print(f\"{dm.decode([token])}, at age {age/365:.0f} ({age:.1f} days)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"k\", \"r\", \"b\", \"y\"]\n",
    "\n",
    "# Plot loss\n",
    "plt.figure()\n",
    "for m_idx, _ in enumerate(models):\n",
    "    # Training\n",
    "    iterations = np.linspace(0, len(loss_curves_train[m_idx]), len(loss_curves_train[m_idx])) * opt.eval_interval\n",
    "    plt.plot(iterations, loss_curves_train[m_idx], label=f\"{m_names[m_idx]}-train\", c=cols[m_idx], linestyle='dashed')\n",
    "    # Validation\n",
    "    iterations = np.linspace(0, len(loss_curves_val[m_idx]), len(loss_curves_val[m_idx])) * opt.eval_interval\n",
    "    plt.plot(iterations, loss_curves_val[m_idx], label=f\"{m_names[m_idx]}-val\", c=cols[m_idx])\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"figs/TTE/logloss.png\")\n",
    "\n",
    "# Plot classifier loss\n",
    "plt.figure()\n",
    "for m_idx, _ in enumerate(models):\n",
    "    # Training\n",
    "    iterations = np.linspace(0, len(loss_curves_train_clf[m_idx]), len(loss_curves_train_clf[m_idx])) * opt.eval_interval\n",
    "    plt.plot(iterations, loss_curves_train_clf[m_idx], label=f\"{m_names[m_idx]}-train\", c=cols[m_idx], linestyle='dashed')\n",
    "    # Validation\n",
    "    iterations = np.linspace(0, len(loss_curves_val_clf[m_idx]), len(loss_curves_val_clf[m_idx])) * opt.eval_interval\n",
    "    plt.plot(iterations, loss_curves_val_clf[m_idx], label=f\"{m_names[m_idx]}-val\", c=cols[m_idx])\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"figs/TTE/logloss_clf.png\")\n",
    "\n",
    "# Plot tte loss\n",
    "plt.figure()\n",
    "for m_idx, _ in enumerate(models):\n",
    "    # Training\n",
    "    iterations = np.linspace(0, len(loss_curves_train_tte[m_idx]), len(loss_curves_train_tte[m_idx])) * opt.eval_interval\n",
    "    plt.plot(iterations, loss_curves_train_tte[m_idx], label=f\"{m_names[m_idx]}-train\", c=cols[m_idx], linestyle='dashed')\n",
    "    # Validation\n",
    "    iterations = np.linspace(0, len(loss_curves_val_tte[m_idx]), len(loss_curves_val_tte[m_idx])) * opt.eval_interval\n",
    "    plt.plot(iterations, loss_curves_val_tte[m_idx], label=f\"{m_names[m_idx]}-val\", c=cols[m_idx])\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"figs/TTE/logloss_tte.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diabetes: How related conditions are impacted by each other\n",
    "Probability of type II diabetes before and after a type I diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_token = dm.tokenizer._stoi[\"TYPE1DM\"]\n",
    "t2_token = dm.tokenizer._stoi[\"TYPE2DIABETES\"]\n",
    "\n",
    "\n",
    "base_prompt = [\"DEPRESSION\"]\n",
    "ages_in_years = [20]\n",
    "\n",
    "to_days = lambda a_list: torch.FloatTensor([365 * _a for _a in a_list]).reshape((1,-1)).to(device)\n",
    "\n",
    "# Create a set of prompts\n",
    "prompts, ages, values, desc = [], [], [], []\n",
    "desc.append(\"Control\")\n",
    "prompts.append(base_prompt)\n",
    "ages.append(ages_in_years)\n",
    "desc.append(\"Type 1\")\n",
    "prompts.append(base_prompt + [\"TYPE1DM\"])\n",
    "ages.append(ages_in_years + [21])\n",
    "desc.append(\"Type 2\")\n",
    "prompts.append(base_prompt + [\"TYPE2DIABETES\"])\n",
    "ages.append(ages_in_years + [21])\n",
    "\n",
    "for model_idx, model in enumerate(models):\n",
    "    print(f\"\\n\\n{m_names[model_idx]}\\n--------------------------------------\")\n",
    "    with torch.no_grad(): \n",
    "            model.eval()\n",
    "    \n",
    "            for p_idx, (prompt, age) in enumerate(zip(prompts, ages)):\n",
    "                print(f\"\\n{desc[p_idx]}: \\t ({','.join(prompt)}): \")\n",
    "                encoded_prompt = torch.from_numpy(np.array(dm.encode(prompt)).reshape((1,-1))).to(device)\n",
    "                (lgts, tte_dist), _, _ = model(encoded_prompt,\n",
    "                                                       # values=torch.tensor(value).to(device),\n",
    "                                                       ages=to_days(age),\n",
    "                                                       is_generation=True)\n",
    "                probs = torch.nn.functional.softmax(lgts, dim=2)\n",
    "                print(f\"\\tprobability of type I diabetes: {100*float(probs[0, 0, t1_token].cpu().detach().numpy()):.4f}%\")\n",
    "                print(f\"\\tprobability of type II diabetes: {100*float(probs[0, 0, t2_token].cpu().detach().numpy()):.4f}%\")\n",
    "\n",
    "# Note: adding a diagnosis (even if potentially orthogonal) at the beginning of the prompt increases probability of either type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Age: How increasing prompt age affects likelihood of age related diagnoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\"ALLERGICRHINITISCONJ\"]\n",
    "ages = [[4],[8],[20],[30],[60],[80],[90]]\n",
    "\n",
    "# target_conditions=[\"TYPE1DM\"]#, \"TYPE2DIABETES\", \"OSTEOARTHRITIS\", \"ANY_DEAFNESS_HEARING_LOSS\"]\n",
    "\n",
    "for model_idx, model in enumerate(models):\n",
    "    print(f\"\\n\\n{m_names[model_idx]}\\n--------------------------------------\")\n",
    "\n",
    "    # for condition in target_conditions:\n",
    "    #     print(f\"Probability of {condition}\")\n",
    "    #     target_token = dm.tokenizer._stoi[condition]\n",
    "\n",
    "    for p_idx, age in enumerate(ages):\n",
    "        print(f\"\\nAge {age[-1]}\\n======\")\n",
    "        encoded_prompt = torch.from_numpy(np.array(dm.encode(prompt)).reshape((1,-1))).to(device)\n",
    "        (lgts, tte_dist), _, _ = model(encoded_prompt,\n",
    "                                       ages=to_days(age),\n",
    "                                       is_generation=True)\n",
    "        probs = torch.nn.functional.softmax(lgts, dim=2) * 100\n",
    "\n",
    "        # top K\n",
    "        k = 10\n",
    "        print(f\"Top {k}\")\n",
    "        topk_prob, topk_ind = torch.topk(probs[0,0,:], k)\n",
    "        for i, j in zip(dm.decode(topk_ind.tolist()).split(\" \"), topk_prob):\n",
    "            print(f\"\\t{i}: {j:.2f}%\")\n",
    "\n",
    "        # bottom K\n",
    "        k = 30\n",
    "        print(f\"Bottom {k}\")\n",
    "        topk_prob, topk_ind = torch.topk(-probs[0,0,:], k)\n",
    "        for i, j in zip(dm.decode(topk_ind.tolist()).split(\" \"), topk_prob):\n",
    "            print(f\"\\t{i}: {-j:.2f}%\")\n",
    "        \n",
    "            # print(f\"Age: {age[-1]} years old:  {100*float(probs[0, 0, target_token].cpu().detach().numpy()):.4f}%\")\n",
    "\n",
    "# Note: adding a diagnosis (even if potentially orthogonal) at the beginning of the prompt increases probability of either type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: model architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_idx, model in enumerate(models):\n",
    "    print(f\"\\n\\n{m_names[model_idx]}\\n\" + \"=\"*len(m_names[model_idx]))\n",
    "    print(f\"\\n\\n{model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to html --no-input TTE.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "sys_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
