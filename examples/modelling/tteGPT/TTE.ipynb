{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJpXpmjEYC_T"
   },
   "source": [
    "# Demo Notebook:\n",
    "## Time to Event Transformer For Causal Sequence Modelling \n",
    "\n",
    "Including time, and excluding values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rds/bear-apps/2022a/EL8-ice/software/PyTorch/1.12.1-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/cuda/__init__.py:83: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  /dev/shm/branfosj-admin/build-2022a/PyTorch/1.12.1/foss-2022a-CUDA-11.7.0/pytorch-v1.12.1/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "/rds/homes/g/gaddcz/Projects/CPRD/examples/modelling/tteGPT\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning \n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sqlite3\n",
    "from dataclasses import dataclass\n",
    "import logging\n",
    "from CPRD.data.foundational_loader import FoundationalDataModule\n",
    "from CPRD.src.models.TTE.task_heads.causal import TTETransformerForCausalSequenceModelling\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# TODO:\n",
    "# replace experiment boilerplate with pytorch lightning\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "# device = \"cpu\"    # if more informative debugging statements are needed\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set config to be equivalent architecture of kaparthy benchmark, however they are not comparable tasks.\n",
    "@dataclass\n",
    "class DemoConfig:\n",
    "    block_size: int = 256        # what is the maximum context length for predictions?\n",
    "    n_layer: int = 6\n",
    "    n_head: int = 6\n",
    "    n_embd: int = 384\n",
    "    bias: bool = True\n",
    "    attention_type: str = \"global\"    \n",
    "    dropout: float = 0.0\n",
    "    unk_freq_threshold: float = 0.0\n",
    "    TTELayer = \"Exponential\"\n",
    "\n",
    "config = DemoConfig()\n",
    "\n",
    "@dataclass\n",
    "class OptConfig:\n",
    "    batch_size: int = 128\n",
    "    eval_interval: int = 1\n",
    "    learning_rate: float = 3e-4\n",
    "    epochs: int = 50\n",
    "    \n",
    "opt = OptConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(os.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data loader on a reduced cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading Polars dataset from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/archive/Version2/polars/\n",
      "INFO:root:{'diagnosis_table':                    event  count\n",
      "0        ADDISON_DISEASE    250\n",
      "1                     AF  17696\n",
      "2          ALCOHOLMISUSE  31555\n",
      "3     ALLCA_NOBCC_VFINAL  35375\n",
      "4   ALLERGICRHINITISCONJ  71148\n",
      "..                   ...    ...\n",
      "69               TYPE1DM   3184\n",
      "70         TYPE2DIABETES  30317\n",
      "71    ULCERATIVE_COLITIS   2697\n",
      "72      VALVULARDISEASES   9296\n",
      "73     VISUAL_IMPAIRMENT   3512\n",
      "\n",
      "[74 rows x 2 columns], 'measurement_table':                                                  event    count  count_obs  \\\n",
      "0                        25-Hydroxyvitamin_D2_level_92    10102       9212   \n",
      "1                        25-Hydroxyvitamin_D3_level_90    10010       9927   \n",
      "2                    AST_-_aspartate_transam._SGOT__46    41243      38870   \n",
      "3                                   AST_serum_level_47   165378     157037   \n",
      "4                        Albumin___creatinine_ratio_37     6758       1980   \n",
      "..                                                 ...      ...        ...   \n",
      "103                     Total_cholesterol_HDL_ratio_95   350071     348702   \n",
      "104                          Total_white_cell_count_18  2156773    2137990   \n",
      "105                  Urine_albumin_creatinine_ratio_35   313919     231542   \n",
      "106             Urine_microalbumin_creatinine_ratio_36     5785       2145   \n",
      "107  eGFR_using_creatinine__CKD-EPI__per_1.73_squar...   310164     303984   \n",
      "\n",
      "     bias       scale  \n",
      "0    0.10       81.90  \n",
      "1    1.00      349.00  \n",
      "2    0.00     2235.00  \n",
      "3    0.00     3961.00  \n",
      "4    0.00     3708.00  \n",
      "..    ...         ...  \n",
      "103  0.00      310.00  \n",
      "104 -3.90  6500003.90  \n",
      "105  0.00     5453.80  \n",
      "106  0.00     7778.00  \n",
      "107  1.73      788.27  \n",
      "\n",
      "[108 rows x 5 columns]}\n",
      "INFO:root:Using tokenizer tabular\n",
      "INFO:root:Tokenzier created based on 80.20M tokens\n",
      "INFO:root:Creating dataset\n",
      "Calculating chunk index splits : 508it [02:03,  4.11it/s]\n",
      "INFO:root:Creating dataset\n",
      "Calculating chunk index splits : 26it [00:07,  3.71it/s]\n",
      "INFO:root:Creating dataset\n",
      "Calculating chunk index splits : 24it [00:06,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8466869 training patients\n",
      "405239 validation patients\n",
      "373904 test patients\n",
      "184 vocab elements\n"
     ]
    }
   ],
   "source": [
    "# Get a list of patients which fit a reduced set of criterion\n",
    "path_to_db = \"/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/archive/Version2/\"\n",
    "\n",
    "# Build \n",
    "dm = FoundationalDataModule(path_to_db=path_to_db,\n",
    "                            load=True,\n",
    "                            tokenizer=\"tabular\",\n",
    "                            batch_size=opt.batch_size,\n",
    "                            max_seq_length=config.block_size,\n",
    "                            unk_freq_threshold=config.unk_freq_threshold,\n",
    "                            min_workers=10, # os.cpu_count()\n",
    "                           )\n",
    "\n",
    "vocab_size = dm.train_set.tokenizer.vocab_size\n",
    "\n",
    "print(f\"{len(dm.train_set)} training patients\")\n",
    "print(f\"{len(dm.val_set)} validation patients\")\n",
    "print(f\"{len(dm.test_set)} test patients\")\n",
    "print(f\"{vocab_size} vocab elements\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.107887983322144\n",
      "0.47054362297058105\n",
      "0.06446027755737305\n",
      "0.00037550926208496094\n",
      "0.0003097057342529297\n",
      "0.00037980079650878906\n",
      "0.0002963542938232422\n",
      "0.001577138900756836\n",
      "0.00029468536376953125\n",
      "0.0002884864807128906\n",
      "0.004949808120727539\n",
      "0.001485586166381836\n",
      "0.003221750259399414\n",
      "0.00033926963806152344\n",
      "0.00030159950256347656\n",
      "0.0003521442413330078\n",
      "0.00030422210693359375\n",
      "0.0014061927795410156\n",
      "0.0003914833068847656\n",
      "0.0003902912139892578\n",
      "0.0014438629150390625\n",
      "0.001495361328125\n",
      "0.001459360122680664\n",
      "0.0014119148254394531\n",
      "0.0014584064483642578\n",
      "0.0014448165893554688\n",
      "0.0014202594757080078\n",
      "0.0014004707336425781\n",
      "0.0013849735260009766\n",
      "0.0014469623565673828\n",
      "0.0013654232025146484\n",
      "0.0014650821685791016\n",
      "0.0023679733276367188\n",
      "0.0014128684997558594\n",
      "0.0014066696166992188\n",
      "0.0013804435729980469\n",
      "0.0014569759368896484\n",
      "0.0013403892517089844\n",
      "0.0013632774353027344\n",
      "0.00141143798828125\n",
      "0.0013647079467773438\n",
      "0.0014128684997558594\n",
      "0.0014157295227050781\n",
      "0.0013294219970703125\n",
      "0.0013530254364013672\n",
      "0.0013823509216308594\n",
      "0.0014750957489013672\n",
      "0.0014145374298095703\n",
      "0.0014355182647705078\n",
      "0.0014064311981201172\n",
      "0.0013742446899414062\n",
      "0.0014300346374511719\n",
      "0.0014300346374511719\n",
      "0.001451730728149414\n",
      "0.0013747215270996094\n",
      "0.0014338493347167969\n",
      "0.0014345645904541016\n",
      "0.0014238357543945312\n",
      "0.0014176368713378906\n",
      "0.0014309883117675781\n",
      "0.0014276504516601562\n",
      "0.0013875961303710938\n",
      "0.0013484954833984375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# start = time.time()   # starting time\n",
    "# for row_idx, row in enumerate(dm.train_set):\n",
    "#     print(time.time() - start)\n",
    "#     start = time.time()\n",
    "#     if row_idx > opt.batch_size - 1:\n",
    "#         break\n",
    "\n",
    "start = time.time()   # starting time\n",
    "for row_idx, row in enumerate(dm.train_dataloader()):\n",
    "    print(time.time() - start)\n",
    "    time.sleep(np.abs(np.random.normal(10,0.5)))\n",
    "    start = time.time()\n",
    "    if row_idx > 300:\n",
    "        break\n",
    "# print(f\"{row} loaded in {time.time()-start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "pl.Config.set_tbl_rows(vocab_size + 1)\n",
    "print(dm.tokenizer._event_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create models and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models, m_names = [], []\n",
    "\n",
    "# My development model\n",
    "for tte_layer in [\"Exponential\", \"Geometric\"]:\n",
    "    config = DemoConfig()\n",
    "    config.TTELayer = tte_layer\n",
    "    models.append(TTETransformerForCausalSequenceModelling(config, vocab_size).to(device))\n",
    "    m_names.append(f\"TPPTransformerForCausalSequenceModelling: {tte_layer} TTE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_curves_train = [[] for _ in models]\n",
    "loss_curves_train_clf = [[] for _ in models]\n",
    "loss_curves_train_tte = [[] for _ in models]\n",
    "\n",
    "loss_curves_val = [[] for _ in models]\n",
    "loss_curves_val_clf = [[] for _ in models]\n",
    "loss_curves_val_tte = [[] for _ in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hoelkOrFY8bN",
    "outputId": "961304cd-e379-40d4-dd56-8de0b91d2861"
   },
   "outputs": [],
   "source": [
    "for m_idx, (model, m_name) in enumerate(zip(models, m_names)):\n",
    "    \n",
    "    print(f\"Training model `{m_name}`, with {sum(p.numel() for p in model.parameters())/1e6} M parameters\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # create a PyTorch optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=opt.learning_rate)\n",
    "\n",
    "    best_val, epochs_since_best = np.inf, 0\n",
    "    for epoch in range(opt.epochs):\n",
    "        epoch_loss, epoch_clf_loss, epoch_tte_loss = 0, 0, 0\n",
    "        model.train()\n",
    "        for i, batch in tqdm(enumerate(dm.train_dataloader()), desc=f\"Training epoch {epoch}\", total=len(dm.train_dataloader())):\n",
    "            if i > 50:\n",
    "                break\n",
    "                \n",
    "            # evaluate the loss\n",
    "            _, (loss_clf, loss_tte), loss = model(batch['tokens'].to(device), \n",
    "                                                  ages=batch['ages'].to(device), \n",
    "                                                  attention_mask=batch['attention_mask'].to(device)  \n",
    "                                                  )\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            # record\n",
    "            epoch_clf_loss += loss_clf.item()\n",
    "            epoch_tte_loss += loss_tte.item()\n",
    "        epoch_loss /= i\n",
    "        epoch_clf_loss /= i\n",
    "        epoch_tte_loss /= i\n",
    "        loss_curves_train[m_idx].append(epoch_loss)\n",
    "        loss_curves_train_clf[m_idx].append(epoch_clf_loss)\n",
    "        loss_curves_train_tte[m_idx].append(epoch_tte_loss)\n",
    "\n",
    "        # evaluate the loss on val set\n",
    "        with torch.no_grad(): \n",
    "            model.eval()\n",
    "            if epoch % opt.eval_interval == 0 or epoch == opt.epochs - 1:\n",
    "                val_loss, val_clf_loss, val_tte_loss = 0, 0, 0\n",
    "                for j, batch in tqdm(enumerate(dm.val_dataloader()), desc=f\"Validation epoch {epoch}\", total=len(dm.val_dataloader())):\n",
    "                    if j > 20:\n",
    "                        break\n",
    "                    _, (loss_clf, loss_tte), loss = model(batch['tokens'].to(device), \n",
    "                                                          ages=batch['ages'].to(device),\n",
    "                                                          attention_mask=batch['attention_mask'].to(device) \n",
    "                                                          )\n",
    "                    val_loss += loss.item()\n",
    "                    # record\n",
    "                    val_clf_loss += loss_clf.item()\n",
    "                    val_tte_loss += loss_tte.item()\n",
    "                val_loss /= j\n",
    "                val_clf_loss /= j\n",
    "                val_tte_loss /= j\n",
    "                loss_curves_val[m_idx].append(val_loss)\n",
    "                loss_curves_val_clf[m_idx].append(val_clf_loss)\n",
    "                loss_curves_val_tte[m_idx].append(val_tte_loss)\n",
    "                print(f\"Epoch {epoch}:\\tTrain loss {epoch_loss:.2f}  ({epoch_clf_loss:.2f}, {epoch_tte_loss:.2f}). Val loss {val_loss:.2f} ({val_clf_loss:.2f}, {val_tte_loss:.2f})\")          \n",
    "                # TODO: Note not fully accurate as last batch is likely not the same size, will be fixed with lightning\n",
    "        \n",
    "            if val_loss >= best_val:\n",
    "                epochs_since_best += 1\n",
    "                if epochs_since_best >= 5:\n",
    "                    break\n",
    "            else:\n",
    "                best_val = val_loss\n",
    "                epochs_since_best = 0\n",
    "\n",
    "    # Test trained model with a prompt\n",
    "    # ----------------    \n",
    "    # set context: diagnosis of depression at 20 years old\n",
    "    tokens = torch.from_numpy(np.array(dm.encode([\"DEPRESSION\"])).reshape((1,-1))).to(device)\n",
    "    ages = torch.tensor([[20*365]], device=device)\n",
    "    # values = torch.tensor([[torch.nan]], device=device)\n",
    "    # generate: sample the next 10 tokens\n",
    "    new_tokens, new_ages = model.generate(tokens, ages, max_new_tokens=10)\n",
    "    generated = dm.decode(new_tokens[0].tolist())\n",
    "    # report:\n",
    "    #    note, Not considering value yet.\n",
    "    for _cat, _age in zip(generated.split(\" \"), new_ages[0, :]):\n",
    "        print(f\"\\t {_cat} at age {_age/365:.0f} ({_age:.1f} days)\")    # with value {_value}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing output to real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dm.train_dataloader():\n",
    "    break\n",
    "conditions = batch[\"tokens\"].numpy().tolist()\n",
    "# delta_ages = batch[\"ages\"][:, 1:] - batch[\"ages\"][:, :-1]\n",
    "for idx, (token, age) in enumerate(zip(conditions[0], batch[\"ages\"][0,:])):\n",
    "    if token == 0 or idx >= 10:\n",
    "        break\n",
    "    print(f\"{dm.decode([token])}, at age {age/365:.0f} ({age:.1f} days)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"k\", \"r\", \"b\", \"y\"]\n",
    "\n",
    "# Plot loss\n",
    "plt.figure()\n",
    "for m_idx, _ in enumerate(models):\n",
    "    # Training\n",
    "    iterations = np.linspace(0, len(loss_curves_train[m_idx]), len(loss_curves_train[m_idx])) * opt.eval_interval\n",
    "    plt.plot(iterations, loss_curves_train[m_idx], label=f\"{m_names[m_idx]}-train\", c=cols[m_idx], linestyle='dashed')\n",
    "    # Validation\n",
    "    iterations = np.linspace(0, len(loss_curves_val[m_idx]), len(loss_curves_val[m_idx])) * opt.eval_interval\n",
    "    plt.plot(iterations, loss_curves_val[m_idx], label=f\"{m_names[m_idx]}-val\", c=cols[m_idx])\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"figs/TTE/logloss.png\")\n",
    "\n",
    "# Plot classifier loss\n",
    "plt.figure()\n",
    "for m_idx, _ in enumerate(models):\n",
    "    # Training\n",
    "    iterations = np.linspace(0, len(loss_curves_train_clf[m_idx]), len(loss_curves_train_clf[m_idx])) * opt.eval_interval\n",
    "    plt.plot(iterations, loss_curves_train_clf[m_idx], label=f\"{m_names[m_idx]}-train\", c=cols[m_idx], linestyle='dashed')\n",
    "    # Validation\n",
    "    iterations = np.linspace(0, len(loss_curves_val_clf[m_idx]), len(loss_curves_val_clf[m_idx])) * opt.eval_interval\n",
    "    plt.plot(iterations, loss_curves_val_clf[m_idx], label=f\"{m_names[m_idx]}-val\", c=cols[m_idx])\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"figs/TTE/logloss_clf.png\")\n",
    "\n",
    "# Plot tte loss\n",
    "plt.figure()\n",
    "for m_idx, _ in enumerate(models):\n",
    "    # Training\n",
    "    iterations = np.linspace(0, len(loss_curves_train_tte[m_idx]), len(loss_curves_train_tte[m_idx])) * opt.eval_interval\n",
    "    plt.plot(iterations, loss_curves_train_tte[m_idx], label=f\"{m_names[m_idx]}-train\", c=cols[m_idx], linestyle='dashed')\n",
    "    # Validation\n",
    "    iterations = np.linspace(0, len(loss_curves_val_tte[m_idx]), len(loss_curves_val_tte[m_idx])) * opt.eval_interval\n",
    "    plt.plot(iterations, loss_curves_val_tte[m_idx], label=f\"{m_names[m_idx]}-val\", c=cols[m_idx])\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"figs/TTE/logloss_tte.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diabetes: How related conditions are impacted by each other\n",
    "Probability of type II diabetes before and after a type I diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_token = dm.tokenizer._stoi[\"TYPE1DM\"]\n",
    "t2_token = dm.tokenizer._stoi[\"TYPE2DIABETES\"]\n",
    "\n",
    "\n",
    "base_prompt = [\"DEPRESSION\"]\n",
    "ages_in_years = [20]\n",
    "\n",
    "to_days = lambda a_list: torch.FloatTensor([365 * _a for _a in a_list]).reshape((1,-1)).to(device)\n",
    "\n",
    "# Create a set of prompts\n",
    "prompts, ages, values, desc = [], [], [], []\n",
    "desc.append(\"Control\")\n",
    "prompts.append(base_prompt)\n",
    "ages.append(ages_in_years)\n",
    "desc.append(\"Type 1\")\n",
    "prompts.append(base_prompt + [\"TYPE1DM\"])\n",
    "ages.append(ages_in_years + [21])\n",
    "desc.append(\"Type 2\")\n",
    "prompts.append(base_prompt + [\"TYPE2DIABETES\"])\n",
    "ages.append(ages_in_years + [21])\n",
    "\n",
    "for model_idx, model in enumerate(models):\n",
    "    print(f\"\\n\\n{m_names[model_idx]}\\n--------------------------------------\")\n",
    "    with torch.no_grad(): \n",
    "            model.eval()\n",
    "    \n",
    "            for p_idx, (prompt, age) in enumerate(zip(prompts, ages)):\n",
    "                print(f\"\\n{desc[p_idx]}: \\t ({','.join(prompt)}): \")\n",
    "                encoded_prompt = torch.from_numpy(np.array(dm.encode(prompt)).reshape((1,-1))).to(device)\n",
    "                (lgts, tte_dist), _, _ = model(encoded_prompt,\n",
    "                                                       # values=torch.tensor(value).to(device),\n",
    "                                                       ages=to_days(age),\n",
    "                                                       is_generation=True)\n",
    "                probs = torch.nn.functional.softmax(lgts, dim=2)\n",
    "                print(f\"\\tprobability of type I diabetes: {100*float(probs[0, 0, t1_token].cpu().detach().numpy()):.4f}%\")\n",
    "                print(f\"\\tprobability of type II diabetes: {100*float(probs[0, 0, t2_token].cpu().detach().numpy()):.4f}%\")\n",
    "\n",
    "# Note: adding a diagnosis (even if potentially orthogonal) at the beginning of the prompt increases probability of either type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Age: How increasing prompt age affects likelihood of age related diagnoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\"ALLERGICRHINITISCONJ\"]\n",
    "ages = [[4],[8],[20],[30],[60],[80],[90]]\n",
    "\n",
    "# target_conditions=[\"TYPE1DM\"]#, \"TYPE2DIABETES\", \"OSTEOARTHRITIS\", \"ANY_DEAFNESS_HEARING_LOSS\"]\n",
    "\n",
    "for model_idx, model in enumerate(models):\n",
    "    print(f\"\\n\\n{m_names[model_idx]}\\n--------------------------------------\")\n",
    "\n",
    "    # for condition in target_conditions:\n",
    "    #     print(f\"Probability of {condition}\")\n",
    "    #     target_token = dm.tokenizer._stoi[condition]\n",
    "\n",
    "    for p_idx, age in enumerate(ages):\n",
    "        print(f\"\\nAge {age[-1]}\\n======\")\n",
    "        encoded_prompt = torch.from_numpy(np.array(dm.encode(prompt)).reshape((1,-1))).to(device)\n",
    "        (lgts, tte_dist), _, _ = model(encoded_prompt,\n",
    "                                       ages=to_days(age),\n",
    "                                       is_generation=True)\n",
    "        probs = torch.nn.functional.softmax(lgts, dim=2) * 100\n",
    "\n",
    "        # top K\n",
    "        k = 10\n",
    "        print(f\"Top {k}\")\n",
    "        topk_prob, topk_ind = torch.topk(probs[0,0,:], k)\n",
    "        for i, j in zip(dm.decode(topk_ind.tolist()).split(\" \"), topk_prob):\n",
    "            print(f\"\\t{i}: {j:.2f}%\")\n",
    "\n",
    "        # bottom K\n",
    "        k = 30\n",
    "        print(f\"Bottom {k}\")\n",
    "        topk_prob, topk_ind = torch.topk(-probs[0,0,:], k)\n",
    "        for i, j in zip(dm.decode(topk_ind.tolist()).split(\" \"), topk_prob):\n",
    "            print(f\"\\t{i}: {-j:.2f}%\")\n",
    "        \n",
    "            # print(f\"Age: {age[-1]} years old:  {100*float(probs[0, 0, target_token].cpu().detach().numpy()):.4f}%\")\n",
    "\n",
    "# Note: adding a diagnosis (even if potentially orthogonal) at the beginning of the prompt increases probability of either type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: model architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_idx, model in enumerate(models):\n",
    "    print(f\"\\n\\n{m_names[model_idx]}\\n\" + \"=\"*len(m_names[model_idx]))\n",
    "    print(f\"\\n\\n{model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to html --no-input TTE.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "sys_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
