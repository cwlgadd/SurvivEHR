{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJpXpmjEYC_T"
   },
   "source": [
    "# Demo Notebook:\n",
    "## Time to Event Transformer For Causal Time Series Modelling \n",
    "\n",
    "Including time and tabular values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added path '/rds/homes/g/gaddcz/Projects/CPRD/virtual-envTorch2.0-icelake/lib/python3.10/site-packages' at start of search paths.\n",
      "env: SQLITE_TMPDIR=/rds/projects/g/gokhalkm-optimal/DataforCharles\n",
      "env: TMPDIR=/rds/projects/g/gokhalkm-optimal/DataforCharles\n",
      "/rds/projects/g/gokhalkm-optimal/DataforCharles\n",
      "/rds/projects/g/gokhalkm-optimal/DataforCharles\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "node_type = os.getenv('BB_CPU')\n",
    "venv_dir = f'/rds/homes/g/gaddcz/Projects/CPRD/virtual-envTorch2.0-{node_type}'\n",
    "venv_site_pkgs = Path(venv_dir) / 'lib' / f'python{sys.version_info.major}.{sys.version_info.minor}' / 'site-packages'\n",
    "if venv_site_pkgs.exists():\n",
    "    sys.path.insert(0, str(venv_site_pkgs))\n",
    "    print(f\"Added path '{venv_site_pkgs}' at start of search paths.\")\n",
    "else:\n",
    "    print(f\"Path '{venv_site_pkgs}' not found. Check that it exists and/or that it exists for node-type '{node_type}'.\")\n",
    "\n",
    "# Perform sqlite operations on disk\n",
    "%env SQLITE_TMPDIR=/rds/projects/g/gokhalkm-optimal/DataforCharles\n",
    "%env TMPDIR=/rds/projects/g/gokhalkm-optimal/DataforCharles\n",
    "!echo $SQLITE_TMPDIR\n",
    "!echo $TMPDIR\n",
    "!echo $USERPROFILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "/rds/homes/g/gaddcz/Projects/CPRD/examples/modelling/tteGPT\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sqlite3\n",
    "from dataclasses import dataclass\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from CPRD.data.foundational_loader import FoundationalDataModule\n",
    "from CPRD.src.models.TTE.task_heads.causal_tabular import TTETransformerForCausalTimeSeriesModelling\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = \"cpu\"    # if more informative debugging statements are needed\n",
    "print(device)\n",
    "\n",
    "!pwd\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set config to be equivalent architecture of kaparthy benchmark, however they are not comparable tasks.\n",
    "@dataclass\n",
    "class DemoConfig:\n",
    "    # Training input sequences\n",
    "    block_size: int = 128  \n",
    "    # Multi-head attention configurations\n",
    "    n_layer: int = 12   # 6, 12\n",
    "    n_head: int = 8    # 6, 12\n",
    "    n_embd: int = 1024  # 384 , 768\n",
    "    layer_norm_bias = False\n",
    "    attention_type = \"global\"\n",
    "    window_size = 256            # the window size for local attention\n",
    "    max_positions = 512          # the maximum sequence length that this model might ever be used with \n",
    "    # SA dropouts\n",
    "    attention_dropout = 0.0\n",
    "    resid_dropout = 0.0\n",
    "    dropout = 0.0\n",
    "    \n",
    "    #\n",
    "    \n",
    "    bias: bool = True\n",
    "    unk_freq_threshold: float = 0.0\n",
    "    TTELayer = \"Exponential\"                                  # \"Geometric\"\n",
    "    tokens_for_univariate_regression = None\n",
    "\n",
    "config = DemoConfig()\n",
    "\n",
    "@dataclass\n",
    "class OptConfig:\n",
    "    batch_size: int = 64\n",
    "    eval_interval: int = 1\n",
    "    learning_rate: float = 3e-4\n",
    "    epochs: int = 50\n",
    "    \n",
    "opt = OptConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data loader on a reduced cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading Polars dataset from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/polars/\n",
      "INFO:root:Using tokenizer tabular\n",
      "INFO:root:Tokenzier created based on 3584.43M tokens\n",
      "INFO:root:Creating split=train/ dataset\n",
      "INFO:root:\t Loading split=train/ hash map for parquet\n",
      "INFO:root:\t Hash map created for split=train/ with 22,912,046 samples\n",
      "INFO:root:Creating split=test/ dataset\n",
      "INFO:root:\t Loading split=test/ hash map for parquet\n",
      "INFO:root:\t Hash map created for split=test/ with 1,207,449 samples\n",
      "INFO:root:Creating split=val/ dataset\n",
      "INFO:root:\t Loading split=val/ hash map for parquet\n",
      "INFO:root:\t Hash map created for split=val/ with 1,226,576 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184 vocab elements\n"
     ]
    }
   ],
   "source": [
    "# Get a list of patients which fit a reduced set of criterion\n",
    "# path_to_db = \"/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/archive/Version2/\"\n",
    "path_to_db = \"/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/\"\n",
    "\n",
    "# Build \n",
    "dm = FoundationalDataModule(path_to_db=path_to_db,\n",
    "                            load=True,\n",
    "                            tokenizer=\"tabular\",\n",
    "                            batch_size=opt.batch_size,\n",
    "                            max_seq_length=config.block_size,\n",
    "                            unk_freq_threshold=config.unk_freq_threshold,\n",
    "                            min_workers=20,\n",
    "                            inclusion_conditions=[\"COUNTRY = 'E'\"],\n",
    "                           )\n",
    "\n",
    "vocab_size = dm.train_set.tokenizer.vocab_size\n",
    "print(f\"{vocab_size} vocab elements\")\n",
    "\n",
    "# Extract the measurements, using the fact that the diagnoses are all up upper case. This is needed for automatically setting the configuration below\n",
    "measurements_for_univariate_regression = [record for record in dm.tokenizer._event_counts[\"EVENT\"] if record.upper() != record]\n",
    "# display(measurements_for_univariate_regression)\n",
    "# list of univariate measurements to model with Normal distribution\n",
    "config.tokens_for_univariate_regression = dm.encode(measurements_for_univariate_regression) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View a single patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to retrieve sample index 1 was 0.5601367950439453 seconds\n",
      "\n",
      "SEX                 | F\n",
      "IMD                 | 4.0\n",
      "ETHNICITY           | WHITE\n",
      "birth_year          | 1997.0\n",
      "\n",
      "Token                                                                      | Age               | Standardised value\n",
      "===================================================================================================================\n",
      "Mean_corpusc_Hb_conc__MCHC__14                                             | 1771              | nan               \n",
      "Mean_corpusc_haemoglobin_MCH__13                                           | 1771              | -0.40             \n",
      "Mean_corpuscular_volume__MCV__11                                           | 1771              | -0.10             \n",
      "Monocyte_count_23                                                          | 1771              | 0.06              \n",
      "Neutrophil_count_19                                                        | 1771              | 0.04              \n",
      "Platelet_count_12                                                          | 1771              | 0.28              \n",
      "Red_blood_cell__RBC__count_10                                              | 1771              | -0.13             \n",
      "Total_white_cell_count_18                                                  | 1771              | 0.06              \n",
      "Basophil_count_22                                                          | 3248              | -0.01             \n",
      "Eosinophil_count_21                                                        | 3248              | nan               \n",
      "Erythrocyte_sedimentation_rate_61                                          | 3248              | 0.12              \n",
      "Haematocrit_15                                                             | 3248              | 0.01              \n"
     ]
    }
   ],
   "source": [
    "dm.train_set.view_sample(1, max_dynamic_events=12, report_time=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create models and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using Temporal Positional Encoding. This module uses the patient's age at an event within their time series.\n",
      "INFO:root:Using ExponentialTTELayer. This module predicts the time until next event as an exponential distribution\n"
     ]
    }
   ],
   "source": [
    "model = TTETransformerForCausalTimeSeriesModelling(config, vocab_size).to(device)\n",
    "\n",
    "loss_curves_train = []\n",
    "loss_curves_train_clf = []\n",
    "loss_curves_train_tte = []\n",
    "loss_curves_train_values = []\n",
    "\n",
    "loss_curves_val = []\n",
    "loss_curves_val_clf = []\n",
    "loss_curves_val_tte = []\n",
    "loss_curves_val_values = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hoelkOrFY8bN",
    "outputId": "961304cd-e379-40d4-dd56-8de0b91d2861",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with 154.024665 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   0%|          | 1001/358001 [10:23<61:43:09,  1.61it/s]\n",
      "Validation epoch 0:   1%|          | 101/19166 [00:27<1:25:56,  3.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\tTrain loss -0.25  (3.73, -1.11, -3.37). Val loss -0.73 (3.16, -1.98, -3.36)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   0%|          | 55/358001 [00:39<70:31:02,  1.41it/s]\n",
      "Exception ignored in: <function WeakValueDictionary.__init__.<locals>.remove at 0x7f072f6ec280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/rds/bear-apps/2022a/EL8-ice/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/weakref.py\", line 106, in remove\n",
      "    def remove(wr, selfref=ref(self), _atomic_removal=_remove_dead_weakref):\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 21\u001b[0m\n\u001b[1;32m     15\u001b[0m _, (loss_clf, loss_tte, loss_values), loss \u001b[38;5;241m=\u001b[39m model(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), \n\u001b[1;32m     16\u001b[0m                                                    ages\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mages\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), \n\u001b[1;32m     17\u001b[0m                                                    values\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     18\u001b[0m                                                    attention_mask\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)   \n\u001b[1;32m     19\u001b[0m                                                    )\n\u001b[1;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     23\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/rds/bear-apps/2022a/EL8-ice/software/PyTorch/2.0.1-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/rds/bear-apps/2022a/EL8-ice/software/PyTorch/2.0.1-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(f\"Training model with {sum(p.numel() for p in model.parameters())/1e6} M parameters\")\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=opt.learning_rate)\n",
    "\n",
    "best_val, epochs_since_best = np.inf, 0\n",
    "for epoch in range(opt.epochs):\n",
    "    epoch_loss, epoch_clf_loss, epoch_tte_loss, epoch_values_loss = 0, 0, 0, 0\n",
    "    model.train()\n",
    "    for i, batch in tqdm(enumerate(dm.train_dataloader()), desc=f\"Training epoch {epoch}\", total=len(dm.train_dataloader())):\n",
    "        if i > 1000:\n",
    "            break\n",
    "\n",
    "        # evaluate the loss\n",
    "        _, (loss_clf, loss_tte, loss_values), loss = model(batch['tokens'].to(device), \n",
    "                                                           ages=batch['ages'].to(device), \n",
    "                                                           values=batch['values'].to(device),\n",
    "                                                           attention_mask=batch['attention_mask'].to(device)   \n",
    "                                                           )\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # record\n",
    "        epoch_clf_loss += loss_clf.item()\n",
    "        epoch_tte_loss += loss_tte.item()\n",
    "        epoch_values_loss += loss_values.item()\n",
    "    epoch_loss /= i\n",
    "    epoch_clf_loss /= i\n",
    "    epoch_tte_loss /= i\n",
    "    epoch_values_loss /= i\n",
    "    loss_curves_train.append(epoch_loss)\n",
    "    loss_curves_train_clf.append(epoch_clf_loss)\n",
    "    loss_curves_train_tte.append(epoch_tte_loss)\n",
    "    loss_curves_train_values.append(epoch_values_loss)\n",
    "\n",
    "    # evaluate the loss on val set\n",
    "    with torch.no_grad(): \n",
    "        model.eval()\n",
    "        if epoch % opt.eval_interval == 0 or epoch == opt.epochs - 1:\n",
    "            val_loss, val_clf_loss, val_tte_loss, val_values_loss = 0, 0, 0, 0\n",
    "            for j, batch in tqdm(enumerate(dm.val_dataloader()), desc=f\"Validation epoch {epoch}\", total=len(dm.val_dataloader())):\n",
    "                if j > 100:\n",
    "                    break\n",
    "                _, (loss_clf, loss_tte, loss_values), loss = model(batch['tokens'].to(device), \n",
    "                                                                   ages=batch['ages'].to(device),\n",
    "                                                                   values=batch['values'].to(device),\n",
    "                                                                   attention_mask=batch['attention_mask'].to(device)   \n",
    "                                                                   )\n",
    "            \n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                # record\n",
    "                val_clf_loss += loss_clf.item()\n",
    "                val_tte_loss += loss_tte.item()\n",
    "                val_values_loss += loss_values.item()\n",
    "            val_loss /= j\n",
    "            val_clf_loss /= j\n",
    "            val_tte_loss /= j\n",
    "            val_values_loss /= j\n",
    "            loss_curves_val.append(val_loss)\n",
    "            loss_curves_val_clf.append(val_clf_loss)\n",
    "            loss_curves_val_tte.append(val_tte_loss)\n",
    "            loss_curves_val_values.append(val_values_loss)\n",
    "\n",
    "            print(f\"Epoch {epoch}:\\tTrain loss {epoch_loss:.2f}  ({epoch_clf_loss:.2f}, {epoch_tte_loss:.2f}, {epoch_values_loss:.2f}). Val loss {val_loss:.2f} ({val_clf_loss:.2f}, {val_tte_loss:.2f}, {val_values_loss:.2f})\")          \n",
    "            # TODO: Note not fully accurate as last batch is likely not the same size, will be fixed with lightning\n",
    "    \n",
    "        if val_loss >= best_val:\n",
    "            epochs_since_best += 1\n",
    "            if epochs_since_best >= 5:\n",
    "                break\n",
    "        else:\n",
    "            best_val = val_loss\n",
    "            epochs_since_best = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training model with 86.823001 M parameters\n",
    "Training epoch 0:   0%|          | 1001/358001 [07:15<43:07:25,  2.30it/s]\n",
    "Validation epoch 0:   1%|          | 101/19166 [00:22<1:09:29,  4.57it/s]\n",
    "\n",
    "Epoch 0:\tTrain loss -1.25  (3.63, -1.28, -6.11). Val loss -3.57 (2.76, -2.22, -11.23)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Training model with 154.024665 M parameters\n",
    "\n",
    "Training epoch 0:   0%|          | 1001/358001 [10:27<62:12:13,  1.59it/s]\n",
    "Validation epoch 0:   1%|          | 101/19166 [00:27<1:26:54,  3.66it/s]\n",
    "\n",
    "Epoch 0:\tTrain loss -0.18  (3.60, -1.14, -3.00). Val loss -2.03 (2.93, -2.16, -6.86)\n",
    "\n",
    "Training epoch 1:   0%|          | 1001/358001 [10:28<62:13:17,  1.59it/s]\n",
    "Validation epoch 1:   1%|          | 101/19166 [00:27<1:27:06,  3.65it/s]\n",
    "\n",
    "Epoch 1:\tTrain loss -2.77  (2.27, -2.36, -8.22). Val loss -4.16 (1.81, -2.56, -11.74)\n",
    "\n",
    "Training epoch 2:   0%|          | 1001/358001 [10:28<62:18:36,  1.59it/s]\n",
    "Validation epoch 2:   1%|          | 101/19166 [00:27<1:27:07,  3.65it/s]\n",
    "\n",
    "Epoch 2:\tTrain loss -3.83  (1.61, -2.57, -10.53). Val loss -4.41 (1.61, -2.57, -12.26)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default context start\n",
    "prompt = [\"O_E___height_1\", \"O_E___weight_2\"]\n",
    "values = [163, 90]\n",
    "ages_in_years = [18.2, 18.2]\n",
    "\n",
    "# define encoding functions (TODO: add this wrap to datamodule\n",
    "encode_prompt = lambda prompt_list: torch.from_numpy(np.array(dm.encode(prompt_list)).reshape((1,-1))).to(device)\n",
    "encode_value = lambda prompt_list, value_list: torch.tensor(np.array([dm.standardise(_cat, _val) for _cat, _val in zip(prompt_list, value_list) ]).reshape((1,-1)), dtype=torch.float32).to(device)\n",
    "encode_age = lambda age_list: torch.tensor([365 * _age for _age in age_list], dtype=torch.int64).reshape((1,-1)).to(device)\n",
    "\n",
    "# Convert for model\n",
    "tokens = encode_prompt(prompt)\n",
    "values_scaled = encode_value(prompt, values)\n",
    "ages_in_days = encode_age(ages_in_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      "O_E___height_1                                    -0.04          at age 18 (6643.0 days)\n",
      "O_E___weight_2                                    0.16           at age 18 (6643.0 days)\n",
      "==========================================================================================\n",
      "GENERATION\n",
      "O_E___weight_2                                    0.09           at age 18 (6699.0 days)\n",
      "Systolic_blood_pressure_4                         -0.23          at age 19 (6942.3 days)\n",
      "Body_mass_index_3                                 -0.13          at age 21 (7743.8 days)\n",
      "Diastolic_blood_pressure_5                        -0.18          at age 21 (7745.7 days)\n",
      "O_E___height_1                                    0.01           at age 21 (7762.7 days)\n",
      "O_E___weight_2                                    -0.04          at age 21 (7781.6 days)\n",
      "Systolic_blood_pressure_4                         -0.17          at age 21 (7808.5 days)\n",
      "Basophil_count_22                                 0.22           at age 22 (8143.9 days)\n",
      "Eosinophil_count_21                               0.14           at age 22 (8147.8 days)\n",
      "Eosinophil_count_21                               -0.02          at age 22 (8149.4 days)\n"
     ]
    }
   ],
   "source": [
    "# generate: sample the next 10 tokens\n",
    "new_tokens, new_ages, new_values = model.generate(tokens, ages_in_days, values_scaled, max_new_tokens=10)\n",
    "\n",
    "# report:\n",
    "print(f\"PROMPT:\")\n",
    "for _idx, (_cat, _age, _value) in enumerate(zip(dm.decode(new_tokens[0].tolist()).split(\" \"), \n",
    "                                                new_ages[0, :], \n",
    "                                                new_values[0, :]\n",
    "                                               )\n",
    "                                           ):\n",
    "    # _value = dm.unstandardise(_cat, _value)\n",
    "    print(f\"{_cat}\".ljust(50) + f\"{_value:.02f}\".ljust(15) + f\"at age {_age/365:.0f} ({_age:.1f} days)\")    # with value {_value}\n",
    "    if _idx == tokens.shape[-1] - 1:\n",
    "        print(\"=\"*90)\n",
    "        print(f\"GENERATION\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing generation to real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to retrieve sample index 1 was 0.12309074401855469 seconds\n",
      "\n",
      "SEX                 | F\n",
      "IMD                 | 4.0\n",
      "ETHNICITY           | WHITE\n",
      "birth_year          | 1997.0\n",
      "\n",
      "Token                                                                      | Age               | Standardised value\n",
      "===================================================================================================================\n",
      "Mean_corpuscular_volume__MCV__11                                           | 8000              | -0.20             \n",
      "Monocyte_count_23                                                          | 8000              | 0.20              \n",
      "Neutrophil_count_19                                                        | 8000              | 0.33              \n",
      "Platelet_count_12                                                          | 8000              | 0.13              \n",
      "Red_blood_cell__RBC__count_10                                              | 8000              | -0.05             \n",
      "Serum_C_reactive_protein_level_59                                          | 8000              | 0.02              \n",
      "Serum_TSH_level_71                                                         | 8000              | 0.09              \n",
      "Serum_alanine_aminotransferase_level_45                                    | 8000              | -0.09             \n"
     ]
    }
   ],
   "source": [
    "dm.train_set.view_sample(1, max_dynamic_events=10, report_time=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot loss\n",
    "plt.figure()\n",
    "# Training\n",
    "iterations = np.linspace(0, len(loss_curves_train), len(loss_curves_train)) * opt.eval_interval\n",
    "plt.plot(iterations, loss_curves_train, label=\"train\")\n",
    "# Validation\n",
    "iterations = np.linspace(0, len(loss_curves_val), len(loss_curves_val)) * opt.eval_interval\n",
    "plt.plot(iterations, loss_curves_val, label=\"val\", linestyle='dashed')\n",
    "plt.legend()\n",
    "plt.savefig(f\"figs/TTE_tab/loss.png\")\n",
    "\n",
    "# Plot Classifier loss\n",
    "plt.figure()\n",
    "# Training\n",
    "iterations = np.linspace(0, len(loss_curves_train_clf), len(loss_curves_train_clf)) * opt.eval_interval\n",
    "plt.plot(iterations, loss_curves_train_clf, label=\"train\")\n",
    "# Validation\n",
    "iterations = np.linspace(0, len(loss_curves_val_clf), len(loss_curves_val_clf)) * opt.eval_interval\n",
    "plt.plot(iterations, loss_curves_val_clf, label=\"val\", linestyle='dashed')\n",
    "plt.legend()\n",
    "plt.savefig(f\"figs/TTE_tab/loss_clf.png\")\n",
    "\n",
    "# Plot TTE loss\n",
    "plt.figure()\n",
    "# Training\n",
    "iterations = np.linspace(0, len(loss_curves_train_tte), len(loss_curves_train_tte)) * opt.eval_interval\n",
    "plt.plot(iterations, loss_curves_train_tte, label=\"train\", )\n",
    "# Validation\n",
    "iterations = np.linspace(0, len(loss_curves_val_tte), len(loss_curves_val_tte)) * opt.eval_interval\n",
    "plt.plot(iterations, loss_curves_val_tte, label=\"val\", linestyle='dashed')\n",
    "plt.legend()\n",
    "plt.savefig(f\"figs/TTE_tab/loss_tte.png\")\n",
    "\n",
    "# Plot value loss\n",
    "plt.figure()\n",
    "# Training\n",
    "iterations = np.linspace(0, len(loss_curves_train_values), len(loss_curves_train_values)) * opt.eval_interval\n",
    "plt.plot(iterations, loss_curves_train_values, label=\"train\", )\n",
    "# Validation\n",
    "iterations = np.linspace(0, len(loss_curves_val_values), len(loss_curves_val_values)) * opt.eval_interval\n",
    "plt.plot(iterations, loss_curves_val_values, label=\"val\", linestyle='dashed')\n",
    "plt.legend()\n",
    "plt.savefig(f\"figs/TTE_tab/loss_values.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diabetes: How related conditions are impacted by each other\n",
    "Probability of type II diabetes before and after a type I diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Control: \t (DEPRESSION): \n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "\tprobability of type I diabetes: 0.0302%\n",
      "\tprobability of type II diabetes: 0.1716%\n",
      "\n",
      "Type 1: \t (DEPRESSION,TYPE1DM): \n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 2])\n",
      "\tprobability of type I diabetes: 0.0321%\n",
      "\tprobability of type II diabetes: 0.1864%\n",
      "\n",
      "Type 2: \t (DEPRESSION,TYPE2DIABETES): \n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 2])\n",
      "\tprobability of type I diabetes: 0.0342%\n",
      "\tprobability of type II diabetes: 0.1831%\n"
     ]
    }
   ],
   "source": [
    "t1_token = dm.tokenizer._stoi[\"TYPE1DM\"]\n",
    "t2_token = dm.tokenizer._stoi[\"TYPE2DIABETES\"]\n",
    "\n",
    "\n",
    "base_prompt = [\"DEPRESSION\"]\n",
    "ages_in_years = [20]\n",
    "base_values = [torch.tensor([torch.nan])]\n",
    "\n",
    "to_days = lambda a_list: torch.FloatTensor([365 * _a for _a in a_list]).reshape((1,-1)).to(device)\n",
    "\n",
    "# Create a set of prompts\n",
    "prompts, ages, values, desc = [], [], [], []\n",
    "# control prompt\n",
    "desc.append(\"Control\")\n",
    "prompts.append(base_prompt)\n",
    "ages.append(ages_in_years)\n",
    "values.append(base_values)\n",
    "# prompt with type 1 diabetes\n",
    "desc.append(\"Type 1\")\n",
    "prompts.append(base_prompt + [\"TYPE1DM\"])\n",
    "ages.append(ages_in_years + [21])\n",
    "values.append(base_values + [torch.tensor([torch.nan])])\n",
    "\n",
    "desc.append(\"Type 2\")\n",
    "prompts.append(base_prompt + [\"TYPE2DIABETES\"])\n",
    "ages.append(ages_in_years + [21])\n",
    "values.append(base_values + [torch.tensor([torch.nan])])\n",
    "\n",
    "with torch.no_grad(): \n",
    "    model.eval()\n",
    "\n",
    "    for p_idx, (prompt, age, value) in enumerate(zip(prompts, ages, values)):\n",
    "        print(f\"\\n{desc[p_idx]}: \\t ({','.join(prompt)}): \")\n",
    "        encoded_prompt = torch.from_numpy(np.array(dm.encode(prompt)).reshape((1,-1))).to(device)\n",
    "        value = torch.tensor(value).reshape((1,-1)).to(device)\n",
    "        (lgts, tte_dist, val_dist), _, _ = model(encoded_prompt,\n",
    "                                                 values=value,\n",
    "                                                 ages=to_days(age),\n",
    "                                                 is_generation=True)\n",
    "        probs = torch.nn.functional.softmax(lgts, dim=2)\n",
    "        print(f\"\\tprobability of type I diabetes: {100*float(probs[0, 0, t1_token].cpu().detach().numpy()):.4f}%\")\n",
    "        print(f\"\\tprobability of type II diabetes: {100*float(probs[0, 0, t2_token].cpu().detach().numpy()):.4f}%\")\n",
    "\n",
    "# Note: adding a diagnosis (even if potentially orthogonal) at the beginning of the prompt increases probability of either type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Values: How increasing BMI affects likelihood of diagnoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([-2.], device='cuda:0'), tensor([-1.5556], device='cuda:0'), tensor([-1.1111], device='cuda:0'), tensor([-0.6667], device='cuda:0'), tensor([-0.2222], device='cuda:0'), tensor([0.2222], device='cuda:0'), tensor([0.6667], device='cuda:0'), tensor([1.1111], device='cuda:0'), tensor([1.5556], device='cuda:0'), tensor([2.], device='cuda:0')]\n",
      "Value tensor([-2.], device='cuda:0')\n",
      "======\n",
      "\tDiastolic_blood_pressure_5: 5.72%\n",
      "\tBody_mass_index_3: 1.66%\n",
      "\tHYPERTENSION: 0.01%\n",
      "\tTYPE2DIABETES: 0.01%\n",
      "\tCKDSTAGE3TO5: 0.00%\n",
      "\tOSTEOARTHRITIS: 0.00%\n",
      "\tTYPE1DM: 0.00%\n",
      "Value tensor([-1.5556], device='cuda:0')\n",
      "======\n",
      "\tDiastolic_blood_pressure_5: 7.37%\n",
      "\tBody_mass_index_3: 1.71%\n",
      "\tTYPE2DIABETES: 0.01%\n",
      "\tHYPERTENSION: 0.01%\n",
      "\tCKDSTAGE3TO5: 0.00%\n",
      "\tOSTEOARTHRITIS: 0.00%\n",
      "\tTYPE1DM: 0.00%\n",
      "Value tensor([-1.1111], device='cuda:0')\n",
      "======\n",
      "\tDiastolic_blood_pressure_5: 10.39%\n",
      "\tBody_mass_index_3: 1.65%\n",
      "\tTYPE2DIABETES: 0.01%\n",
      "\tHYPERTENSION: 0.01%\n",
      "\tCKDSTAGE3TO5: 0.01%\n",
      "\tOSTEOARTHRITIS: 0.00%\n",
      "\tTYPE1DM: 0.00%\n",
      "Value tensor([-0.6667], device='cuda:0')\n",
      "======\n",
      "\tDiastolic_blood_pressure_5: 18.54%\n",
      "\tBody_mass_index_3: 1.45%\n",
      "\tTYPE2DIABETES: 0.01%\n",
      "\tCKDSTAGE3TO5: 0.00%\n",
      "\tHYPERTENSION: 0.00%\n",
      "\tOSTEOARTHRITIS: 0.00%\n",
      "\tTYPE1DM: 0.00%\n",
      "Value tensor([-0.2222], device='cuda:0')\n",
      "======\n",
      "\tDiastolic_blood_pressure_5: 48.46%\n",
      "\tBody_mass_index_3: 0.96%\n",
      "\tTYPE2DIABETES: 0.00%\n",
      "\tHYPERTENSION: 0.00%\n",
      "\tCKDSTAGE3TO5: 0.00%\n",
      "\tOSTEOARTHRITIS: 0.00%\n",
      "\tTYPE1DM: 0.00%\n",
      "Value tensor([0.2222], device='cuda:0')\n",
      "======\n",
      "\tDiastolic_blood_pressure_5: 69.02%\n",
      "\tBody_mass_index_3: 0.91%\n",
      "\tTYPE2DIABETES: 0.01%\n",
      "\tHYPERTENSION: 0.00%\n",
      "\tCKDSTAGE3TO5: 0.00%\n",
      "\tOSTEOARTHRITIS: 0.00%\n",
      "\tTYPE1DM: 0.00%\n",
      "Value tensor([0.6667], device='cuda:0')\n",
      "======\n",
      "\tDiastolic_blood_pressure_5: 60.21%\n",
      "\tBody_mass_index_3: 1.27%\n",
      "\tTYPE2DIABETES: 0.01%\n",
      "\tHYPERTENSION: 0.01%\n",
      "\tCKDSTAGE3TO5: 0.01%\n",
      "\tOSTEOARTHRITIS: 0.01%\n",
      "\tTYPE1DM: 0.00%\n",
      "Value tensor([1.1111], device='cuda:0')\n",
      "======\n",
      "\tDiastolic_blood_pressure_5: 43.97%\n",
      "\tBody_mass_index_3: 1.26%\n",
      "\tTYPE2DIABETES: 0.02%\n",
      "\tHYPERTENSION: 0.01%\n",
      "\tOSTEOARTHRITIS: 0.01%\n",
      "\tCKDSTAGE3TO5: 0.01%\n",
      "\tTYPE1DM: 0.00%\n",
      "Value tensor([1.5556], device='cuda:0')\n",
      "======\n",
      "\tDiastolic_blood_pressure_5: 30.51%\n",
      "\tBody_mass_index_3: 1.08%\n",
      "\tTYPE2DIABETES: 0.03%\n",
      "\tHYPERTENSION: 0.02%\n",
      "\tOSTEOARTHRITIS: 0.02%\n",
      "\tCKDSTAGE3TO5: 0.01%\n",
      "\tTYPE1DM: 0.00%\n",
      "Value tensor([2.], device='cuda:0')\n",
      "======\n",
      "\tDiastolic_blood_pressure_5: 20.37%\n",
      "\tBody_mass_index_3: 0.84%\n",
      "\tTYPE2DIABETES: 0.03%\n",
      "\tOSTEOARTHRITIS: 0.02%\n",
      "\tHYPERTENSION: 0.02%\n",
      "\tCKDSTAGE3TO5: 0.01%\n",
      "\tTYPE1DM: 0.00%\n"
     ]
    }
   ],
   "source": [
    "events_of_interest = [\"Body_mass_index_3\", \"Diastolic_blood_pressure_5\", \n",
    "                      \"TYPE1DM\", \"TYPE2DIABETES\",\n",
    "                      \"HYPERTENSION\", \"OSTEOARTHRITIS\",\n",
    "                      \"CKDSTAGE3TO5\",\n",
    "                      \"HF\", \"ISCHAEMICSTROKE\"\n",
    "                     ]\n",
    "\n",
    "prompt = [\"Body_mass_index_3\"]\n",
    "# values = [torch.tensor([standardise(_cat, v) for _cat in prompt], device=device) for v in [12.,15.,18.,21.,24.,30.,40.]]\n",
    "values = [torch.tensor([float(v) for _cat in prompt], device=device) for v in np.linspace(-2,2,10)]\n",
    "print(values)\n",
    "age = [40]\n",
    "\n",
    "# for condition in target_conditions:\n",
    "#     print(f\"Probability of {condition}\")\n",
    "#     target_token = dm.tokenizer._stoi[condition]\n",
    "\n",
    "for p_idx, value in enumerate(values):\n",
    "    print(f\"Value {value}\\n======\")\n",
    "    \n",
    "    encoded_prompt = torch.from_numpy(np.array(dm.encode(prompt)).reshape((1,-1))).to(device)\n",
    "    value = value.reshape((1,-1))\n",
    "    \n",
    "    (lgts, tte_dist, val_dist), _, _ = model(encoded_prompt,\n",
    "                                              values=value,\n",
    "                                             ages=to_days(age),\n",
    "                                             is_generation=True)\n",
    "    probs = torch.nn.functional.softmax(lgts, dim=2) * 100\n",
    "    \n",
    "    topk_prob, topk_ind = torch.sort(probs[0,0,:], descending=True)\n",
    "    for i, j in zip(dm.decode(topk_ind.tolist()).split(\" \"), topk_prob):\n",
    "        if i in events_of_interest:\n",
    "            print(f\"\\t{i}: {j:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Values: How increasing diastolic_blood_pressure affects likelihood of diagnoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value tensor([-2.], device='cuda:0')\n",
      "======\n",
      "\tDiastolic_blood_pressure_5: 1.46%\n",
      "\tBody_mass_index_3: 1.36%\n",
      "\tTYPE2DIABETES: 0.01%\n",
      "\tCKDSTAGE3TO5: 0.01%\n",
      "\tOSTEOARTHRITIS: 0.00%\n",
      "\tHYPERTENSION: 0.00%\n",
      "\tTYPE1DM: 0.00%\n",
      "Value tensor([-1.5556], device='cuda:0')\n",
      "======\n",
      "\tDiastolic_blood_pressure_5: 1.54%\n",
      "\tBody_mass_index_3: 1.18%\n",
      "\tTYPE2DIABETES: 0.01%\n",
      "\tCKDSTAGE3TO5: 0.01%\n",
      "\tOSTEOARTHRITIS: 0.00%\n",
      "\tHYPERTENSION: 0.00%\n",
      "\tTYPE1DM: 0.00%\n",
      "Value tensor([-1.1111], device='cuda:0')\n",
      "======\n",
      "\tDiastolic_blood_pressure_5: 1.49%\n",
      "\tBody_mass_index_3: 0.89%\n",
      "\tTYPE2DIABETES: 0.00%\n",
      "\tCKDSTAGE3TO5: 0.00%\n",
      "\tOSTEOARTHRITIS: 0.00%\n",
      "\tHYPERTENSION: 0.00%\n",
      "\tTYPE1DM: 0.00%\n",
      "Value tensor([-0.6667], device='cuda:0')\n",
      "======\n",
      "\tDiastolic_blood_pressure_5: 1.21%\n",
      "\tBody_mass_index_3: 0.52%\n",
      "\tTYPE2DIABETES: 0.00%\n",
      "\tCKDSTAGE3TO5: 0.00%\n",
      "\tOSTEOARTHRITIS: 0.00%\n",
      "\tHYPERTENSION: 0.00%\n",
      "\tTYPE1DM: 0.00%\n",
      "Value tensor([-0.2222], device='cuda:0')\n",
      "======\n",
      "\tDiastolic_blood_pressure_5: 0.85%\n",
      "\tBody_mass_index_3: 0.25%\n",
      "\tTYPE2DIABETES: 0.00%\n",
      "\tCKDSTAGE3TO5: 0.00%\n",
      "\tHYPERTENSION: 0.00%\n",
      "\tOSTEOARTHRITIS: 0.00%\n",
      "\tTYPE1DM: 0.00%\n",
      "Value tensor([0.2222], device='cuda:0')\n",
      "======\n",
      "\tDiastolic_blood_pressure_5: 1.07%\n",
      "\tBody_mass_index_3: 0.20%\n",
      "\tTYPE2DIABETES: 0.00%\n",
      "\tHYPERTENSION: 0.00%\n",
      "\tOSTEOARTHRITIS: 0.00%\n",
      "\tCKDSTAGE3TO5: 0.00%\n",
      "\tTYPE1DM: 0.00%\n",
      "Value tensor([0.6667], device='cuda:0')\n",
      "======\n",
      "\tDiastolic_blood_pressure_5: 3.45%\n",
      "\tBody_mass_index_3: 0.45%\n",
      "\tHYPERTENSION: 0.00%\n",
      "\tOSTEOARTHRITIS: 0.00%\n",
      "\tTYPE2DIABETES: 0.00%\n",
      "\tCKDSTAGE3TO5: 0.00%\n",
      "\tTYPE1DM: 0.00%\n",
      "Value tensor([1.1111], device='cuda:0')\n",
      "======\n",
      "\tDiastolic_blood_pressure_5: 11.88%\n",
      "\tBody_mass_index_3: 1.41%\n",
      "\tHYPERTENSION: 0.03%\n",
      "\tOSTEOARTHRITIS: 0.02%\n",
      "\tTYPE2DIABETES: 0.01%\n",
      "\tCKDSTAGE3TO5: 0.01%\n",
      "\tTYPE1DM: 0.00%\n",
      "Value tensor([1.5556], device='cuda:0')\n",
      "======\n",
      "\tDiastolic_blood_pressure_5: 22.09%\n",
      "\tBody_mass_index_3: 2.50%\n",
      "\tHYPERTENSION: 0.07%\n",
      "\tOSTEOARTHRITIS: 0.07%\n",
      "\tTYPE2DIABETES: 0.03%\n",
      "\tCKDSTAGE3TO5: 0.01%\n",
      "\tTYPE1DM: 0.00%\n",
      "Value tensor([2.], device='cuda:0')\n",
      "======\n",
      "\tDiastolic_blood_pressure_5: 28.79%\n",
      "\tBody_mass_index_3: 3.01%\n",
      "\tHYPERTENSION: 0.11%\n",
      "\tOSTEOARTHRITIS: 0.11%\n",
      "\tTYPE2DIABETES: 0.05%\n",
      "\tCKDSTAGE3TO5: 0.02%\n",
      "\tTYPE1DM: 0.00%\n"
     ]
    }
   ],
   "source": [
    "events_of_interest = [\"Body_mass_index_3\", \"Diastolic_blood_pressure_5\", \n",
    "                      \"TYPE1DM\", \"TYPE2DIABETES\",\n",
    "                      \"HYPERTENSION\", \"OSTEOARTHRITIS\",\n",
    "                      \"CKDSTAGE3TO5\",\n",
    "                      \"HF\", \"ISCHAEMICSTROKE\"\n",
    "                     ]\n",
    "\n",
    "prompt = [\"Diastolic_blood_pressure_5\"]\n",
    "# values = [torch.tensor([standardise(_cat, _value) for _cat in prompt], device=device) for _value in [60.,70.,80.,90.,100.,120.]]\n",
    "values = [torch.tensor([float(v) for _cat in prompt], device=device) for v in np.linspace(-2,2,10)]\n",
    "age = [40]\n",
    "\n",
    "\n",
    "# for condition in target_conditions:\n",
    "#     print(f\"Probability of {condition}\")\n",
    "#     target_token = dm.tokenizer._stoi[condition]\n",
    "\n",
    "for p_idx, value in enumerate(values):\n",
    "    print(f\"Value {value}\\n======\")\n",
    "    encoded_prompt = torch.from_numpy(np.array(dm.encode(prompt)).reshape((1,-1))).to(device)\n",
    "    value = value.reshape((1,-1))\n",
    "    \n",
    "    (lgts, tte_dist, val_dist), _, _ = model(encoded_prompt,\n",
    "                                              values=value,\n",
    "                                             ages=to_days(age),\n",
    "                                             is_generation=True)\n",
    "    probs = torch.nn.functional.softmax(lgts, dim=2) * 100\n",
    "    \n",
    "    topk_prob, topk_ind = torch.sort(probs[0,0,:], descending=True)\n",
    "    for i, j in zip(dm.decode(topk_ind.tolist()).split(\" \"), topk_prob):\n",
    "        if i in events_of_interest:\n",
    "            print(f\"\\t{i}: {j:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Values: How varying diagnosis affects value of diastolic_blood_pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Diagnosis ['DEPRESSION']\n",
      "======\n",
      "standardised diastolic_blood_pressure ~ N(-0.1, 0.2)\n",
      "\n",
      "Diagnosis ['TYPE2DIABETES']\n",
      "======\n",
      "standardised diastolic_blood_pressure ~ N(-0.1, 0.2)\n",
      "\n",
      "Diagnosis ['HF']\n",
      "======\n",
      "standardised diastolic_blood_pressure ~ N(-0.1, 0.2)\n",
      "\n",
      "Diagnosis ['HYPERTENSION']\n",
      "======\n",
      "standardised diastolic_blood_pressure ~ N(-0.1, 0.2)\n"
     ]
    }
   ],
   "source": [
    "# display(dm.tokenizer._stoi)\n",
    "t1_token = dm.tokenizer._stoi[\"Diastolic_blood_pressure_5\"]\n",
    "\n",
    "diagnoses = [[\"DEPRESSION\"],[\"TYPE2DIABETES\"], [\"HF\"], [\"HYPERTENSION\"]]\n",
    "values = torch.tensor([torch.nan], device=device)\n",
    "age = [39]\n",
    "\n",
    "\n",
    "# for condition in target_conditions:\n",
    "#     print(f\"Probability of {condition}\")\n",
    "#     target_token = dm.tokenizer._stoi[condition]\n",
    "\n",
    "for p_idx, diagnosis in enumerate(diagnoses):\n",
    "    print(f\"\\nDiagnosis {diagnosis}\\n======\")\n",
    "    encoded_prompt = torch.from_numpy(np.array(dm.encode(diagnosis)).reshape((1,-1))).to(device)\n",
    "    values = values.reshape((1,-1))\n",
    "    (lgts, tte_dist, val_dist), _, _ = model(encoded_prompt,\n",
    "                                              values=values,\n",
    "                                             ages=to_days(age),\n",
    "                                             is_generation=True)\n",
    "    dist = val_dist[model.value_layer.token_key(t1_token)]\n",
    "    print(f\"standardised diastolic_blood_pressure ~ N({dist.loc.item():.1f}, {dist.scale.item():.1f})\")\n",
    "    # print(f\"\\tprobability of type II diabetes: {100*float(probs[0, 0, t2_token].cpu().detach().numpy()):.4f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Values: How increasing bmi affects value of diastolic_blood_pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values [-2.0]\n",
      "======\n",
      "standardised diastolic_blood_pressure ~ N(-0.5, 0.2)\n",
      "Values [-1.5555555820465088]\n",
      "======\n",
      "standardised diastolic_blood_pressure ~ N(-0.4, 0.2)\n",
      "Values [-1.1111111640930176]\n",
      "======\n",
      "standardised diastolic_blood_pressure ~ N(-0.4, 0.2)\n",
      "Values [-0.6666666865348816]\n",
      "======\n",
      "standardised diastolic_blood_pressure ~ N(-0.3, 0.2)\n",
      "Values [-0.2222222238779068]\n",
      "======\n",
      "standardised diastolic_blood_pressure ~ N(-0.1, 0.2)\n",
      "Values [0.2222222238779068]\n",
      "======\n",
      "standardised diastolic_blood_pressure ~ N(0.0, 0.2)\n",
      "Values [0.6666666865348816]\n",
      "======\n",
      "standardised diastolic_blood_pressure ~ N(0.1, 0.2)\n",
      "Values [1.1111111640930176]\n",
      "======\n",
      "standardised diastolic_blood_pressure ~ N(0.2, 0.2)\n",
      "Values [1.5555555820465088]\n",
      "======\n",
      "standardised diastolic_blood_pressure ~ N(0.3, 0.2)\n",
      "Values [2.0]\n",
      "======\n",
      "standardised diastolic_blood_pressure ~ N(0.3, 0.2)\n"
     ]
    }
   ],
   "source": [
    "t1_token = dm.tokenizer._stoi[\"Diastolic_blood_pressure_5\"]\n",
    "\n",
    "prompt = [\"Body_mass_index_3\"]\n",
    "# values = [torch.tensor([standardise(_cat, _value) for _cat in prompt], device=device) for _value in [12.,15.,18.,21.,24.,30.,40.,50.]]\n",
    "values = [torch.tensor([float(v) for _cat in prompt], device=device) for v in np.linspace(-2,2,10)]\n",
    "age = [40]\n",
    "\n",
    "# for condition in target_conditions:\n",
    "#     print(f\"Probability of {condition}\")\n",
    "#     target_token = dm.tokenizer._stoi[condition]\n",
    "\n",
    "for p_idx, value in enumerate(values):\n",
    "    print(f\"Values {value.tolist()}\\n======\")\n",
    "    encoded_prompt = torch.from_numpy(np.array(dm.encode(prompt)).reshape((1,-1))).to(device)\n",
    "    value = value.reshape((1,-1))\n",
    "\n",
    "    (lgts, tte_dist, val_dist), _, _ = model(encoded_prompt,\n",
    "                                             values=value,\n",
    "                                             ages=to_days(age),\n",
    "                                             is_generation=True)\n",
    "    \n",
    "    dist = val_dist[model.value_layer.token_key(t1_token)]\n",
    "    print(f\"standardised diastolic_blood_pressure ~ N({dist.loc.item():.1f}, {dist.scale.item():.1f})\")\n",
    "    # print(f\"\\tprobability of type II diabetes: {100*float(probs[0, 0, t2_token].cpu().detach().numpy()):.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: model architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TTETransformerForCausalTimeSeriesModelling(\n",
       "  (transformer): TTETransformer(\n",
       "    (wpe): TemporalPositionalEncoding()\n",
       "    (wte): DataEmbeddingLayer(\n",
       "      (static_proj): Linear(in_features=16, out_features=384, bias=True)\n",
       "      (dynamic_embedding_layer): SplitDynamicEmbeddingLayer(\n",
       "        (cat_event_embed_layer): Embedding(184, 384, padding_idx=0)\n",
       "        (cat_event_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (num_value_embed_layer): EmbeddingBag(184, 384, mode=sum, padding_idx=0)\n",
       "        (num_value_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (blocks): ModuleList(\n",
       "      (0): Block(\n",
       "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiHeadedSelfAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (acti): ReLU()\n",
       "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiHeadedSelfAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (acti): ReLU()\n",
       "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiHeadedSelfAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (acti): ReLU()\n",
       "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiHeadedSelfAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (acti): ReLU()\n",
       "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiHeadedSelfAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (acti): ReLU()\n",
       "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiHeadedSelfAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (acti): ReLU()\n",
       "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=384, out_features=184, bias=False)\n",
       "  (tte_layer): ExponentialTTELayer(\n",
       "    (proj): Linear(in_features=384, out_features=1, bias=True)\n",
       "  )\n",
       "  (value_layer): GaussianRegressionLayer(\n",
       "    (regression_layers): ModuleDict(\n",
       "      (Token 15): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 17): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 24): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 26): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 41): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 46): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 49): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 50): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 52): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 53): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 57): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 59): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 61): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 62): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 64): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 67): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 68): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 71): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 74): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 78): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 79): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 80): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 86): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 88): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 89): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 91): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 92): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 93): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 95): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 97): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 98): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 99): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 100): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 101): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 102): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 103): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 105): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 106): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 107): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 109): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 111): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 113): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 114): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 116): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 119): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 121): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 122): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 123): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 124): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 125): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 126): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 127): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 128): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 129): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 130): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 131): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 132): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 133): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 134): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 135): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 136): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 137): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 138): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 139): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 140): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 141): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 142): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 143): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 144): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 145): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 146): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 147): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 148): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 149): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 150): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 151): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 152): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 153): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 154): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 155): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 156): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 157): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 158): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 159): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 160): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 161): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 162): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 163): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 164): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 165): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 166): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 167): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 168): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 169): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 170): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 171): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 172): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 173): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 174): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 175): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 176): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 177): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 178): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 179): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 180): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 181): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 182): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 183): Linear(in_features=384, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook TTE_tabular.ipynb to html\n",
      "[NbConvertApp] Writing 606251 bytes to TTE_tabular.html\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to html --no-input TTE_tabular.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "sys_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
