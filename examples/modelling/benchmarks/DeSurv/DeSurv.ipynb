{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJpXpmjEYC_T"
   },
   "source": [
    "# Demo Notebook:\n",
    "## DeSurv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added path '/rds/homes/g/gaddcz/Projects/CPRD/virtual-envTorch2.0-icelake/lib/python3.10/site-packages' at start of search paths.\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "node_type = os.getenv('BB_CPU')\n",
    "venv_dir = f'/rds/homes/g/gaddcz/Projects/CPRD/virtual-envTorch2.0-{node_type}'\n",
    "venv_site_pkgs = Path(venv_dir) / 'lib' / f'python{sys.version_info.major}.{sys.version_info.minor}' / 'site-packages'\n",
    "if venv_site_pkgs.exists():\n",
    "    sys.path.insert(0, str(venv_site_pkgs))\n",
    "    print(f\"Added path '{venv_site_pkgs}' at start of search paths.\")\n",
    "else:\n",
    "    print(f\"Path '{venv_site_pkgs}' not found. Check that it exists and/or that it exists for node-type '{node_type}'.\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda.\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sqlite3\n",
    "from dataclasses import dataclass\n",
    "import logging\n",
    "from CPRD.data.foundational_loader import FoundationalDataModule\n",
    "import pickle \n",
    "from tqdm import tqdm\n",
    "\n",
    "from pycox.datasets import support\n",
    "from pycox.evaluation import EvalSurv\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from CPRD.src.modules.head_layers.survival.desurv import ODESurvSingle as ODESurvSingleNatalia\n",
    "from CPRD.src.modules.head_layers.survival.desurv import ODESurvMultiple as ODESurvMultipleNatalia\n",
    "\n",
    "from CPRD.src.modules.head_layers.survival.desurv_original import ODESurvSingle as ODESurvSingleOriginal1\n",
    "from CPRD.src.modules.head_layers.survival.desurv_original import ODESurvMultiple as ODESurvMultipleOriginal1\n",
    "\n",
    "from DeSurv.src.classes import ODESurvSingle as ODESurvSingleOriginal2\n",
    "from DeSurv.src.classes import ODESurvMultiple as ODESurvMultipleOriginal2\n",
    "\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = \"cpu\"    # if more informative debugging statements are needed\n",
    "print(f\"Using device: {device}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['X_train', 'y_train', 'X_val', 'y_val', 'X_test', 'y_test'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6000, 279)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "numpy.float32"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(6000,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(6000,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "numpy.float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2921666666666667\n",
      "1.5458627448876698\n",
      "1.080586215953429\n",
      "0.1262559\n",
      "0.0005474090576171875\n",
      "4.402191162109375\n",
      "(array([0, 1, 2, 3, 4, 5]), array([31472,  1956,   373,   895,   942,   120]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['X_train', 'y_train', 'X_val', 'y_val', 'X_test', 'y_test'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = \"CVD\"\n",
    "competing_risk = True\n",
    "\n",
    "# the time grid which we generate over\n",
    "t_eval = np.linspace(0, 1, 1000) \n",
    "# the time grid which we calculate scores over\n",
    "time_grid = np.linspace(start=0, stop=1 , num=300)\n",
    "\n",
    "\n",
    "match dataset.lower():\n",
    "    case \"pycox\":\n",
    "        df_train = support.read_df()\n",
    "        df_test = df_train.sample(frac=0.2)\n",
    "        df_train = df_train.drop(df_test.index)\n",
    "        df_val = df_train.sample(frac=0.2)\n",
    "        df_train = df_train.drop(df_val.index)\n",
    "        \n",
    "        cols_standardize = ['x0', 'x7', 'x8', 'x9', 'x10', 'x11', 'x12', 'x13']\n",
    "        cols_leave = ['x1', 'x2', 'x3', 'x4', 'x5', 'x6']\n",
    "        \n",
    "        standardize = [([col], StandardScaler()) for col in cols_standardize]\n",
    "        leave = [(col, None) for col in cols_leave]\n",
    "        \n",
    "        x_mapper = DataFrameMapper(standardize + leave)\n",
    "        \n",
    "        x_train = x_mapper.fit_transform(df_train).astype('float32')\n",
    "        x_val = x_mapper.transform(df_val).astype('float32')\n",
    "        x_test = x_mapper.transform(df_test).astype('float32')\n",
    "        \n",
    "        get_target = lambda df: (df['duration'].values, df['event'].values)\n",
    "        y_train = get_target(df_train)\n",
    "        y_val = get_target(df_val)\n",
    "        y_test = get_target(df_test)\n",
    "        \n",
    "        t_train, e_train = y_train\n",
    "        t_val, e_val = y_val\n",
    "        t_test, e_test = y_test\n",
    "        \n",
    "        t_train_max = np.amax(t_train)\n",
    "        t_train = t_train / t_train_max\n",
    "        t_val = t_val / t_train_max\n",
    "        t_test = t_test / t_train_max\n",
    "        \n",
    "\n",
    "    case \"hypertension\" | \"cvd\":\n",
    "        \n",
    "        with open(f'/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_{dataset}/CrossSectionalData.pickle', \"rb\") as handle:\n",
    "            data = pickle.load(handle)\n",
    "        \n",
    "        # display(data[\"X_train\"].head())\n",
    "        # display(data[\"y_train\"])\n",
    "        print(data.keys())\n",
    "        \n",
    "        subset = True\n",
    "        if subset:\n",
    "            data[\"X_train\"] = data[\"X_train\"][:6000]\n",
    "            data[\"y_train\"] = data[\"y_train\"][:6000]\n",
    "            data[\"X_val\"] = data[\"X_val\"][:10000]\n",
    "            data[\"y_val\"] = data[\"y_val\"][:10000]\n",
    "            data[\"X_test\"] = data[\"X_test\"][:50000]\n",
    "            data[\"y_test\"] = data[\"y_test\"][:50000]\n",
    "\n",
    "\n",
    "        x_train = data[\"X_train\"].to_numpy(dtype=np.float32)\n",
    "        x_val = data[\"X_val\"].to_numpy(dtype=np.float32)\n",
    "        x_test = data[\"X_test\"].to_numpy(dtype=np.float32)\n",
    "        \n",
    "        t_train = np.asarray([i[1] for i in data[\"y_train\"]])\n",
    "        t_val = np.asarray([i[1] for i in data[\"y_val\"]])        \n",
    "        t_test = np.asarray([i[1] for i in data[\"y_test\"]])\n",
    "\n",
    "        if competing_risk is False:\n",
    "            e_train = np.asarray([0 if i[0] == 0 else 1 for i in data[\"y_train\"]])\n",
    "            e_val = np.asarray([0 if i[0] == 0 else 1 for i in data[\"y_val\"]])\n",
    "            e_test = np.asarray([0 if i[0] == 0 else 1 for i in data[\"y_test\"]])\n",
    "        else:\n",
    "            e_train = np.asarray([i[0] for i in data[\"y_train\"]])\n",
    "            e_val = np.asarray([i[0] for i in data[\"y_val\"]])\n",
    "            e_test = np.asarray([i[0] for i in data[\"y_test\"]])\n",
    "\n",
    "batch_size = 32\n",
    "dataset_train = TensorDataset(*[torch.tensor(u,dtype=dtype_) for u, dtype_ in [(x_train,torch.float32),\n",
    "                                                                               (t_train,torch.float32),\n",
    "                                                                               (e_train,torch.long)]])\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=batch_size, pin_memory=True, shuffle=True, drop_last=True)\n",
    "\n",
    "dataset_val = TensorDataset(*[torch.tensor(u,dtype=dtype_) for u, dtype_ in [(x_val,torch.float32),\n",
    "                                                                               (t_val,torch.float32),\n",
    "                                                                               (e_val,torch.long)]])\n",
    "data_loader_val = DataLoader(dataset_val, batch_size=batch_size, pin_memory=True, shuffle=True)\n",
    "\n",
    "dataset_test = TensorDataset(*[torch.tensor(u,dtype=dtype_) for u, dtype_ in [(x_test,torch.float32),\n",
    "                                                                               (t_test,torch.float32),\n",
    "                                                                               (e_test,torch.long)]])\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=batch_size, pin_memory=True, shuffle=True)\n",
    "\n",
    "display(x_train.shape)\n",
    "display(type(x_train))\n",
    "display(type(x_train[0,0]))\n",
    "display(e_train.shape)\n",
    "display(type(e_train))\n",
    "display(type(e_train[0]))\n",
    "display(t_train.shape)\n",
    "display(type(t_train))\n",
    "display(type(t_train[0]))\n",
    "\n",
    "print(np.mean(e_train))\n",
    "print(np.mean(t_train))\n",
    "print(np.std(t_train))\n",
    "print(np.mean(x_train))\n",
    "print(t_train.min())\n",
    "print(t_train.max())\n",
    "\n",
    "print(np.unique(e_test, return_counts=True))\n",
    "\n",
    "data.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def divide_chunks(array, n):\n",
    "#     for i in range(0, array.shape[0], n): \n",
    "#         yield array[i:i + n, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:original\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CondODENet: cpu specified, cpu used\n",
      "FCNet: cpu specified, cpu used\n",
      "\n",
      "\n",
      "CVD_cr_natalia_ with 20460 parameters\n",
      "Training\n",
      "\tEpoch:  0. Total loss:     4626.13\n",
      "best_epoch: 0\n",
      "\tEpoch:  0. Total val loss:     5049.69\n",
      "\tEpoch:  1. Total loss:     3767.31\n",
      "best_epoch: 1\n",
      "\tEpoch:  1. Total val loss:     4937.90\n",
      "\tEpoch:  2. Total loss:     3674.22\n",
      "\tEpoch:  2. Total val loss:     4944.79\n",
      "\tEpoch:  3. Total loss:     3583.28\n",
      "best_epoch: 3\n",
      "\tEpoch:  3. Total val loss:     4906.40\n",
      "\tEpoch:  4. Total loss:     3522.92\n",
      "\tEpoch:  4. Total val loss:     4930.55\n",
      "\tEpoch:  5. Total loss:     3447.98\n",
      "\tEpoch:  5. Total val loss:     5018.83\n",
      "\tEpoch:  6. Total loss:     3377.67\n",
      "finished training\n",
      "Testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 699/699 [08:11<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 of [1 2 3 4 5] included from 6 surv CDFs\n",
      "1\n",
      "2 of [1 2 3 4 5] included from 6 surv CDFs\n",
      "2\n",
      "3 of [1 2 3 4 5] included from 6 surv CDFs\n",
      "3\n",
      "4 of [1 2 3 4 5] included from 6 surv CDFs\n",
      "4\n",
      "5 of [1 2 3 4 5] included from 6 surv CDFs\n",
      "5\n",
      "Evaluating survival metrics\n",
      "0.6092955801705897\n",
      "0.0339398728091127\n",
      "0.14779968027386553\n",
      "\n",
      "\n",
      "CVD_cr_original1_ with 20460 parameters\n",
      "Training\n",
      "\tEpoch:  0. Total loss:     5169.06\n",
      "best_epoch: 0\n",
      "\tEpoch:  0. Total val loss:     5072.99\n",
      "\tEpoch:  1. Total loss:     3753.11\n",
      "best_epoch: 1\n",
      "\tEpoch:  1. Total val loss:     4954.16\n",
      "\tEpoch:  2. Total loss:     3665.37\n",
      "\tEpoch:  2. Total val loss:     4996.64\n",
      "\tEpoch:  3. Total loss:     3604.69\n",
      "\tEpoch:  3. Total val loss:     4975.74\n",
      "\tEpoch:  4. Total loss:     3523.51\n",
      "finished training\n",
      "Testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 699/699 [05:17<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 of [1 2 3 4 5] included from 6 surv CDFs\n",
      "1\n",
      "2 of [1 2 3 4 5] included from 6 surv CDFs\n",
      "2\n",
      "3 of [1 2 3 4 5] included from 6 surv CDFs\n",
      "3\n",
      "4 of [1 2 3 4 5] included from 6 surv CDFs\n",
      "4\n",
      "5 of [1 2 3 4 5] included from 6 surv CDFs\n",
      "5\n",
      "Evaluating survival metrics\n",
      "0.5891815191970715\n",
      "0.034083698717162145\n",
      "0.14782213495611307\n",
      "\n",
      "\n",
      "CVD_cr_original2_ with 20460 parameters\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rds/bear-apps/2022a/EL8-ice/software/PyTorch/2.0.1-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch:  0. Total loss:     5313.00\n",
      "best_epoch: 0\n",
      "\tEpoch:  0. Total val loss:     5152.35\n",
      "\tEpoch:  1. Total loss:     3790.26\n",
      "best_epoch: 1\n",
      "\tEpoch:  1. Total val loss:     4961.07\n",
      "\tEpoch:  2. Total loss:     3667.32\n",
      "\tEpoch:  2. Total val loss:     5001.15\n",
      "\tEpoch:  3. Total loss:     3589.61\n",
      "best_epoch: 3\n",
      "\tEpoch:  3. Total val loss:     4932.00\n",
      "\tEpoch:  4. Total loss:     3525.06\n",
      "\tEpoch:  4. Total val loss:     4961.14\n",
      "\tEpoch:  5. Total loss:     3442.06\n",
      "\tEpoch:  5. Total val loss:     5127.22\n",
      "\tEpoch:  6. Total loss:     3400.03\n",
      "finished training\n",
      "Testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 699/699 [00:47<00:00, 14.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 of [1 2 3 4 5] included from 6 surv CDFs\n",
      "1\n",
      "2 of [1 2 3 4 5] included from 6 surv CDFs\n",
      "2\n",
      "3 of [1 2 3 4 5] included from 6 surv CDFs\n",
      "3\n",
      "4 of [1 2 3 4 5] included from 6 surv CDFs\n",
      "4\n",
      "5 of [1 2 3 4 5] included from 6 surv CDFs\n",
      "5\n",
      "Evaluating survival metrics\n",
      "0.6057716222956506\n",
      "0.034037456172092855\n",
      "0.14743363247370506\n"
     ]
    }
   ],
   "source": [
    "hidden_dim = 32\n",
    "training = True\n",
    "lr = 1e-3\n",
    "xdim = x_train.shape[1]\n",
    "\n",
    "models, model_names = [], []\n",
    "if competing_risk is False:\n",
    "    models.append(ODESurvSingleNatalia(xdim, 32, device=device))\n",
    "    model_names.append(f\"{dataset}_sr_natalia_\")\n",
    "    models.append(ODESurvSingleOriginal1(xdim, hidden_dim, device=device))\n",
    "    model_names.append(f\"{dataset}_sr_original1_\")\n",
    "    models.append(ODESurvSingleOriginal2(lr, xdim, hidden_dim, device=device))\n",
    "    model_names.append(f\"{dataset}_sr_original2_\")\n",
    "else:\n",
    "    models.append(ODESurvMultipleNatalia(xdim, [32, 32], num_risks=6))\n",
    "    model_names.append(f\"{dataset}_cr_natalia_\")\n",
    "    models.append(ODESurvMultipleOriginal1(xdim, hidden_dim, num_risks=6))\n",
    "    model_names.append(f\"{dataset}_cr_original1_\")\n",
    "    models.append(ODESurvMultipleOriginal2(lr, xdim, hidden_dim, num_risks=6))\n",
    "    model_names.append(f\"{dataset}_cr_original2_\")\n",
    "\n",
    "for model_name, model in zip(model_names, models):\n",
    "\n",
    "    print(f\"\\n\\n{model_name} with {sum(p.numel() for p in model.parameters() if p.requires_grad)} parameters\")\n",
    "    \n",
    "    if training:\n",
    "        print(f\"Training\")\n",
    "        model.optimize(data_loader_train, n_epochs=20, logging_freq=1, data_loader_val=data_loader_val, max_wait=2)\n",
    "        print(\"finished training\")\n",
    "        torch.save(model.state_dict(), model_name + \"tst_model\")\n",
    "        model.eval()\n",
    "\n",
    "    print(f\"Testing\")    \n",
    "    state_dict = torch.load(model_name + \"tst_model\")\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    \n",
    "    argsortttest = np.argsort(t_test)\n",
    "    t_test = t_test[argsortttest]\n",
    "    e_test = e_test[argsortttest]\n",
    "    x_test = x_test[argsortttest,:]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        # The normalised grid over which to predict\n",
    "        t_test_grid = torch.tensor(np.concatenate([t_eval] * x_test.shape[0], 0), dtype=torch.float32)\n",
    "        x_test_grid = torch.tensor(x_test, dtype=torch.float32).repeat_interleave(t_eval.size, 0)\n",
    "        \n",
    "        pred_bsz = 51200\n",
    "        pred = []\n",
    "        for x_test_batched, t_test_batched in tqdm(zip(torch.split(x_test_grid, pred_bsz), torch.split(t_test_grid, pred_bsz)),\n",
    "                                                   total=(x_test_grid.shape[0]//pred_bsz)+1 ):\n",
    "            \n",
    "            if competing_risk is False:\n",
    "                pred_ = model.predict(x_test_batched, t_test_batched)          # shape: (x_test.batched.shape[0],)\n",
    "            else:\n",
    "                pred_, pi_  = model.predict(x_test_batched, t_test_batched)    # shape: (x_test.batched.shape[0], num_outcomes)\n",
    "                # pred_ = pred_ #* pi_                  # \\sum_k={1,...,K} not \\emptyset F_k\n",
    "            pred.append(pred_)\n",
    "                \n",
    "        pred = torch.concat(pred)\n",
    "    \n",
    "        pred = pred.reshape((x_test.shape[0], t_eval.size, -1)).cpu().detach().numpy()\n",
    "        preds = [pred[:, :, _i] for _i in range(pred.shape[-1])]\n",
    "        # print([_.shape for _ in preds])\n",
    "\n",
    "        # Merge (additively) each outcome risk curve into a single CDF, and update label for if outcome occurred or not\n",
    "        cdf = np.zeros_like(preds[0])\n",
    "        lbls = np.zeros_like(e_test)     \n",
    "        for _outcome_token in np.unique(e_test)[1:]:\n",
    "            print(f\"{_outcome_token} of {np.unique(e_test)[1:]} included from {len(preds)} surv CDFs\")\n",
    "            print(_outcome_token)\n",
    "            cdf += preds[_outcome_token - 1] \n",
    "            lbls += (e_test == _outcome_token)\n",
    "        \n",
    "        surv = pd.DataFrame(np.transpose((1 - cdf.reshape((x_test.shape[0],t_eval.size)))), index=t_eval)\n",
    "\n",
    "    # Evaluate surv curve with unscaled index with unscaled test times to event \n",
    "    print(\"Evaluating survival metrics\")\n",
    "    ev = EvalSurv(surv, t_test, lbls, censor_surv='km')\n",
    "    \n",
    "    print(ev.concordance_td())\n",
    "    print(ev.integrated_brier_score(time_grid))\n",
    "    print(ev.integrated_nbll(time_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 35758000 / (1000 * 1)\n",
    "# print(pred.reshape(x_test.shape[0], 1000,-1).shape)\n",
    "# print(x_test.shape)\n",
    "# pred = np.concatenate(preds, 0)             # n_test * t_eval.size(), num_risks\n",
    "# pred = pred.reshape(x_test.shape[0], 1000,-1)\n",
    "# print(pred.shape)\n",
    "# print([_p.shape for _p in preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "8034.000611305237"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6057716222956506\n",
      "0.034038439349878824\n",
      "0.1474335340266184\n"
     ]
    }
   ],
   "source": [
    "display(t_eval.max())\n",
    "\n",
    "display(t_test.max()*5*365)\n",
    "\n",
    "print(ev.concordance_td())\n",
    "print(ev.integrated_brier_score(t_eval))\n",
    "print(ev.integrated_nbll(t_eval))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "sys_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
