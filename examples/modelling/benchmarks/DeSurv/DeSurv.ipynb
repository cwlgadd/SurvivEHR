{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJpXpmjEYC_T"
   },
   "source": [
    "# Demo Notebook:\n",
    "## DeSurv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added path '/rds/homes/g/gaddcz/Projects/CPRD/virtual-envTorch2.0-icelake/lib/python3.10/site-packages' at start of search paths.\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "node_type = os.getenv('BB_CPU')\n",
    "venv_dir = f'/rds/homes/g/gaddcz/Projects/CPRD/virtual-envTorch2.0-{node_type}'\n",
    "venv_site_pkgs = Path(venv_dir) / 'lib' / f'python{sys.version_info.major}.{sys.version_info.minor}' / 'site-packages'\n",
    "if venv_site_pkgs.exists():\n",
    "    sys.path.insert(0, str(venv_site_pkgs))\n",
    "    print(f\"Added path '{venv_site_pkgs}' at start of search paths.\")\n",
    "else:\n",
    "    print(f\"Path '{venv_site_pkgs}' not found. Check that it exists and/or that it exists for node-type '{node_type}'.\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda.\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sqlite3\n",
    "from dataclasses import dataclass\n",
    "import logging\n",
    "from CPRD.data.foundational_loader import FoundationalDataModule\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pycox.datasets import support\n",
    "from pycox.evaluation import EvalSurv\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from CPRD.src.modules.head_layers.survival.desurv import ODESurvSingle\n",
    "from CPRD.src.modules.head_layers.survival.desurv import ODESurvMultiple\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = \"cpu\"    # if more informative debugging statements are needed\n",
    "print(f\"Using device: {device}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(dataset, competing_risk, sample_size=None):\n",
    "\n",
    "    match dataset.lower():\n",
    "        case \"pycox\":\n",
    "            df_train = support.read_df()\n",
    "            df_test = df_train.sample(frac=0.2)\n",
    "            df_train = df_train.drop(df_test.index)\n",
    "            df_val = df_train.sample(frac=0.2)\n",
    "            df_train = df_train.drop(df_val.index)\n",
    "            \n",
    "            cols_standardize = ['x0', 'x7', 'x8', 'x9', 'x10', 'x11', 'x12', 'x13']\n",
    "            cols_leave = ['x1', 'x2', 'x3', 'x4', 'x5', 'x6']\n",
    "            \n",
    "            standardize = [([col], StandardScaler()) for col in cols_standardize]\n",
    "            leave = [(col, None) for col in cols_leave]\n",
    "            \n",
    "            x_mapper = DataFrameMapper(standardize + leave)\n",
    "            \n",
    "            x_train = x_mapper.fit_transform(df_train).astype('float32')\n",
    "            x_val = x_mapper.transform(df_val).astype('float32')\n",
    "            x_test = x_mapper.transform(df_test).astype('float32')\n",
    "            \n",
    "            get_target = lambda df: (df['duration'].values, df['event'].values)\n",
    "            y_train = get_target(df_train)\n",
    "            y_val = get_target(df_val)\n",
    "            y_test = get_target(df_test)\n",
    "            \n",
    "            t_train, e_train = y_train\n",
    "            t_val, e_val = y_val\n",
    "            t_test, e_test = y_test\n",
    "            \n",
    "            t_train_max = np.amax(t_train)\n",
    "            t_train = t_train / t_train_max\n",
    "            t_val = t_val / t_train_max\n",
    "            t_test = t_test / t_train_max\n",
    "            \n",
    "    \n",
    "        case \"hypertension\" | \"cvd\":\n",
    "    \n",
    "            # Training samples\n",
    "            if sample_size is not None:\n",
    "                save_path =  f\"/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_{dataset}/\" + f\"benchmark_data/N={sample_size}.pickle\" \n",
    "            else:\n",
    "                save_path = f\"/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_{dataset}/\" + \"benchmark_data/all.pickle\"\n",
    "                \n",
    "            with open(save_path, \"rb\") as handle:\n",
    "                print(f\"Loading training dataset from {save_path}\")\n",
    "                data_train = pickle.load(handle)\n",
    "            \n",
    "            # display(data[\"X_train\"].head())\n",
    "            # display(data[\"y_train\"])\n",
    "            # print(data.keys())\n",
    "            \n",
    "            data = {}\n",
    "            data[\"X_train\"] = data_train[\"X_train\"]\n",
    "            data[\"y_train\"] = data_train[\"y_train\"]\n",
    "    \n",
    "            # Test and validation samples\n",
    "    \n",
    "            save_path = f\"/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_{dataset}/\" + \"benchmark_data/all.pickle\"\n",
    "            with open(save_path, \"rb\") as handle:\n",
    "                print(f\"Loading validation/test datasets from {save_path}\")\n",
    "                data_val_test = pickle.load(handle)\n",
    "                \n",
    "            data[\"X_val\"] = data_val_test[\"X_val\"]\n",
    "            data[\"y_val\"] = data_val_test[\"y_val\"]\n",
    "            data[\"X_test\"] = data_val_test[\"X_test\"]\n",
    "            data[\"y_test\"] = data_val_test[\"y_test\"]\n",
    "    \n",
    "            # Convert to correct formats\n",
    "            x_train = data[\"X_train\"].to_numpy(dtype=np.float32)\n",
    "            x_val = data[\"X_val\"].to_numpy(dtype=np.float32)\n",
    "            x_test = data[\"X_test\"].to_numpy(dtype=np.float32)\n",
    "            \n",
    "            t_train = np.asarray([i[1] for i in data[\"y_train\"]])\n",
    "            t_val = np.asarray([i[1] for i in data[\"y_val\"]])        \n",
    "            t_test = np.asarray([i[1] for i in data[\"y_test\"]])\n",
    "    \n",
    "            if competing_risk is False:\n",
    "                e_train = np.asarray([0 if i[0] == 0 else 1 for i in data[\"y_train\"]])\n",
    "                e_val = np.asarray([0 if i[0] == 0 else 1 for i in data[\"y_val\"]])\n",
    "                e_test = np.asarray([0 if i[0] == 0 else 1 for i in data[\"y_test\"]])\n",
    "            else:\n",
    "                e_train = np.asarray([i[0] for i in data[\"y_train\"]])\n",
    "                e_val = np.asarray([i[0] for i in data[\"y_val\"]])\n",
    "                e_test = np.asarray([i[0] for i in data[\"y_test\"]])\n",
    "\n",
    "    # display(x_train.shape)\n",
    "    # display(type(x_train))\n",
    "    # display(type(x_train[0,0]))\n",
    "    # display(e_train.shape)\n",
    "    # display(type(e_train))\n",
    "    # display(type(e_train[0]))\n",
    "    # display(t_train.shape)\n",
    "    # display(type(t_train))\n",
    "    # display(type(t_train[0]))\n",
    "    # print(np.mean(e_train))\n",
    "    # print(np.mean(t_train))\n",
    "    # print(np.std(t_train))\n",
    "    # print(np.mean(x_train))\n",
    "    # print(t_train.min())\n",
    "    # print(t_train.max())\n",
    "    # print(np.unique(e_test, return_counts=True))\n",
    "    \n",
    "    \n",
    "    batch_size = 256\n",
    "    dataset_train = TensorDataset(*[torch.tensor(u,dtype=dtype_) for u, dtype_ in [(x_train,torch.float32),\n",
    "                                                                                   (t_train,torch.float32),\n",
    "                                                                                   (e_train,torch.long)]])\n",
    "    data_loader_train = DataLoader(dataset_train, batch_size=batch_size, pin_memory=True, shuffle=True, drop_last=True)\n",
    "    \n",
    "    dataset_val = TensorDataset(*[torch.tensor(u,dtype=dtype_) for u, dtype_ in [(x_val,torch.float32),\n",
    "                                                                                   (t_val,torch.float32),\n",
    "                                                                                   (e_val,torch.long)]])\n",
    "    data_loader_val = DataLoader(dataset_val, batch_size=batch_size, pin_memory=True, shuffle=True)\n",
    "    \n",
    "    dataset_test = TensorDataset(*[torch.tensor(u,dtype=dtype_) for u, dtype_ in [(x_test,torch.float32),\n",
    "                                                                                   (t_test,torch.float32),\n",
    "                                                                                   (e_test,torch.long)]])\n",
    "    data_loader_test = DataLoader(dataset_test, batch_size=batch_size, pin_memory=True, shuffle=True)\n",
    "\n",
    "    return data_loader_train, data_loader_val, data_loader_test\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example dataloader function usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training dataset from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/benchmark_data/all.pickle\n",
      "Loading validation/test datasets from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/benchmark_data/all.pickle\n"
     ]
    }
   ],
   "source": [
    "data_loader_train, data_loader_val, data_loader_test = get_dataloaders(\"CVD\", False, sample_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 279])\n",
      "tensor(0.9167)\n",
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "for batch in data_loader_test:\n",
    "    x_train = batch[0]\n",
    "    print(batch[0].shape)\n",
    "    print(batch[1][0])\n",
    "    print(batch[2][1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model\n",
    "```\n",
    "Hypertension\n",
    "\n",
    "SR\n",
    "[0.768850848388541]\n",
    "[0.08287342061953376]\n",
    "[0.2629869386534056]\n",
    "\n",
    "CVD\n",
    "\n",
    "SR\n",
    "[0.6660374450461991]\n",
    "[0.03355264167471281]\n",
    "[0.14207857084831382]\n",
    "\n",
    "CR\n",
    "[0.6622136902603167]\n",
    "[0.033540062894645686]\n",
    "[0.142287892974895]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"CVD\" # \"Hypertension\"\n",
    "competing_risk = True\n",
    "sample_sizes = [int(np.exp(_log_n)) for _log_n in np.linspace(np.log(3000), np.log(500000), 10)]      # [3000, 12500, 30000, 60000, 100000]: # 600, 1200, \n",
    "# sample_sizes = [None]\n",
    "sample_sizes = sample_sizes[-1:]\n",
    "\n",
    "training = True\n",
    "lr = 1e-3\n",
    "xdim = x_train.shape[1]\n",
    "\n",
    "\n",
    "# the time grid which we generate over\n",
    "t_eval = np.linspace(0, 1, 1000) \n",
    "# the time grid which we calculate scores over\n",
    "time_grid = np.linspace(start=0, stop=1 , num=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "CVD_cr with 20394 parameters\n",
      "Loading training dataset from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/benchmark_data/N=500000.pickle\n",
      "Loading validation/test datasets from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/benchmark_data/all.pickle\n",
      "Training\n",
      "\tEpoch:  0. Total loss:   296443.99\n",
      "best_epoch: 0\n",
      "\tEpoch:  0. Total val loss:    18485.24\n",
      "\tEpoch:  1. Total loss:   286393.12\n",
      "best_epoch: 1\n",
      "\tEpoch:  1. Total val loss:    18413.58\n",
      "\tEpoch:  2. Total loss:   284411.50\n",
      "\tEpoch:  2. Total val loss:    18414.11\n",
      "\tEpoch:  3. Total loss:   282880.24\n",
      "\tEpoch:  3. Total val loss:    18434.12\n",
      "\tEpoch:  4. Total loss:   281413.23\n",
      "finished training\n",
      "Testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 140/140 [07:51<00:00,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CVD_cr:             N=500000.      Ctd: 0.6582288879213832. IBS: 0.033605688713984706. INBLL: 0.14273257727425628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "models, model_names = [], []\n",
    "if competing_risk is False:\n",
    "    models.append(ODESurvSingle(xdim, [32, 32], device=device))\n",
    "    model_names.append(f\"{dataset}_sr\")\n",
    "else:\n",
    "    models.append(ODESurvMultiple(xdim, [32, 32], num_risks=5))\n",
    "    model_names.append(f\"{dataset}_cr\")\n",
    "\n",
    "all_ctd, all_ibs, all_inbll = [], [], []\n",
    "for model_name, model in zip(model_names, models):\n",
    "\n",
    "    print(f\"\\n\\n{model_name} with {sum(p.numel() for p in model.parameters() if p.requires_grad)} parameters\")\n",
    "\n",
    "    for sample_size in sample_sizes:\n",
    "    \n",
    "        data_loader_train, data_loader_val, data_loader_test = get_dataloaders(dataset, competing_risk, sample_size=sample_size)\n",
    "    \n",
    "        if training:\n",
    "            print(f\"Training\")\n",
    "            model.optimize(data_loader_train, n_epochs=20, logging_freq=1, data_loader_val=data_loader_val, max_wait=2)\n",
    "            print(\"finished training\")\n",
    "            torch.save(model.state_dict(), model_name + \"tst_model\")\n",
    "            model.eval()\n",
    "    \n",
    "        print(f\"Testing\")    \n",
    "        state_dict = torch.load(model_name + \"tst_model\")\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.eval()\n",
    "        \n",
    "        # argsortttest = np.argsort(t_test)\n",
    "        # t_test = t_test[argsortttest]\n",
    "        # e_test = e_test[argsortttest]\n",
    "        # x_test = x_test[argsortttest,:]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "    \n",
    "            ctd = []\n",
    "            ibs = []\n",
    "            inbll = []\n",
    "            for batch in tqdm(data_loader_test, total=(len(data_loader_test)), desc=\"Testing\"):\n",
    "    \n",
    "                x_test = batch[0].numpy()\n",
    "                t_test = batch[1].numpy()\n",
    "                e_test = batch[2].numpy()\n",
    "    \n",
    "                # The normalised grid over which to predict\n",
    "                t_test_grid = torch.tensor(np.concatenate([t_eval] * x_test.shape[0], 0), dtype=torch.float32)\n",
    "                x_test_grid = torch.tensor(x_test, dtype=torch.float32).repeat_interleave(t_eval.size, 0)\n",
    "                \n",
    "                pred_bsz = 51200\n",
    "                pred = []\n",
    "                for x_test_batched, t_test_batched in zip(torch.split(x_test_grid, pred_bsz), torch.split(t_test_grid, pred_bsz)):\n",
    "                    \n",
    "                    if competing_risk is False:\n",
    "                        pred_ = model.predict(x_test_batched, t_test_batched)          # shape: (x_test.batched.shape[0],)\n",
    "                    else:\n",
    "                        pred_, pi_  = model.predict(x_test_batched, t_test_batched)    # shape: (x_test.batched.shape[0], num_outcomes)\n",
    "                    pred.append(pred_)\n",
    "                        \n",
    "                pred = torch.concat(pred)\n",
    "            \n",
    "                pred = pred.reshape((x_test.shape[0], t_eval.size, -1)).cpu().detach().numpy()\n",
    "                preds = [pred[:, :, _i] for _i in range(pred.shape[-1])]\n",
    "                # print([_.shape for _ in preds])\n",
    "        \n",
    "                # Merge (additively) each outcome risk curve into a single CDF, and update label for if outcome occurred or not\n",
    "                cdf = np.zeros_like(preds[0])\n",
    "                lbls = np.zeros_like(e_test)     \n",
    "                for _outcome_token in np.unique(e_test)[1:]:\n",
    "                    # print(f\"{_outcome_token} of {np.unique(e_test)[1:]} included from {len(preds)} surv CDFs\")\n",
    "                    # print(_outcome_token)\n",
    "                    cdf += preds[_outcome_token - 1] \n",
    "                    lbls += (e_test == _outcome_token)\n",
    "                \n",
    "                surv = pd.DataFrame(np.transpose((1 - cdf.reshape((x_test.shape[0],t_eval.size)))), index=t_eval)\n",
    "    \n",
    "                # Evaluate surv curve with unscaled index with unscaled test times to event \n",
    "                ev = EvalSurv(surv, t_test, lbls, censor_surv='km')\n",
    "                try:\n",
    "                    # Same treatment as in SurvivEHR\n",
    "                    ctd.append(ev.concordance_td())\n",
    "                    ibs.append(ev.integrated_brier_score(time_grid))\n",
    "                    inbll.append(ev.integrated_nbll(time_grid))\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "            ctd = np.mean(ctd)\n",
    "            ibs = np.mean(ibs)\n",
    "            inbll = np.mean(inbll)\n",
    "\n",
    "            print(f\"{model_name}:\".ljust(20) + f\"N={sample_size}.\".ljust(15) + f\"Ctd: {ctd}. IBS: {ibs}. INBLL: {inbll}\")\n",
    "\n",
    "        all_ctd.append(ctd)\n",
    "        all_ibs.append(ibs)\n",
    "        all_inbll.append(inbll)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(surv.head())\n",
    "print(surv.shape)\n",
    "print(preds[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_sizes[-1] = 572096\n",
    "print(sample_sizes)\n",
    "print(all_ctd)\n",
    "print(all_ibs)\n",
    "print(all_inbll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output across different setups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypertension Single Risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypertension Competing Risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypertension_cr:N=2999.        Ctd: 0.6815094745489083. IBS: 0.09055914251601428. INBLL: 0.31029164842950396\n",
    "# Hypertension_cr:N=5296.        Ctd: 0.7124953181062986. IBS: 0.08950243267368668. INBLL: 0.2998138831024105\n",
    "# Hypertension_cr:N=9351.        Ctd: 0.7274689831170883. IBS: 0.08788381776138605. INBLL: 0.29219335002924224\n",
    "# Hypertension_cr:N=16509.       Ctd: 0.730682386913791. IBS: 0.08713170539614512. INBLL: 0.2915679214433027\n",
    "# Hypertension_cr:N=29148.       Ctd: 0.7389774246331557. IBS: 0.08589961516053798. INBLL: 0.2797340688171351\n",
    "# Hypertension_cr:N=51461.       Ctd: 0.7411341306621856. IBS: 0.08509218669879787. INBLL: 0.27232015638634294\n",
    "# Hypertension_cr:N=90856.       Ctd: 0.7515646895266107. IBS: 0.08443103008855017. INBLL: 0.27128357563201616\n",
    "# Hypertension_cr:N=160407.      Ctd: 0.7600540925292503. IBS: 0.08451878180772691. INBLL: 0.2692630746813842\n",
    "# Hypertension_cr:N=283203.      Ctd: 0.7620305063798369. IBS: 0.08404512042043363. INBLL: 0.2666228576617026\n",
    "# Hypertension_cr:N=500000.      Ctd: 0.761360566628242. IBS: 0.08371696663855675. INBLL: 0.26584980102348854"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cardiovascular disease Single Risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cardiovascular disease Competing Risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CVD_cr:          N=2999.        Ctd: 0.5625189849883911. IBS: 0.03435665860871859. INBLL: 0.14990771301641295\n",
    "# CVD_cr:          N=5296.        Ctd: 0.5869489534497859. IBS: 0.03411933684032459. INBLL: 0.14823684700350284\n",
    "# CVD_cr:          N=9351.        Ctd: 0.5993921885610577. IBS: 0.03442101009915632. INBLL: 0.14955773914208445\n",
    "# CVD_cr:          N=16509.       Ctd: 0.6101382510977712. IBS: 0.03387871579676817. INBLL: 0.14660058710195034\n",
    "# CVD_cr:          N=29148.       Ctd: 0.6200633894187596. IBS: 0.033806085396407184. INBLL: 0.1458525721893707\n",
    "# CVD_cr:          N=51461.       Ctd: 0.624818164536866. IBS: 0.033786555455045275. INBLL: 0.14535719048483275\n",
    "# CVD_cr:          N=90856.       Ctd: 0.6355840601226502. IBS: 0.03364728847608413. INBLL: 0.14414393636167722\n",
    "# CVD_cr:          N=160407.      Ctd: 0.6469820170508909. IBS: 0.033556625283347644. INBLL: 0.14308219809140393\n",
    "# CVD_cr:          N=283203.      Ctd: 0.6557505764681258. IBS: 0.033524955828030355. INBLL: 0.1423563785027365\n",
    "# CVD_cr:          N=500000.      Ctd: 0.6582288879213832. IBS: 0.033605688713984706. INBLL: 0.14273257727425628"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "sys_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
