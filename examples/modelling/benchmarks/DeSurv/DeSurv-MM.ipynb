{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJpXpmjEYC_T"
   },
   "source": [
    "# Demo Notebook:\n",
    "## DeSurv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added path '/rds/homes/g/gaddcz/Projects/CPRD/virtual-envTorch2.0-icelake/lib/python3.10/site-packages' at start of search paths.\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "node_type = os.getenv('BB_CPU')\n",
    "venv_dir = f'/rds/homes/g/gaddcz/Projects/CPRD/virtual-envTorch2.0-{node_type}'\n",
    "venv_site_pkgs = Path(venv_dir) / 'lib' / f'python{sys.version_info.major}.{sys.version_info.minor}' / 'site-packages'\n",
    "if venv_site_pkgs.exists():\n",
    "    sys.path.insert(0, str(venv_site_pkgs))\n",
    "    print(f\"Added path '{venv_site_pkgs}' at start of search paths.\")\n",
    "else:\n",
    "    print(f\"Path '{venv_site_pkgs}' not found. Check that it exists and/or that it exists for node-type '{node_type}'.\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from pycox.datasets import support\n",
    "from pycox.evaluation import EvalSurv\n",
    "from scipy.integrate import trapz\n",
    "\n",
    "from FastEHR.dataloader import FoundationalDataModule\n",
    "\n",
    "from CPRD.src.modules.head_layers.survival.desurv import ODESurvSingle\n",
    "from CPRD.src.modules.head_layers.survival.desurv import ODESurvMultiple\n",
    "from CPRD.examples.modelling.benchmarks.make_method_loaders import get_dataloaders\n",
    "from CPRD.examples.modelling.SurvivEHR.custom_outcome_methods import custom_mm_outcomes\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = \"cpu\"    # if more informative debugging statements are needed\n",
    "print(f\"Using device: {device}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the indicies which relate to the diagnoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Creating unsupervised collator for DataModule\n",
      "INFO:root:Using meta information from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/meta_information_QuantJenny.pickle\n",
      "INFO:root:Using train file-row count dictionary from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_MultiMorbidity50+/file_row_count_dict_train.pickle\n",
      "INFO:root:Using test file-row count dictionary from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_MultiMorbidity50+/file_row_count_dict_test.pickle\n",
      "INFO:root:Using val file-row count dictionary from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_MultiMorbidity50+/file_row_count_dict_val.pickle\n",
      "INFO:root:Tokenzier created based on 7,555,415,275 tokens\n",
      "INFO:root:Using tabular tokenizer, created from meta information and containing 265 tokens\n",
      "INFO:root:Set seed to 42\n",
      "INFO:root:Loaded /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_MultiMorbidity50+/split=train/ dataset, with 1,650,998 samples\n",
      "INFO:root:Set seed to 42\n",
      "INFO:root:Loaded /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_MultiMorbidity50+/split=test/ dataset, with 107,557 samples\n",
      "INFO:root:Set seed to 42\n",
      "INFO:root:Loaded /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_MultiMorbidity50+/split=val/ dataset, with 91,487 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 31, 33, 34, 35, 36, 37, 38, 41, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 58, 59, 60, 61, 63, 64, 67, 71, 72, 73, 75, 77, 80, 82, 85, 89, 90, 93, 94, 96, 97, 98, 104, 106, 108, 109, 110, 112, 115, 119, 121, 129, 135, 138, 141, 144, 148, 149, 151]\n"
     ]
    }
   ],
   "source": [
    "with initialize(version_base=None, config_path=\"../../SurvivEHR/confs\", job_name=\"desurv-mm-notebook\"):\n",
    "    cfg = compose(config_name=\"config_CompetingRisk11M\")\n",
    "\n",
    "dm = FoundationalDataModule(path_to_db=cfg.data.path_to_db,\n",
    "                            path_to_ds=\"/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_MultiMorbidity50+/\",\n",
    "                            overwrite_meta_information=cfg.data.meta_information_path,\n",
    "                            load=True)\n",
    "\n",
    "# Get the indicies for the diagnoses used to stratify patient groups (under the SurvivEHR setup)\n",
    "conditions = custom_mm_outcomes(dm)\n",
    "encoded_conditions = dm.tokenizer.encode(conditions)                    # The indicies of the MM events in the xsectional dataset (not adjusted for UNK/PAD/static data)\n",
    "\n",
    "# Get the number of baseline static variables (after one-hot encoding etc), and the vocab size excluding PAD and UNK tokens\n",
    "num_cov = dm.train_set[0][\"static_covariates\"].shape[0]\n",
    "num_context_tokens = dm.tokenizer._event_counts.shape[0] - 1            # Removing UNK token, which is not included in xsectional datasets\n",
    "\n",
    "# Convert the `encoded_conditions` indicies to the equivalent in the xsectional dataset\n",
    "encoded_conditions_xsec = [_ind + num_cov - 1 for _ind in encoded_conditions]\n",
    "print(encoded_conditions_xsec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example/test dataloader usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training dataset from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_MultiMorbidity50+/benchmark_data/N=20000_seed1.pickle\n",
      "(20000, 279)\n",
      "Loading validation/test datasets from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_MultiMorbidity50+/benchmark_data/all.pickle\n",
      "279\n"
     ]
    }
   ],
   "source": [
    "data_loader_train, data_loader_val, data_loader_test = get_dataloaders(\"MultiMorbidity50+\", True, sample_size=20000, seed=1)\n",
    "\n",
    "for batch in data_loader_test:\n",
    "    x_train = batch[0]\n",
    "    num_xsectional_in_dims = x_train[0].shape[0]\n",
    "    average_time_to_event = torch.mean(batch[1])\n",
    "    targets = batch[2]\n",
    "    break\n",
    "\n",
    "print(num_xsectional_in_dims)\n",
    "assert num_xsectional_in_dims == num_cov + num_context_tokens, f\"{num_xsectional_in_dims} != {num_cov} + {num_context_tokens}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error check: The last event in the dm tokenizer index should align to the new last event index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest_dm_tokenizer_ind = dm.tokenizer._event_counts.shape[0]\n",
    "# print(dm.tokenizer.decode([264]))\n",
    "assert largest_dm_tokenizer_ind + num_cov -1 == num_xsectional_in_dims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"MultiMorbidity50+\"\n",
    "competing_risk = False\n",
    "# sample_sizes = [int(np.exp(_log_n)) for _log_n in np.linspace(np.log(3000), np.log(500000), 10)]      # [3000, 12500, 30000, 60000, 100000]: # 600, 1200, \n",
    "sample_sizes = [20000] #283203, 2999, 5296, 9351, 16509, 29148, 51461, 90856, 160407, \n",
    "\n",
    "lr = 1e-3\n",
    "xdim = x_train.shape[1]\n",
    "\n",
    "# the time grid which we generate over\n",
    "t_eval = np.linspace(0, 1, 1000) \n",
    "# the time grid which we calculate scores over\n",
    "time_grid = np.linspace(start=0, stop=1 , num=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training dataset from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_MultiMorbidity50+/benchmark_data/N=20000_seed1.pickle\n",
      "(20000, 279)\n",
      "Loading validation/test datasets from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_MultiMorbidity50+/benchmark_data/all.pickle\n",
      "\n",
      "\n",
      "DeSurv-MultiMorbidity50+-Ns20000-seed1 with 10081 parameters\n",
      "Loaded previously trained model\n",
      "Testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 421/421 [04:53<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeSurv-MultiMorbidity50+-Ns20000-seed1:N=20000.       Ctd: 0.6011444192666519. IBS: 0.1533536157774033. INBLL: 0.4668451181069194\n",
      "Loading training dataset from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_MultiMorbidity50+/benchmark_data/N=20000_seed2.pickle\n",
      "(20000, 279)\n",
      "Loading validation/test datasets from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_MultiMorbidity50+/benchmark_data/all.pickle\n",
      "\n",
      "\n",
      "DeSurv-MultiMorbidity50+-Ns20000-seed2 with 10081 parameters\n",
      "Loaded previously trained model\n",
      "Testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 421/421 [05:20<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeSurv-MultiMorbidity50+-Ns20000-seed2:N=20000.       Ctd: 0.6013657435469636. IBS: 0.1534244307392567. INBLL: 0.467045335486115\n",
      "Loading training dataset from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_MultiMorbidity50+/benchmark_data/N=20000_seed3.pickle\n",
      "(20000, 279)\n",
      "Loading validation/test datasets from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_MultiMorbidity50+/benchmark_data/all.pickle\n",
      "\n",
      "\n",
      "DeSurv-MultiMorbidity50+-Ns20000-seed3 with 10081 parameters\n",
      "Loaded previously trained model\n",
      "Testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 421/421 [05:06<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeSurv-MultiMorbidity50+-Ns20000-seed3:N=20000.       Ctd: 0.6016623977315547. IBS: 0.1529709001483522. INBLL: 0.4661057071524355\n",
      "Loading training dataset from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_MultiMorbidity50+/benchmark_data/N=20000_seed4.pickle\n",
      "(20000, 279)\n",
      "Loading validation/test datasets from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_MultiMorbidity50+/benchmark_data/all.pickle\n",
      "\n",
      "\n",
      "DeSurv-MultiMorbidity50+-Ns20000-seed4 with 10081 parameters\n",
      "Loaded previously trained model\n",
      "Testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 421/421 [04:55<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeSurv-MultiMorbidity50+-Ns20000-seed4:N=20000.       Ctd: 0.5997379291750893. IBS: 0.15314308465268128. INBLL: 0.46653789613422\n",
      "Loading training dataset from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_MultiMorbidity50+/benchmark_data/N=20000_seed5.pickle\n",
      "(20000, 279)\n",
      "Loading validation/test datasets from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_MultiMorbidity50+/benchmark_data/all.pickle\n",
      "\n",
      "\n",
      "DeSurv-MultiMorbidity50+-Ns20000-seed5 with 10081 parameters\n",
      "Loaded previously trained model\n",
      "Testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 421/421 [05:08<00:00,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeSurv-MultiMorbidity50+-Ns20000-seed5:N=20000.       Ctd: 0.6028073757006384. IBS: 0.15275661753237382. INBLL: 0.46536915735690737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_names, all_ctd, all_ibs, all_inbll = [], [], [], []\n",
    "all_obs_RMST, all_pred_RMST = [], []\n",
    "\n",
    "for sample_size in sample_sizes:\n",
    "\n",
    "    seeds = [1,2,3,4,5]\n",
    "\n",
    "    for seed in seeds:\n",
    "        # Load dataset\n",
    "        data_loader_train, data_loader_val, data_loader_test = get_dataloaders(dataset, competing_risk, sample_size=sample_size, seed=seed)\n",
    "    \n",
    "        # Initialise model\n",
    "        model_name = f\"DeSurv-{dataset}-Ns{sample_size}-seed{seed}\"\n",
    "        if competing_risk is False:\n",
    "            model = ODESurvSingle(xdim, [32, 32], device=device)\n",
    "        else:\n",
    "            model = ODESurvMultiple(xdim, [32, 32], num_risks=5)\n",
    "        print(f\"\\n\\n{model_name} with {sum(p.numel() for p in model.parameters() if p.requires_grad)} parameters\")\n",
    "\n",
    "        # Load or train model\n",
    "        torch.manual_seed(seed)\n",
    "        try:\n",
    "            state_dict = torch.load(\"outputs/MM/\" + model_name + \"_tst_model\")\n",
    "            model.load_state_dict(state_dict)\n",
    "            print(f\"Loaded previously trained model\")\n",
    "        except:\n",
    "            print(f\"Training\")\n",
    "            model.optimize(data_loader_train, n_epochs=20, logging_freq=1, data_loader_val=data_loader_val, max_wait=2)\n",
    "            print(\"finished training\")\n",
    "            torch.save(model.state_dict(), \"outputs/MM/\" + model_name + \"_tst_model\")\n",
    "            model.eval()\n",
    "    \n",
    "            # state_dict = torch.load(\"outputs/MM/\" + model_name + \"_tst_model\")\n",
    "            # model.load_state_dict(state_dict)\n",
    "           \n",
    "        \n",
    "        # argsortttest = np.argsort(t_test)\n",
    "        # t_test = t_test[argsortttest]\n",
    "        # e_test = e_test[argsortttest]\n",
    "        # x_test = x_test[argsortttest,:]\n",
    "        \n",
    "        print(f\"Testing\")    \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "    \n",
    "            ctd = []\n",
    "            ibs = []\n",
    "            inbll = []\n",
    "            obs_RMST_by_number_of_preexisting_conditions = [[] for _ in range(len(encoded_conditions_xsec))]\n",
    "            pred_RMST_by_number_of_preexisting_conditions = [[] for _ in range(len(encoded_conditions_xsec))]\n",
    "            for batch in tqdm(data_loader_test, total=(len(data_loader_test)), desc=\"Testing\"):\n",
    "    \n",
    "                x_test = batch[0].numpy()\n",
    "                t_test = batch[1].numpy()\n",
    "                e_test = batch[2].numpy()\n",
    "    \n",
    "                # The normalised grid over which to predict\n",
    "                t_test_grid = torch.tensor(np.concatenate([t_eval] * x_test.shape[0], 0), dtype=torch.float32)\n",
    "                x_test_grid = torch.tensor(x_test, dtype=torch.float32).repeat_interleave(t_eval.size, 0)\n",
    "                \n",
    "                pred_bsz = 51200\n",
    "                pred = []\n",
    "                for x_test_batched, t_test_batched in zip(torch.split(x_test_grid, pred_bsz), torch.split(t_test_grid, pred_bsz)):\n",
    "                    \n",
    "                    if competing_risk is False:\n",
    "                        pred_ = model.predict(x_test_batched, t_test_batched)          # shape: (x_test.batched.shape[0],)\n",
    "                    else:\n",
    "                        pred_, pi_  = model.predict(x_test_batched, t_test_batched)    # shape: (x_test.batched.shape[0], num_outcomes)\n",
    "                    pred.append(pred_)\n",
    "                        \n",
    "                pred = torch.concat(pred)\n",
    "            \n",
    "                pred = pred.reshape((x_test.shape[0], t_eval.size, -1)).cpu().detach().numpy()\n",
    "                preds = [pred[:, :, _i] for _i in range(pred.shape[-1])]\n",
    "                # print([_.shape for _ in preds])\n",
    "        \n",
    "                # Merge (additively) each outcome risk curve into a single CDF, and update label for if outcome occurred or not\n",
    "                cdf = np.zeros_like(preds[0])\n",
    "                lbls = np.zeros_like(e_test)     \n",
    "                for _outcome_token in np.unique(e_test)[1:]:\n",
    "                    # print(f\"{_outcome_token} of {np.unique(e_test)[1:]} included from {len(preds)} surv CDFs\")\n",
    "                    # print(_outcome_token)\n",
    "                    cdf += preds[_outcome_token - 1] \n",
    "                    lbls += (e_test == _outcome_token)\n",
    "\n",
    "                ###########################\n",
    "                # Get RMST Survival times #\n",
    "                ###########################                \n",
    "                surv = 1 - cdf\n",
    "                for sample in range(surv.shape[0]):\n",
    "                    # Get the number of pre-existing conditions\n",
    "                    sample_stratification_label = np.sum(x_test[sample][encoded_conditions_xsec] == 1)\n",
    "                    # Get the RMST predicted under the survival curve\n",
    "                    sample_predicted_rmst = trapz(surv[sample,:], t_eval)\n",
    "                    pred_RMST_by_number_of_preexisting_conditions[sample_stratification_label].append(sample_predicted_rmst)\n",
    "                    \n",
    "                    if e_test[sample] != 0:\n",
    "                        # Get the observed RMST - warning: this is IGNORING CENSORING\n",
    "                        obs_RMST_by_number_of_preexisting_conditions[sample_stratification_label].append(t_test[sample])\n",
    "\n",
    "                ########################\n",
    "                # Get survival metrics #\n",
    "                ########################\n",
    "                surv = pd.DataFrame(np.transpose((1 - cdf.reshape((x_test.shape[0],t_eval.size)))), index=t_eval)                \n",
    "                ev = EvalSurv(surv, t_test, lbls, censor_surv='km')         # Evaluate surv curve with unscaled index with unscaled test times to event \n",
    "                # Log overall scores\n",
    "                ctd.append(ev.concordance_td())\n",
    "                ibs.append(ev.integrated_brier_score(time_grid))\n",
    "                inbll.append(ev.integrated_nbll(time_grid))\n",
    "            \n",
    "            ctd = np.mean(ctd)\n",
    "            ibs = np.mean(ibs)\n",
    "            inbll = np.mean(inbll)\n",
    "    \n",
    "            print(f\"{model_name}:\".ljust(20) + f\"N={sample_size}.\".ljust(15) + f\"Ctd: {ctd}. IBS: {ibs}. INBLL: {inbll}\")\n",
    "\n",
    "        model_names.append(model_name)\n",
    "        all_ctd.append(ctd)\n",
    "        all_ibs.append(ibs)\n",
    "        all_inbll.append(inbll)\n",
    "        all_pred_RMST.append(pred_RMST_by_number_of_preexisting_conditions)\n",
    "        all_obs_RMST.append(obs_RMST_by_number_of_preexisting_conditions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pred_RMST = [[np.mean(_i) if len(_i) > 0 else np.nan for _i in _pred_RMST] for _pred_RMST in all_pred_RMST]\n",
    "mean_obs_RMST = [[np.mean(_i) if len(_i) > 0 else np.nan for _i in _obs_RMST] for _obs_RMST in all_obs_RMST]\n",
    "\n",
    "num_pre_existing = np.arange(len(obs_RMST))\n",
    "\n",
    "plt.close()\n",
    "for _mean_pred_RMST in mean_pred_RMST:\n",
    "    plt.plot(num_pre_existing[:10], _mean_pred_RMST[:10], color='b')\n",
    "    \n",
    "plt.plot(num_pre_existing[:10], mean_obs_RMST[0][:10], color='k')   # these are evaluated on the `all` the test data - which is shared across subsampled datasets \n",
    "plt.xlabel(\"Number of pre-existing conditions\")\n",
    "plt.ylabel(\"Survival time\")\n",
    "plt.savefig(\"calibration_desurv.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model names: \n",
      "\t ['DeSurv-MultiMorbidity50+-Ns20000-seed1', 'DeSurv-MultiMorbidity50+-Ns20000-seed2', 'DeSurv-MultiMorbidity50+-Ns20000-seed3', 'DeSurv-MultiMorbidity50+-Ns20000-seed4', 'DeSurv-MultiMorbidity50+-Ns20000-seed5']\n",
      "\n",
      "Concordance (time-dependent): \n",
      "\t [0.6011444192666519, 0.6013657435469636, 0.6016623977315547, 0.5997379291750893, 0.6028073757006384]\n",
      "\n",
      "Integrated Brier Score: \n",
      "\t [0.1533536157774033, 0.1534244307392567, 0.1529709001483522, 0.15314308465268128, 0.15275661753237382]\n",
      "\n",
      "INBLL: \n",
      "\t [0.4668451181069194, 0.467045335486115, 0.4661057071524355, 0.46653789613422, 0.46536915735690737]\n",
      "\n",
      "Naive observed RMST: \n",
      "\t [0.81378, 0.7986956, 0.8275756, 0.81665397, 0.7681157, 0.67035997, 0.6242169, 0.56990874, 0.5747824, 0.49749595]\n",
      "\n",
      "Predicted RMST:\n",
      "\t [0.798844688330255, 0.7708229796585668, 0.7570225176336992, 0.7470901657899797, 0.7330107328216403, 0.7157286434126142, 0.7040444357894772, 0.7021473203879012, 0.7107821175735707, 0.6752714445342004]\n",
      "\t [0.7973564746561381, 0.773194057221414, 0.7613585044491132, 0.749241512834487, 0.7310259550719992, 0.7122158206672135, 0.6963973660426651, 0.6937813386860369, 0.6981394346541998, 0.6524348616189269]\n",
      "\t [0.7914300814762657, 0.767487999141849, 0.7545496279381216, 0.7426060204012568, 0.7236882413489153, 0.6979399799145496, 0.6751987821740187, 0.6611210912777561, 0.6519078477919439, 0.5902857017074655]\n",
      "\t [0.7913978328916471, 0.7671306689761851, 0.7522634449436449, 0.7374865400378049, 0.7159458919941637, 0.6838428314997237, 0.6609260309647536, 0.6426236402971583, 0.6336418736079498, 0.5787680368455619]\n",
      "\t [0.794093800440899, 0.7691643042114423, 0.7556932095813013, 0.7432164646093948, 0.7238974128283592, 0.7020813742799495, 0.6859360143470719, 0.6717467545535366, 0.6689243635378399, 0.6256275322165982]\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nModel names: \\n\\t {model_names}\")\n",
    "print(f\"\\nConcordance (time-dependent): \\n\\t {all_ctd}\")\n",
    "print(f\"\\nIntegrated Brier Score: \\n\\t {all_ibs}\")\n",
    "print(f\"\\nINBLL: \\n\\t {all_inbll}\")\n",
    "print(f\"\\nNaive observed RMST: \\n\\t {mean_obs_RMST[0][:10]}\")\n",
    "print(f\"\\nPredicted RMST:\")\n",
    "for _mean_pred_RMST in mean_pred_RMST:\n",
    "    print(f\"\\t {_mean_pred_RMST[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output across different setups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypertension Single Risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6013435730841796\n",
      "(0.6002630775641054, 0.6024240686042537, 0.0010804955200741837)\n",
      "0.15312972977001346\n",
      "(0.1528606396925774, 0.15339881984744952, 0.0002690900774360697)\n",
      "0.4663806428473194\n",
      "(0.4657267343591914, 0.46703455133544763, 0.0006539084881281537)\n"
     ]
    }
   ],
   "source": [
    "from statistics import NormalDist\n",
    "\n",
    "def confidence_interval(data, confidence=0.95):\n",
    "  dist = NormalDist.from_samples(data)\n",
    "  z = NormalDist().inv_cdf((1 + confidence) / 2.)\n",
    "  h = dist.stdev * z / ((len(data) - 1) ** .5)\n",
    "  return dist.mean - h, dist.mean + h, h\n",
    "\n",
    "data = [0.6011444192666519, 0.6013657435469636, 0.6016623977315547, 0.5997379291750893, 0.6028073757006384]\n",
    "print(np.mean(data))\n",
    "print(confidence_interval(data))\n",
    "\n",
    "data = [0.1533536157774033, 0.1534244307392567, 0.1529709001483522, 0.15314308465268128, 0.15275661753237382]\n",
    "print(np.mean(data))\n",
    "print(confidence_interval(data))\n",
    "\n",
    "data =  [0.4668451181069194, 0.467045335486115, 0.4661057071524355, 0.46653789613422, 0.46536915735690737]\n",
    "print(np.mean(data))\n",
    "print(confidence_interval(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "sys_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
