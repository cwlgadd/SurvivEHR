{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJpXpmjEYC_T"
   },
   "source": [
    "# Demo Notebook:\n",
    "## DeSurv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added path '/rds/homes/g/gaddcz/Projects/CPRD/virtual-envTorch2.0-icelake/lib/python3.10/site-packages' at start of search paths.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "node_type = os.getenv('BB_CPU')\n",
    "venv_dir = f'/rds/homes/g/gaddcz/Projects/CPRD/virtual-envTorch2.0-{node_type}'\n",
    "venv_site_pkgs = Path(venv_dir) / 'lib' / f'python{sys.version_info.major}.{sys.version_info.minor}' / 'site-packages'\n",
    "if venv_site_pkgs.exists():\n",
    "    sys.path.insert(0, str(venv_site_pkgs))\n",
    "    print(f\"Added path '{venv_site_pkgs}' at start of search paths.\")\n",
    "else:\n",
    "    print(f\"Path '{venv_site_pkgs}' not found. Check that it exists and/or that it exists for node-type '{node_type}'.\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from pathlib import Path\n",
    "# import sys\n",
    "\n",
    "# node_type = os.getenv('BB_CPU')\n",
    "# venv_dir = '/rds/projects/s/subramaa-mum-predict/CharlesGadd_Oxford/virtual_envs/SurvivEHRbenchmarking/'\n",
    "# venv_path = venv_dir + f'cpu-{node_type}'\n",
    "# venv_site_pkgs = Path(venv_path) / 'lib' / f'python{sys.version_info.major}.{sys.version_info.minor}' / 'site-packages'\n",
    "# if venv_site_pkgs.exists():\n",
    "#     sys.path.insert(0, str(venv_site_pkgs))\n",
    "#     print(f\"Added path '{venv_site_pkgs}' at start of search paths.\")\n",
    "# else:\n",
    "#     print(f\"Path '{venv_site_pkgs}' not found. Check that it exists and/or that it exists for node-type '{node_type}'.\")\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda.\n"
     ]
    }
   ],
   "source": [
    "# import pytorch_lightning\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sqlite3\n",
    "from dataclasses import dataclass\n",
    "import logging\n",
    "from FastEHR.dataloader import FoundationalDataModule\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pycox.datasets import support\n",
    "from pycox.evaluation import EvalSurv\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from CPRD.src.modules.head_layers.survival.desurv import ODESurvSingle\n",
    "from CPRD.src.modules.head_layers.survival.desurv import ODESurvMultiple\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = \"cpu\"    # if more informative debugging statements are needed\n",
    "print(f\"Using device: {device}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(dataset, competing_risk, sample_size=None, seed=None):\n",
    "\n",
    "    match dataset.lower():\n",
    "        case \"pycox\":\n",
    "            df_train = support.read_df()\n",
    "            df_test = df_train.sample(frac=0.2)\n",
    "            df_train = df_train.drop(df_test.index)\n",
    "            df_val = df_train.sample(frac=0.2)\n",
    "            df_train = df_train.drop(df_val.index)\n",
    "            \n",
    "            cols_standardize = ['x0', 'x7', 'x8', 'x9', 'x10', 'x11', 'x12', 'x13']\n",
    "            cols_leave = ['x1', 'x2', 'x3', 'x4', 'x5', 'x6']\n",
    "            \n",
    "            standardize = [([col], StandardScaler()) for col in cols_standardize]\n",
    "            leave = [(col, None) for col in cols_leave]\n",
    "            \n",
    "            x_mapper = DataFrameMapper(standardize + leave)\n",
    "            \n",
    "            x_train = x_mapper.fit_transform(df_train).astype('float32')\n",
    "            x_val = x_mapper.transform(df_val).astype('float32')\n",
    "            x_test = x_mapper.transform(df_test).astype('float32')\n",
    "            \n",
    "            get_target = lambda df: (df['duration'].values, df['event'].values)\n",
    "            y_train = get_target(df_train)\n",
    "            y_val = get_target(df_val)\n",
    "            y_test = get_target(df_test)\n",
    "            \n",
    "            t_train, e_train = y_train\n",
    "            t_val, e_val = y_val\n",
    "            t_test, e_test = y_test\n",
    "            \n",
    "            t_train_max = np.amax(t_train)\n",
    "            t_train = t_train / t_train_max\n",
    "            t_val = t_val / t_train_max\n",
    "            t_test = t_test / t_train_max\n",
    "            \n",
    "    \n",
    "        case \"hypertension\" | \"cvd\":\n",
    "    \n",
    "            # Training samples\n",
    "            if sample_size is not None:\n",
    "                save_path =  f\"/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_{dataset}/\" + f\"benchmark_data/N={sample_size}_seed{seed}.pickle\" \n",
    "            else:\n",
    "                save_path = f\"/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_{dataset}/\" + \"benchmark_data/all.pickle\"\n",
    "                \n",
    "            with open(save_path, \"rb\") as handle:\n",
    "                print(f\"Loading training dataset from {save_path}\")\n",
    "                data_train = pickle.load(handle)\n",
    "                print(f\"{data_train['X_train'].shape}\")\n",
    "            \n",
    "            # display(data[\"X_train\"].head())\n",
    "            # display(data[\"y_train\"])\n",
    "            # print(data.keys())\n",
    "            \n",
    "            data = {}\n",
    "            data[\"X_train\"] = data_train[\"X_train\"]\n",
    "            data[\"y_train\"] = data_train[\"y_train\"]\n",
    "    \n",
    "            # Test and validation samples\n",
    "    \n",
    "            save_path = f\"/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_{dataset}/\" + \"benchmark_data/all.pickle\"\n",
    "            with open(save_path, \"rb\") as handle:\n",
    "                print(f\"Loading validation/test datasets from {save_path}\")\n",
    "                data_val_test = pickle.load(handle)\n",
    "                \n",
    "            data[\"X_val\"] = data_val_test[\"X_val\"]\n",
    "            data[\"y_val\"] = data_val_test[\"y_val\"]\n",
    "            data[\"X_test\"] = data_val_test[\"X_test\"]\n",
    "            data[\"y_test\"] = data_val_test[\"y_test\"]\n",
    "    \n",
    "            # Convert to correct formats\n",
    "            x_train = data[\"X_train\"].to_numpy(dtype=np.float32)\n",
    "            x_val = data[\"X_val\"].to_numpy(dtype=np.float32)\n",
    "            x_test = data[\"X_test\"].to_numpy(dtype=np.float32)\n",
    "            \n",
    "            t_train = np.asarray([i[1] for i in data[\"y_train\"]])\n",
    "            t_val = np.asarray([i[1] for i in data[\"y_val\"]])        \n",
    "            t_test = np.asarray([i[1] for i in data[\"y_test\"]])\n",
    "    \n",
    "            if competing_risk is False:\n",
    "                e_train = np.asarray([0 if i[0] == 0 else 1 for i in data[\"y_train\"]])\n",
    "                e_val = np.asarray([0 if i[0] == 0 else 1 for i in data[\"y_val\"]])\n",
    "                e_test = np.asarray([0 if i[0] == 0 else 1 for i in data[\"y_test\"]])\n",
    "            else:\n",
    "                e_train = np.asarray([i[0] for i in data[\"y_train\"]])\n",
    "                e_val = np.asarray([i[0] for i in data[\"y_val\"]])\n",
    "                e_test = np.asarray([i[0] for i in data[\"y_test\"]])\n",
    "\n",
    "    # display(x_train.shape)\n",
    "    # display(type(x_train))\n",
    "    # display(type(x_train[0,0]))\n",
    "    # display(e_train.shape)\n",
    "    # display(type(e_train))\n",
    "    # display(type(e_train[0]))\n",
    "    # display(t_train.shape)\n",
    "    # display(type(t_train))\n",
    "    # display(type(t_train[0]))\n",
    "    # print(np.mean(e_train))\n",
    "    # print(np.mean(t_train))\n",
    "    # print(np.std(t_train))\n",
    "    # print(np.mean(x_train))\n",
    "    # print(t_train.min())\n",
    "    # print(t_train.max())\n",
    "    # print(np.unique(e_test, return_counts=True))\n",
    "    \n",
    "    \n",
    "    batch_size = 256\n",
    "    dataset_train = TensorDataset(*[torch.tensor(u,dtype=dtype_) for u, dtype_ in [(x_train,torch.float32),\n",
    "                                                                                   (t_train,torch.float32),\n",
    "                                                                                   (e_train,torch.long)]])\n",
    "    data_loader_train = DataLoader(dataset_train, batch_size=batch_size, pin_memory=True, shuffle=True, drop_last=True)\n",
    "    \n",
    "    dataset_val = TensorDataset(*[torch.tensor(u,dtype=dtype_) for u, dtype_ in [(x_val,torch.float32),\n",
    "                                                                                   (t_val,torch.float32),\n",
    "                                                                                   (e_val,torch.long)]])\n",
    "    data_loader_val = DataLoader(dataset_val, batch_size=batch_size, pin_memory=True, shuffle=True)\n",
    "    \n",
    "    dataset_test = TensorDataset(*[torch.tensor(u,dtype=dtype_) for u, dtype_ in [(x_test,torch.float32),\n",
    "                                                                                   (t_test,torch.float32),\n",
    "                                                                                   (e_test,torch.long)]])\n",
    "    data_loader_test = DataLoader(dataset_test, batch_size=batch_size, pin_memory=True, shuffle=True)\n",
    "\n",
    "    return data_loader_train, data_loader_val, data_loader_test\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example dataloader function usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training dataset from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_Hypertension/benchmark_data/all.pickle\n",
      "(572096, 279)\n",
      "Loading validation/test datasets from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_Hypertension/benchmark_data/all.pickle\n"
     ]
    }
   ],
   "source": [
    "data_loader_train, data_loader_val, data_loader_test = get_dataloaders(\"Hypertension\", False, sample_size=None, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 279])\n",
      "tensor(1.4163)\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
      "        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "for batch in data_loader_test:\n",
    "    x_train = batch[0]\n",
    "    print(batch[0].shape)\n",
    "    print(torch.mean(batch[1]))\n",
    "    print(batch[2])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model\n",
    "```\n",
    "Hypertension\n",
    "\n",
    "SR\n",
    "[0.768850848388541]\n",
    "[0.08287342061953376]\n",
    "[0.2629869386534056]\n",
    "\n",
    "CVD\n",
    "\n",
    "SR\n",
    "[0.6660374450461991]\n",
    "[0.03355264167471281]\n",
    "[0.14207857084831382]\n",
    "\n",
    "CR\n",
    "[0.6622136902603167]\n",
    "[0.033540062894645686]\n",
    "[0.142287892974895]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"Hypertension\" \n",
    "competing_risk = False\n",
    "# sample_sizes = [int(np.exp(_log_n)) for _log_n in np.linspace(np.log(3000), np.log(500000), 10)]      # [3000, 12500, 30000, 60000, 100000]: # 600, 1200, \n",
    "sample_sizes = [2999, 5296, 9351, 16509, 29148, 51461, 90856, 160407, 283203, 500000, None]\n",
    "sample_sizes = [None]\n",
    "# sample_sizes = [500000]\n",
    "# sample_sizes = sample_sizes[-1:]\n",
    "\n",
    "# training = True\n",
    "lr = 1e-3\n",
    "xdim = x_train.shape[1]\n",
    "\n",
    "\n",
    "# the time grid which we generate over\n",
    "t_eval = np.linspace(0, 1, 1000) \n",
    "# the time grid which we calculate scores over\n",
    "time_grid = np.linspace(start=0, stop=1 , num=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training dataset from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_Hypertension/benchmark_data/all.pickle\n",
      "(572096, 279)\n",
      "Loading validation/test datasets from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_Hypertension/benchmark_data/all.pickle\n",
      "\n",
      "\n",
      "DeSurv-Hypertension-NsNone-seed1 with 10081 parameters\n",
      "Training\n",
      "\tEpoch:  0. Total loss:   262342.64\n",
      "best_epoch: 0\n",
      "\tEpoch:  0. Total val loss:    14993.43\n",
      "\tEpoch:  1. Total loss:   247637.81\n",
      "best_epoch: 1\n",
      "\tEpoch:  1. Total val loss:    14772.03\n",
      "\tEpoch:  2. Total loss:   245857.33\n",
      "\tEpoch:  2. Total val loss:    14809.54\n",
      "\tEpoch:  3. Total loss:   244750.78\n",
      "best_epoch: 3\n",
      "\tEpoch:  3. Total val loss:    14764.87\n",
      "\tEpoch:  4. Total loss:   243993.23\n",
      "best_epoch: 4\n",
      "\tEpoch:  4. Total val loss:    14751.96\n",
      "\tEpoch:  5. Total loss:   243276.68\n",
      "best_epoch: 5\n",
      "\tEpoch:  5. Total val loss:    14706.59\n",
      "\tEpoch:  6. Total loss:   242696.47\n",
      "\tEpoch:  6. Total val loss:    14747.26\n",
      "\tEpoch:  7. Total loss:   242295.63\n",
      "\tEpoch:  7. Total val loss:    14729.99\n",
      "\tEpoch:  8. Total loss:   241866.22\n",
      "finished training\n",
      "Testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 140/140 [00:14<00:00,  9.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeSurv-Hypertension-NsNone-seed1:N=None.        Ctd: 0.7708035555642572. IBS: 0.08267526462773596. INBLL: 0.2625306123346124\n",
      "Loading training dataset from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_Hypertension/benchmark_data/all.pickle\n",
      "(572096, 279)\n",
      "Loading validation/test datasets from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_Hypertension/benchmark_data/all.pickle\n",
      "\n",
      "\n",
      "DeSurv-Hypertension-NsNone-seed2 with 10081 parameters\n",
      "Training\n",
      "\tEpoch:  0. Total loss:   264250.41\n",
      "best_epoch: 0\n",
      "\tEpoch:  0. Total val loss:    14995.57\n",
      "\tEpoch:  1. Total loss:   249065.96\n",
      "best_epoch: 1\n",
      "\tEpoch:  1. Total val loss:    14806.92\n",
      "\tEpoch:  2. Total loss:   246848.68\n",
      "\tEpoch:  2. Total val loss:    14929.38\n",
      "\tEpoch:  3. Total loss:   245412.42\n",
      "best_epoch: 3\n",
      "\tEpoch:  3. Total val loss:    14699.10\n",
      "\tEpoch:  4. Total loss:   244466.25\n",
      "\tEpoch:  4. Total val loss:    14812.86\n",
      "\tEpoch:  5. Total loss:   243831.36\n",
      "\tEpoch:  5. Total val loss:    14706.39\n",
      "\tEpoch:  6. Total loss:   243325.18\n",
      "finished training\n",
      "Testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 140/140 [00:14<00:00,  9.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeSurv-Hypertension-NsNone-seed2:N=None.        Ctd: 0.7695751200897563. IBS: 0.08273968181756607. INBLL: 0.262755230788484\n",
      "Loading training dataset from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_Hypertension/benchmark_data/all.pickle\n",
      "(572096, 279)\n",
      "Loading validation/test datasets from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_Hypertension/benchmark_data/all.pickle\n",
      "\n",
      "\n",
      "DeSurv-Hypertension-NsNone-seed3 with 10081 parameters\n",
      "Training\n",
      "\tEpoch:  0. Total loss:   263525.15\n",
      "best_epoch: 0\n",
      "\tEpoch:  0. Total val loss:    15057.56\n",
      "\tEpoch:  1. Total loss:   248587.60\n",
      "best_epoch: 1\n",
      "\tEpoch:  1. Total val loss:    14861.83\n",
      "\tEpoch:  2. Total loss:   246639.46\n",
      "best_epoch: 2\n",
      "\tEpoch:  2. Total val loss:    14745.99\n",
      "\tEpoch:  3. Total loss:   245216.52\n",
      "\tEpoch:  3. Total val loss:    14906.11\n",
      "\tEpoch:  4. Total loss:   244196.80\n",
      "best_epoch: 4\n",
      "\tEpoch:  4. Total val loss:    14715.75\n",
      "\tEpoch:  5. Total loss:   243366.60\n",
      "\tEpoch:  5. Total val loss:    14817.76\n",
      "\tEpoch:  6. Total loss:   242818.69\n",
      "\tEpoch:  6. Total val loss:    14751.50\n",
      "\tEpoch:  7. Total loss:   242379.21\n",
      "finished training\n",
      "Testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 140/140 [00:14<00:00,  9.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeSurv-Hypertension-NsNone-seed3:N=None.        Ctd: 0.7724139601735452. IBS: 0.08270569036836782. INBLL: 0.2625610527660121\n",
      "Loading training dataset from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_Hypertension/benchmark_data/all.pickle\n",
      "(572096, 279)\n",
      "Loading validation/test datasets from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_Hypertension/benchmark_data/all.pickle\n",
      "\n",
      "\n",
      "DeSurv-Hypertension-NsNone-seed4 with 10081 parameters\n",
      "Training\n",
      "\tEpoch:  0. Total loss:   265191.16\n",
      "best_epoch: 0\n",
      "\tEpoch:  0. Total val loss:    15200.15\n",
      "\tEpoch:  1. Total loss:   249490.30\n",
      "best_epoch: 1\n",
      "\tEpoch:  1. Total val loss:    14855.25\n",
      "\tEpoch:  2. Total loss:   246943.33\n",
      "best_epoch: 2\n",
      "\tEpoch:  2. Total val loss:    14765.10\n",
      "\tEpoch:  3. Total loss:   245495.67\n",
      "best_epoch: 3\n",
      "\tEpoch:  3. Total val loss:    14728.11\n",
      "\tEpoch:  4. Total loss:   244464.41\n",
      "best_epoch: 4\n",
      "\tEpoch:  4. Total val loss:    14718.48\n",
      "\tEpoch:  5. Total loss:   243684.61\n",
      "\tEpoch:  5. Total val loss:    14782.91\n",
      "\tEpoch:  6. Total loss:   243141.17\n",
      "\tEpoch:  6. Total val loss:    14799.96\n",
      "\tEpoch:  7. Total loss:   242588.22\n",
      "finished training\n",
      "Testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 140/140 [00:14<00:00,  9.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeSurv-Hypertension-NsNone-seed4:N=None.        Ctd: 0.7729993169568111. IBS: 0.08248514899737762. INBLL: 0.2621711782943788\n",
      "Loading training dataset from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_Hypertension/benchmark_data/all.pickle\n",
      "(572096, 279)\n",
      "Loading validation/test datasets from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_Hypertension/benchmark_data/all.pickle\n",
      "\n",
      "\n",
      "DeSurv-Hypertension-NsNone-seed5 with 10081 parameters\n",
      "Training\n",
      "\tEpoch:  0. Total loss:   266318.23\n",
      "best_epoch: 0\n",
      "\tEpoch:  0. Total val loss:    14920.48\n",
      "\tEpoch:  1. Total loss:   247918.10\n",
      "best_epoch: 1\n",
      "\tEpoch:  1. Total val loss:    14816.83\n",
      "\tEpoch:  2. Total loss:   245815.22\n",
      "best_epoch: 2\n",
      "\tEpoch:  2. Total val loss:    14786.59\n",
      "\tEpoch:  3. Total loss:   244699.37\n",
      "best_epoch: 3\n",
      "\tEpoch:  3. Total val loss:    14723.82\n",
      "\tEpoch:  4. Total loss:   243835.01\n",
      "best_epoch: 4\n",
      "\tEpoch:  4. Total val loss:    14681.30\n",
      "\tEpoch:  5. Total loss:   243240.67\n",
      "\tEpoch:  5. Total val loss:    14835.21\n",
      "\tEpoch:  6. Total loss:   242698.33\n",
      "\tEpoch:  6. Total val loss:    14708.85\n"
     ]
    }
   ],
   "source": [
    "model_names, all_ctd, all_ibs, all_inbll = [], [], [], []\n",
    "\n",
    "for sample_size in sample_sizes:\n",
    "\n",
    "    seeds = [1,2,3,4,5]\n",
    "\n",
    "    for seed in seeds:\n",
    "        # Load dataset\n",
    "        data_loader_train, data_loader_val, data_loader_test = get_dataloaders(dataset, competing_risk, sample_size=sample_size, seed=seed)\n",
    "    \n",
    "        # Initialise model\n",
    "        model_name = f\"DeSurv-{dataset}-Ns{sample_size}-seed{seed}\"\n",
    "        if competing_risk is False:\n",
    "            model = ODESurvSingle(xdim, [32, 32], device=device)\n",
    "        else:\n",
    "            model = ODESurvMultiple(xdim, [32, 32], num_risks=5)\n",
    "        print(f\"\\n\\n{model_name} with {sum(p.numel() for p in model.parameters() if p.requires_grad)} parameters\")\n",
    "\n",
    "        # Load or train model\n",
    "        torch.manual_seed(seed)\n",
    "        try:\n",
    "            state_dict = torch.load(model_name + \"tst_model\")\n",
    "            model.load_state_dict(state_dict)\n",
    "            print(f\"Loaded previously trained model\")\n",
    "        except:\n",
    "            print(f\"Training\")\n",
    "            model.optimize(data_loader_train, n_epochs=20, logging_freq=1, data_loader_val=data_loader_val, max_wait=2)\n",
    "            print(\"finished training\")\n",
    "            torch.save(model.state_dict(), model_name + \"tst_model\")\n",
    "            model.eval()\n",
    "    \n",
    "            # state_dict = torch.load(model_name + \"tst_model\")\n",
    "            # model.load_state_dict(state_dict)\n",
    "           \n",
    "        \n",
    "        # argsortttest = np.argsort(t_test)\n",
    "        # t_test = t_test[argsortttest]\n",
    "        # e_test = e_test[argsortttest]\n",
    "        # x_test = x_test[argsortttest,:]\n",
    "        \n",
    "        print(f\"Testing\")    \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "    \n",
    "            ctd = []\n",
    "            ibs = []\n",
    "            inbll = []\n",
    "            for batch in tqdm(data_loader_test, total=(len(data_loader_test)), desc=\"Testing\"):\n",
    "    \n",
    "                x_test = batch[0].numpy()\n",
    "                t_test = batch[1].numpy()\n",
    "                e_test = batch[2].numpy()\n",
    "    \n",
    "                # The normalised grid over which to predict\n",
    "                t_test_grid = torch.tensor(np.concatenate([t_eval] * x_test.shape[0], 0), dtype=torch.float32)\n",
    "                x_test_grid = torch.tensor(x_test, dtype=torch.float32).repeat_interleave(t_eval.size, 0)\n",
    "                \n",
    "                pred_bsz = 51200\n",
    "                pred = []\n",
    "                for x_test_batched, t_test_batched in zip(torch.split(x_test_grid, pred_bsz), torch.split(t_test_grid, pred_bsz)):\n",
    "                    \n",
    "                    if competing_risk is False:\n",
    "                        pred_ = model.predict(x_test_batched, t_test_batched)          # shape: (x_test.batched.shape[0],)\n",
    "                    else:\n",
    "                        pred_, pi_  = model.predict(x_test_batched, t_test_batched)    # shape: (x_test.batched.shape[0], num_outcomes)\n",
    "                    pred.append(pred_)\n",
    "                        \n",
    "                pred = torch.concat(pred)\n",
    "            \n",
    "                pred = pred.reshape((x_test.shape[0], t_eval.size, -1)).cpu().detach().numpy()\n",
    "                preds = [pred[:, :, _i] for _i in range(pred.shape[-1])]\n",
    "                # print([_.shape for _ in preds])\n",
    "        \n",
    "                # Merge (additively) each outcome risk curve into a single CDF, and update label for if outcome occurred or not\n",
    "                cdf = np.zeros_like(preds[0])\n",
    "                lbls = np.zeros_like(e_test)     \n",
    "                for _outcome_token in np.unique(e_test)[1:]:\n",
    "                    # print(f\"{_outcome_token} of {np.unique(e_test)[1:]} included from {len(preds)} surv CDFs\")\n",
    "                    # print(_outcome_token)\n",
    "                    cdf += preds[_outcome_token - 1] \n",
    "                    lbls += (e_test == _outcome_token)\n",
    "                \n",
    "                surv = pd.DataFrame(np.transpose((1 - cdf.reshape((x_test.shape[0],t_eval.size)))), index=t_eval)\n",
    "                # print(surv)\n",
    "    \n",
    "                # Evaluate surv curve with unscaled index with unscaled test times to event \n",
    "                ev = EvalSurv(surv, t_test, lbls, censor_surv='km')\n",
    "                # try:\n",
    "                    # Same treatment as in SurvivEHR\n",
    "                ctd.append(ev.concordance_td())\n",
    "                ibs.append(ev.integrated_brier_score(time_grid))\n",
    "                inbll.append(ev.integrated_nbll(time_grid))\n",
    "                # print(f\"{ctd} {ibs}, {inbll}\")\n",
    "                # except:\n",
    "                #     pass\n",
    "                \n",
    "            ctd = np.mean(ctd)\n",
    "            ibs = np.mean(ibs)\n",
    "            inbll = np.mean(inbll)\n",
    "    \n",
    "            print(f\"{model_name}:\".ljust(20) + f\"N={sample_size}.\".ljust(15) + f\"Ctd: {ctd}. IBS: {ibs}. INBLL: {inbll}\")\n",
    "\n",
    "        model_names.append(model_name)\n",
    "        all_ctd.append(ctd)\n",
    "        all_ibs.append(ibs)\n",
    "        all_inbll.append(inbll)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_names)\n",
    "print(all_ctd)\n",
    "print(all_ibs)\n",
    "print(all_inbll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(surv.head())\n",
    "print(surv.shape)\n",
    "print(preds[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_sizes[-1] = 572096\n",
    "print(sample_sizes)\n",
    "print(all_ctd)\n",
    "print(all_ibs)\n",
    "print(all_inbll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output across different setups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypertension Single Risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypertension Competing Risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypertension_cr:N=2999.        Ctd: 0.6815094745489083. IBS: 0.09055914251601428. INBLL: 0.31029164842950396\n",
    "# Hypertension_cr:N=5296.        Ctd: 0.7124953181062986. IBS: 0.08950243267368668. INBLL: 0.2998138831024105\n",
    "# Hypertension_cr:N=9351.        Ctd: 0.7274689831170883. IBS: 0.08788381776138605. INBLL: 0.29219335002924224\n",
    "# Hypertension_cr:N=16509.       Ctd: 0.730682386913791. IBS: 0.08713170539614512. INBLL: 0.2915679214433027\n",
    "# Hypertension_cr:N=29148.       Ctd: 0.7389774246331557. IBS: 0.08589961516053798. INBLL: 0.2797340688171351\n",
    "# Hypertension_cr:N=51461.       Ctd: 0.7411341306621856. IBS: 0.08509218669879787. INBLL: 0.27232015638634294\n",
    "# Hypertension_cr:N=90856.       Ctd: 0.7515646895266107. IBS: 0.08443103008855017. INBLL: 0.27128357563201616\n",
    "# Hypertension_cr:N=160407.      Ctd: 0.7600540925292503. IBS: 0.08451878180772691. INBLL: 0.2692630746813842\n",
    "# Hypertension_cr:N=283203.      Ctd: 0.7620305063798369. IBS: 0.08404512042043363. INBLL: 0.2666228576617026\n",
    "# Hypertension_cr:N=500000.      Ctd: 0.761360566628242. IBS: 0.08371696663855675. INBLL: 0.26584980102348854"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cardiovascular disease Single Risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cardiovascular disease Competing Risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CVD_cr:          N=2999.        Ctd: 0.5625189849883911. IBS: 0.03435665860871859. INBLL: 0.14990771301641295\n",
    "# CVD_cr:          N=5296.        Ctd: 0.5869489534497859. IBS: 0.03411933684032459. INBLL: 0.14823684700350284\n",
    "# CVD_cr:          N=9351.        Ctd: 0.5993921885610577. IBS: 0.03442101009915632. INBLL: 0.14955773914208445\n",
    "# CVD_cr:          N=16509.       Ctd: 0.6101382510977712. IBS: 0.03387871579676817. INBLL: 0.14660058710195034\n",
    "# CVD_cr:          N=29148.       Ctd: 0.6200633894187596. IBS: 0.033806085396407184. INBLL: 0.1458525721893707\n",
    "# CVD_cr:          N=51461.       Ctd: 0.624818164536866. IBS: 0.033786555455045275. INBLL: 0.14535719048483275\n",
    "# CVD_cr:          N=90856.       Ctd: 0.6355840601226502. IBS: 0.03364728847608413. INBLL: 0.14414393636167722\n",
    "# CVD_cr:          N=160407.      Ctd: 0.6469820170508909. IBS: 0.033556625283347644. INBLL: 0.14308219809140393\n",
    "# CVD_cr:          N=283203.      Ctd: 0.6557505764681258. IBS: 0.033524955828030355. INBLL: 0.1423563785027365\n",
    "# CVD_cr:          N=500000.      Ctd: 0.6582288879213832. IBS: 0.033605688713984706. INBLL: 0.14273257727425628\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "sys_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
