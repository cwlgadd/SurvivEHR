{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJpXpmjEYC_T"
   },
   "source": [
    "# Testing the pre-trained NN classifier for next event prediction\n",
    "\n",
    "This model uses a cross-sectional input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added path '/rds/homes/g/gaddcz/Projects/CPRD/virtual-envTorch2.0-icelake/lib/python3.10/site-packages' at start of search paths.\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "env: SLURM_NTASKS_PER_NODE=28\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "node_type = os.getenv('BB_CPU')\n",
    "venv_dir = f'/rds/homes/g/gaddcz/Projects/CPRD/virtual-envTorch2.0-{node_type}'\n",
    "venv_site_pkgs = Path(venv_dir) / 'lib' / f'python{sys.version_info.major}.{sys.version_info.minor}' / 'site-packages'\n",
    "if venv_site_pkgs.exists():\n",
    "    sys.path.insert(0, str(venv_site_pkgs))\n",
    "    print(f\"Added path '{venv_site_pkgs}' at start of search paths.\")\n",
    "else:\n",
    "    print(f\"Path '{venv_site_pkgs}' not found. Check that it exists and/or that it exists for node-type '{node_type}'.\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%env SLURM_NTASKS_PER_NODE=28   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda.\n",
      "env: SLURM_NTASKS_PER_NODE=28\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import logging\n",
    "from hydra import compose, initialize\n",
    "\n",
    "from setup_causal_mlp_experiment import CausalMLPExperiment, setup_mlp_experiment\n",
    "from setup_causal_t_mlp_experiment import CausalTMLPExperiment, setup_t_mlp_experiment\n",
    "from CPRD.examples.modelling.benchmarks.Pretrain.run_experiment import run\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = \"cpu\"    # if more informative debugging statements are needed\n",
    "print(f\"Using device: {device}.\")\n",
    "\n",
    "%env SLURM_NTASKS_PER_NODE=28   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and run pre-trained experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Running cr on 72 CPUs and 1 GPUs\n",
      "INFO:root:====================================================================================================\n",
      "INFO:root:# Loading DataModule for dataset /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/. This will be loaded in causal form.\n",
      "INFO:root:====================================================================================================\n",
      "INFO:root:Creating unsupervised collator for DataModule\n",
      "INFO:root:Using meta information from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/meta_information_QuantJenny.pickle\n",
      "INFO:root:Using train file-row count dictionary from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/file_row_count_dict_train.pickle\n",
      "INFO:root:Using test file-row count dictionary from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/file_row_count_dict_test.pickle\n",
      "INFO:root:Using val file-row count dictionary from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/file_row_count_dict_val.pickle\n",
      "INFO:root:Tokenzier created based on 7,555,415,275 tokens\n",
      "INFO:root:Using tabular tokenizer, created from meta information and containing 265 tokens\n",
      "INFO:root:Set seed to 1337\n",
      "INFO:root:Loaded /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/split=train/ dataset, with 23,613,894 samples\n",
      "INFO:root:Set seed to 42\n",
      "INFO:root:Loaded /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/split=test/ dataset, with 1,508,320 samples\n",
      "INFO:root:Set seed to 42\n",
      "INFO:root:Loaded /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/split=val/ dataset, with 1,426,714 samples\n",
      "INFO:root:is_decoder: true\n",
      "data:\n",
      "  batch_size: 64\n",
      "  unk_freq_threshold: 0.0\n",
      "  min_workers: 12\n",
      "  global_diagnoses: false\n",
      "  repeating_events: true\n",
      "  path_to_db: /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/cprd.db\n",
      "  path_to_ds: /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/\n",
      "  meta_information_path: /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/meta_information_QuantJenny.pickle\n",
      "  subsample_training: null\n",
      "experiment:\n",
      "  type: pre-train\n",
      "  project_name: SurvivEHR\n",
      "  run_id: CrossSectionalMLP\n",
      "  fine_tune_id: null\n",
      "  notes: null\n",
      "  tags: null\n",
      "  train: false\n",
      "  test: true\n",
      "  verbose: true\n",
      "  seed: 1337\n",
      "  log: false\n",
      "  log_dir: /rds/projects/s/subramaa-mum-predict/CharlesGadd_Oxford/FoundationModelOutput/\n",
      "  ckpt_dir: /rds/projects/s/subramaa-mum-predict/CharlesGadd_Oxford/FoundationModelOutput/checkpoints/\n",
      "fine_tuning:\n",
      "  fine_tune_outcomes: null\n",
      "  custom_outcome_method:\n",
      "    _target_: null\n",
      "  custom_stratification_method:\n",
      "    _target_: null\n",
      "  use_callbacks:\n",
      "    hidden_embedding: true\n",
      "    performance_metrics: true\n",
      "    rmst: false\n",
      "  head:\n",
      "    surv_weight: 1\n",
      "    value_weight: 0\n",
      "    learning_rate: 0.0005\n",
      "optim:\n",
      "  num_epochs: 1\n",
      "  learning_rate: 0.0003\n",
      "  scheduler_warmup: true\n",
      "  scheduler: decaycawarmrestarts\n",
      "  scheduler_periods: 10000\n",
      "  learning_rate_decay: 0.8\n",
      "  val_check_interval: 2500\n",
      "  early_stop: true\n",
      "  early_stop_patience: 30\n",
      "  log_every_n_steps: 20\n",
      "  limit_val_batches: 0.025\n",
      "  limit_test_batches: null\n",
      "  accumulate_grad_batches: 1\n",
      "transformer:\n",
      "  block_type: Neo\n",
      "  block_size: 256\n",
      "  n_layer: 6\n",
      "  n_head: 6\n",
      "  n_embd: 384\n",
      "  layer_norm_bias: false\n",
      "  attention_type: global\n",
      "  bias: true\n",
      "  dropout: 0.0\n",
      "  attention_dropout: 0.0\n",
      "  resid_dropout: 0.0\n",
      "  private_heads: 0\n",
      "  use_fine_tune_adapter: false\n",
      "  adapter_dim: 8\n",
      "head:\n",
      "  SurvLayer: cr\n",
      "  surv_weight: 1\n",
      "  tokens_for_univariate_regression:\n",
      "  - 84\n",
      "  - 86\n",
      "  - 107\n",
      "  - 154\n",
      "  - 42\n",
      "  - 231\n",
      "  - 63\n",
      "  - 85\n",
      "  - 247\n",
      "  - 51\n",
      "  - 103\n",
      "  - 146\n",
      "  - 66\n",
      "  - 172\n",
      "  - 152\n",
      "  - 261\n",
      "  - 235\n",
      "  - 193\n",
      "  - 184\n",
      "  - 101\n",
      "  - 223\n",
      "  - 135\n",
      "  - 226\n",
      "  - 202\n",
      "  - 145\n",
      "  - 244\n",
      "  - 165\n",
      "  - 64\n",
      "  - 185\n",
      "  - 238\n",
      "  - 220\n",
      "  - 232\n",
      "  - 240\n",
      "  - 237\n",
      "  - 17\n",
      "  - 239\n",
      "  - 217\n",
      "  - 118\n",
      "  - 219\n",
      "  - 251\n",
      "  - 25\n",
      "  - 150\n",
      "  - 111\n",
      "  - 102\n",
      "  - 15\n",
      "  - 110\n",
      "  - 130\n",
      "  - 128\n",
      "  - 122\n",
      "  - 99\n",
      "  - 77\n",
      "  - 96\n",
      "  - 140\n",
      "  - 50\n",
      "  - 80\n",
      "  - 108\n",
      "  - 138\n",
      "  - 27\n",
      "  - 139\n",
      "  - 125\n",
      "  - 117\n",
      "  - 105\n",
      "  - 109\n",
      "  - 243\n",
      "  - 236\n",
      "  - 208\n",
      "  - 98\n",
      "  - 176\n",
      "  - 209\n",
      "  - 194\n",
      "  - 61\n",
      "  - 73\n",
      "  - 214\n",
      "  - 206\n",
      "  - 229\n",
      "  - 225\n",
      "  - 183\n",
      "  - 186\n",
      "  - 215\n",
      "  - 187\n",
      "  - 248\n",
      "  - 179\n",
      "  - 163\n",
      "  - 180\n",
      "  - 181\n",
      "  - 153\n",
      "  - 245\n",
      "  - 54\n",
      "  - 246\n",
      "  - 112\n",
      "  - 212\n",
      "  - 141\n",
      "  - 204\n",
      "  - 227\n",
      "  - 173\n",
      "  - 55\n",
      "  - 119\n",
      "  - 262\n",
      "  - 127\n",
      "  - 71\n",
      "  - 59\n",
      "  - 144\n",
      "  - 113\n",
      "  - 170\n",
      "  - 241\n",
      "  - 168\n",
      "  - 47\n",
      "  - 159\n",
      "  value_weight: 0.1\n",
      "static: true\n",
      "\n",
      "INFO:root:====================================================================================================\n",
      "INFO:root:# Pre-training experiment\n",
      "INFO:root:====================================================================================================\n",
      "INFO:root:Loading a pre-trained model with the checkpoint path /rds/projects/s/subramaa-mum-predict/CharlesGadd_Oxford/FoundationModelOutput/checkpoints/CrossSectionalMLP.ckpt.\n",
      "INFO:root:Created Performance metric callback for causal self-supervised tasks.\n",
      "INFO:root:Interactive job = True\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/rds/bear-apps/2022a/EL8-ice/software/PyTorch-Lightning/2.1.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:67: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "INFO:root:Testing model.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2209ab9405b54fc785a1aa273f021373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Unable to calculate baseline causal metrics, this sample will be skipped - this will bias metrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6484, 282])\n",
      "265\n",
      "torch.Size([4, 282])\n",
      "265\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/Projects/CPRD/src/models/survival/custom_callbacks/causal_eval.py:101\u001b[0m, in \u001b[0;36mPerformanceMetrics.get_scalar_based_metrics\u001b[0;34m(self, risk_scores, observed_k, _trainer, _pl_module, log_name, suppress_warnings, k_ahead)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     98\u001b[0m     \n\u001b[1;32m     99\u001b[0m     \u001b[38;5;66;03m# Calculate normalised concordance based on position in ranked risk.\u001b[39;00m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# event_concordance = np.where(risk_scores == observed_k.cpu().numpy())[0][0] / (len(risk_scores) - 1)\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m     risk_scores \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margsort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrisk_scores\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    102\u001b[0m     event_concordance \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(risk_scores \u001b[38;5;241m==\u001b[39m observed_k\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mlen\u001b[39m(risk_scores) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36margsort\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/rds/bear-apps/2022a/EL8-ice/software/SciPy-bundle/2022.05-foss-2022a/lib/python3.10/site-packages/numpy/core/fromnumeric.py:1120\u001b[0m, in \u001b[0;36margsort\u001b[0;34m(a, axis, kind, order)\u001b[0m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;124;03mReturns the indices that would sort an array.\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1118\u001b[0m \n\u001b[1;32m   1119\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43margsort\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/rds/bear-apps/2022a/EL8-ice/software/SciPy-bundle/2022.05-foss-2022a/lib/python3.10/site-packages/numpy/core/fromnumeric.py:54\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bound \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/rds/bear-apps/2022a/EL8-ice/software/SciPy-bundle/2022.05-foss-2022a/lib/python3.10/site-packages/numpy/core/fromnumeric.py:43\u001b[0m, in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     42\u001b[0m     wrap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m, method)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wrap:\n",
      "File \u001b[0;32m/rds/bear-apps/2022a/EL8-ice/software/PyTorch/2.0.1-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/_tensor.py:970\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 970\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m initialize(version_base\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, config_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../SurvivEHR/confs\u001b[39m\u001b[38;5;124m\"\u001b[39m, job_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      2\u001b[0m     cfg \u001b[38;5;241m=\u001b[39m compose(config_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig_CompetingRisk11M\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      3\u001b[0m                   overrides\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexperiment.run_id=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCrossSectionalMLP\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m                              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata.meta_information_path=/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/meta_information_QuantJenny.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m                              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m+static=True\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m                             ])\n\u001b[0;32m---> 12\u001b[0m model, dm \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded model with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1e6\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m M parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "File \u001b[0;32m/rds/bear-apps/2022a/EL8-ice/software/Hydra/1.3.2-GCCcore-11.3.0/lib/python3.10/site-packages/hydra/main.py:83\u001b[0m, in \u001b[0;36mmain.<locals>.main_decorator.<locals>.decorated_main\u001b[0;34m(cfg_passthrough)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(task_function)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorated_main\u001b[39m(cfg_passthrough: Optional[DictConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cfg_passthrough \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 83\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg_passthrough\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     85\u001b[0m         args_parser \u001b[38;5;241m=\u001b[39m get_args_parser()\n",
      "File \u001b[0;32m~/Projects/CPRD/examples/modelling/benchmarks/Pretrain/run_experiment.py:140\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mexperiment\u001b[38;5;241m.\u001b[39mtest:\n\u001b[1;32m    139\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting model.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 140\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiment_instance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m experiment_instance, dm\n",
      "File \u001b[0;32m/rds/bear-apps/2022a/EL8-ice/software/PyTorch-Lightning/2.1.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:755\u001b[0m, in \u001b[0;36mTrainer.test\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    753\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    754\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 755\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_test_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\n\u001b[1;32m    757\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/rds/bear-apps/2022a/EL8-ice/software/PyTorch-Lightning/2.1.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m/rds/bear-apps/2022a/EL8-ice/software/PyTorch-Lightning/2.1.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:795\u001b[0m, in \u001b[0;36mTrainer._test_impl\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    791\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    792\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    793\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn, ckpt_path, model_provided\u001b[38;5;241m=\u001b[39mmodel_provided, model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    794\u001b[0m )\n\u001b[0;32m--> 795\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[38;5;66;03m# remove the tensors from the test results\u001b[39;00m\n\u001b[1;32m    797\u001b[0m results \u001b[38;5;241m=\u001b[39m convert_tensors_to_scalars(results)\n",
      "File \u001b[0;32m/rds/bear-apps/2022a/EL8-ice/software/PyTorch-Lightning/2.1.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:990\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    987\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 990\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    994\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    995\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/rds/bear-apps/2022a/EL8-ice/software/PyTorch-Lightning/2.1.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1029\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mzero_grad(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mzero_grad_kwargs)\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluating:\n\u001b[0;32m-> 1029\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting:\n\u001b[1;32m   1031\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m/rds/bear-apps/2022a/EL8-ice/software/PyTorch-Lightning/2.1.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:181\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/rds/bear-apps/2022a/EL8-ice/software/PyTorch-Lightning/2.1.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:134\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/rds/bear-apps/2022a/EL8-ice/software/PyTorch-Lightning/2.1.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:405\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    400\u001b[0m     hook_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_kwargs(\n\u001b[1;32m    401\u001b[0m         batch, batch_idx, dataloader_idx \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_sequential \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_dataloaders \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    402\u001b[0m     )\n\u001b[1;32m    404\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_test_batch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_validation_batch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 405\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_callback_hooks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhook_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    406\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_lightning_module_hook(trainer, hook_name, output, \u001b[38;5;241m*\u001b[39mhook_kwargs\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m    408\u001b[0m trainer\u001b[38;5;241m.\u001b[39m_logger_connector\u001b[38;5;241m.\u001b[39mon_batch_end()\n",
      "File \u001b[0;32m/rds/bear-apps/2022a/EL8-ice/software/PyTorch-Lightning/2.1.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:208\u001b[0m, in \u001b[0;36m_call_callback_hooks\u001b[0;34m(trainer, hook_name, monitoring_callbacks, *args, **kwargs)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callable(fn):\n\u001b[1;32m    207\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Callback]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcallback\u001b[38;5;241m.\u001b[39mstate_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 208\u001b[0m             \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pl_module:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    212\u001b[0m     pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/Projects/CPRD/src/models/survival/custom_callbacks/causal_eval.py:243\u001b[0m, in \u001b[0;36mPerformanceMetrics.on_test_batch_end\u001b[0;34m(self, trainer, pl_module, outputs, batch, batch_idx)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_test_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, trainer, pl_module, outputs, batch, batch_idx):\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m# Run callback\u001b[39;00m\n\u001b[0;32m--> 243\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_callback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_trainer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m                      \u001b[49m\u001b[43m_pl_module\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpl_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mlog_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTest:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m                      \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/CPRD/src/models/survival/custom_callbacks/causal_eval.py:182\u001b[0m, in \u001b[0;36mPerformanceMetrics.run_callback\u001b[0;34m(self, _trainer, _pl_module, batch, log_name, plot_outcome_curves, suppress_warnings)\u001b[0m\n\u001b[1;32m    179\u001b[0m     _observed_outcome_k \u001b[38;5;241m=\u001b[39m k[\u001b[38;5;241m0\u001b[39m][transition]   \n\u001b[1;32m    181\u001b[0m     \u001b[38;5;66;03m# Get the baseline prevalence based concordance metric for each transition\u001b[39;00m\n\u001b[0;32m--> 182\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_scalar_based_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_outcome_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43m_observed_outcome_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43m_trainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43m_pl_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mlog_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_clf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43msuppress_warnings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuppress_warnings\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/CPRD/src/models/survival/custom_callbacks/causal_eval.py:115\u001b[0m, in \u001b[0;36mPerformanceMetrics.get_scalar_based_metrics\u001b[0;34m(self, risk_scores, observed_k, _trainer, _pl_module, log_name, suppress_warnings, k_ahead)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m suppress_warnings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    114\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to calculate baseline causal metrics, this sample will be skipped - this will bias metrics.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 115\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n"
     ]
    }
   ],
   "source": [
    "with initialize(version_base=None, config_path=\"../../SurvivEHR/confs\", job_name=\"\"):\n",
    "    cfg = compose(config_name=\"config_CompetingRisk11M\", \n",
    "                  overrides=[\"experiment.run_id='CrossSectionalMLP'\",\n",
    "                             \"data.meta_information_path=/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/meta_information_QuantJenny.pickle\",\n",
    "                             \"data.min_workers=12\",\n",
    "                             \"experiment.train=False\",\n",
    "                             \"experiment.test=True\",\n",
    "                             \"experiment.log=False\",\n",
    "                             \"+static=True\"\n",
    "                            ])\n",
    "\n",
    "model, dm = run(cfg)\n",
    "print(f\"Loaded model with {sum(p.numel() for p in model.parameters())/1e6} M parameters\")\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "sys_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
