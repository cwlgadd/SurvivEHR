{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d7eb476-b0e9-4c43-be3c-3e82260f1a61",
   "metadata": {},
   "source": [
    "# Demo Notebook:\n",
    "## Random Survival Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62502ac8-4238-415d-ab1e-bb60322fd55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added path '/rds/homes/g/gaddcz/Projects/CPRD/virtual-envTorch2.0-icelake/lib/python3.10/site-packages' at start of search paths.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "node_type = os.getenv('BB_CPU')\n",
    "venv_dir = f'/rds/homes/g/gaddcz/Projects/CPRD/virtual-envTorch2.0-{node_type}'\n",
    "venv_site_pkgs = Path(venv_dir) / 'lib' / f'python{sys.version_info.major}.{sys.version_info.minor}' / 'site-packages'\n",
    "if venv_site_pkgs.exists():\n",
    "    sys.path.insert(0, str(venv_site_pkgs))\n",
    "    print(f\"Added path '{venv_site_pkgs}' at start of search paths.\")\n",
    "else:\n",
    "    print(f\"Path '{venv_site_pkgs}' not found. Check that it exists and/or that it exists for node-type '{node_type}'.\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e38ef008-1d1b-4f0f-b289-de7c5274d67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda.\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sqlite3\n",
    "from dataclasses import dataclass\n",
    "import logging\n",
    "from CPRD.data.foundational_loader import FoundationalDataModule\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pycox.datasets import support\n",
    "from pycox.evaluation import EvalSurv\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn import set_config\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sksurv.datasets import load_gbsg2\n",
    "from sksurv.preprocessing import OneHotEncoder\n",
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = \"cpu\"    # if more informative debugging statements are needed\n",
    "print(f\"Using device: {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbfe7b8b-d667-40f7-80c4-6cc3889af5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_config(display=\"text\")  # displays text representation of estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25fb523-c304-47fe-978a-100e46903d9d",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "857281d4-f0f1-4ce3-867f-95e3f0d1d710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(dataset, competing_risk, sample_size=None):\n",
    "\n",
    "    match dataset.lower():\n",
    "        case \"pycox\":\n",
    "            df_train = support.read_df()\n",
    "            df_test = df_train.sample(frac=0.2)\n",
    "            df_train = df_train.drop(df_test.index)\n",
    "            df_val = df_train.sample(frac=0.2)\n",
    "            df_train = df_train.drop(df_val.index)\n",
    "            \n",
    "            cols_standardize = ['x0', 'x7', 'x8', 'x9', 'x10', 'x11', 'x12', 'x13']\n",
    "            cols_leave = ['x1', 'x2', 'x3', 'x4', 'x5', 'x6']\n",
    "            \n",
    "            standardize = [([col], StandardScaler()) for col in cols_standardize]\n",
    "            leave = [(col, None) for col in cols_leave]\n",
    "            \n",
    "            x_mapper = DataFrameMapper(standardize + leave)\n",
    "            \n",
    "            x_train = x_mapper.fit_transform(df_train).astype('float32')\n",
    "            x_val = x_mapper.transform(df_val).astype('float32')\n",
    "            x_test = x_mapper.transform(df_test).astype('float32')\n",
    "            \n",
    "            get_target = lambda df: (df['duration'].values, df['event'].values)\n",
    "            y_train = get_target(df_train)\n",
    "            y_val = get_target(df_val)\n",
    "            y_test = get_target(df_test)\n",
    "            \n",
    "            t_train, e_train = y_train\n",
    "            t_val, e_val = y_val\n",
    "            t_test, e_test = y_test\n",
    "            \n",
    "            t_train_max = np.amax(t_train)\n",
    "            t_train = t_train / t_train_max\n",
    "            t_val = t_val / t_train_max\n",
    "            t_test = t_test / t_train_max\n",
    "            \n",
    "    \n",
    "        case \"hypertension\" | \"cvd\":\n",
    "    \n",
    "            # Training samples\n",
    "            if sample_size is not None:\n",
    "                save_path =  f\"/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_{dataset}/\" + f\"benchmark_data/N={sample_size}.pickle\" \n",
    "            else:\n",
    "                save_path = f\"/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_{dataset}/\" + \"benchmark_data/all.pickle\"\n",
    "                \n",
    "            with open(save_path, \"rb\") as handle:\n",
    "                print(f\"Loading training dataset from {save_path}\")\n",
    "                data_train = pickle.load(handle)\n",
    "            \n",
    "            # display(data[\"X_train\"].head())\n",
    "            # display(data[\"y_train\"])\n",
    "            # print(data.keys())\n",
    "            \n",
    "            data = {}\n",
    "            data[\"X_train\"] = data_train[\"X_train\"]\n",
    "            data[\"y_train\"] = data_train[\"y_train\"]\n",
    "    \n",
    "            # Test and validation samples\n",
    "    \n",
    "            save_path = f\"/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_{dataset}/\" + \"benchmark_data/all.pickle\"\n",
    "            with open(save_path, \"rb\") as handle:\n",
    "                print(f\"Loading validation/test datasets from {save_path}\")\n",
    "                data_val_test = pickle.load(handle)\n",
    "                \n",
    "            data[\"X_val\"] = data_val_test[\"X_val\"]\n",
    "            data[\"y_val\"] = data_val_test[\"y_val\"]\n",
    "            data[\"X_test\"] = data_val_test[\"X_test\"]\n",
    "            data[\"y_test\"] = data_val_test[\"y_test\"]\n",
    "    \n",
    "            # Convert to correct formats\n",
    "            x_train = data[\"X_train\"].to_numpy(dtype=np.float32)\n",
    "            x_val = data[\"X_val\"].to_numpy(dtype=np.float32)\n",
    "            x_test = data[\"X_test\"].to_numpy(dtype=np.float32)\n",
    "            \n",
    "            t_train = np.asarray([i[1] for i in data[\"y_train\"]])\n",
    "            t_val = np.asarray([i[1] for i in data[\"y_val\"]])        \n",
    "            t_test = np.asarray([i[1] for i in data[\"y_test\"]])\n",
    "    \n",
    "            if competing_risk is False:\n",
    "                e_train = np.asarray([0 if i[0] == 0 else 1 for i in data[\"y_train\"]])\n",
    "                e_val = np.asarray([0 if i[0] == 0 else 1 for i in data[\"y_val\"]])\n",
    "                e_test = np.asarray([0 if i[0] == 0 else 1 for i in data[\"y_test\"]])\n",
    "            else:\n",
    "                e_train = np.asarray([i[0] for i in data[\"y_train\"]])\n",
    "                e_val = np.asarray([i[0] for i in data[\"y_val\"]])\n",
    "                e_test = np.asarray([i[0] for i in data[\"y_test\"]])\n",
    "\n",
    "    # display(x_train.shape)\n",
    "    # display(type(x_train))\n",
    "    # display(type(x_train[0,0]))\n",
    "    # display(e_train.shape)\n",
    "    # display(type(e_train))\n",
    "    # display(type(e_train[0]))\n",
    "    # display(t_train.shape)\n",
    "    # display(type(t_train))\n",
    "    # display(type(t_train[0]))\n",
    "    # print(np.mean(e_train))\n",
    "    # print(np.mean(t_train))\n",
    "    # print(np.std(t_train))\n",
    "    # print(np.mean(x_train))\n",
    "    # print(t_train.min())\n",
    "    # print(t_train.max())\n",
    "    # print(np.unique(e_test, return_counts=True))\n",
    "\n",
    "    # print(x_train.shape)\n",
    "    # print(t_train.shape)\n",
    "    # print(e_train.shape)\n",
    "\n",
    "    \n",
    "    Xtrain = pd.DataFrame(x_train, )    #  columns=list(dm.train_set.tokenizer._stoi.keys())[1:]\n",
    "    Xval = pd.DataFrame(x_val, )    #  columns=list(dm.train_set.tokenizer._stoi.keys())[1:]\n",
    "    Xtest = pd.DataFrame(x_test, )    #  columns=list(dm.train_set.tokenizer._stoi.keys())[1:]\n",
    "\n",
    "    if competing_risk is False:\n",
    "        ytrain = np.array([(_yk, _yt) for _yk, _yt in zip(e_train, t_train)], dtype=[('cens', 'bool'), ('time', '<f8')])\n",
    "        yval = np.array([(_yk, _yt) for _yk, _yt in zip(e_val, t_val)], dtype=[('cens', 'bool'), ('time', '<f8')])\n",
    "        ytest = np.array([(_yk, _yt) for _yk, _yt in zip(e_test, t_test)], dtype=[('cens', 'bool'), ('time', '<f8')])\n",
    "    else:\n",
    "        # Package does not support Competing Risks\n",
    "        raise NotImplementedError\n",
    "        \n",
    "        # ytrain = np.array([(_yk, _yt) for _yk, _yt in zip(e_train, t_train)])# , dtype=[('cens', 'float'), ('time', '<f8')])\n",
    "        # yval = np.array([(_yk, _yt) for _yk, _yt in zip(e_val, t_val)]) #, dtype=[('cens', 'float'), ('time', '<f8')])\n",
    "        # ytest = np.array([(_yk, _yt) for _yk, _yt in zip(e_test, t_test)]) #, dtype=[('cens', 'float'), ('time', '<f8')])\n",
    "    # print(Xtrain.head())\n",
    "    # print(ytrain[:5])\n",
    "\n",
    "    return (Xtrain, ytrain), (Xval, yval), (Xtest, ytest)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfaa037-2ad3-4105-99f4-23184dd0abaf",
   "metadata": {},
   "source": [
    "# Example dataloader function usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c45f158-945d-411a-bc12-fd713aa4df42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training dataset from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/benchmark_data/N=2999.pickle\n",
      "Loading validation/test datasets from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/benchmark_data/all.pickle\n",
      "4.401644706726074\n",
      "(array([0, 1]), array([31472,  4286]))\n",
      "(2999, 279)\n",
      "(2999,)\n",
      "(2999,)\n"
     ]
    }
   ],
   "source": [
    "dataset_train, dataset_val, dataset_test = get_dataloaders(\"CVD\", False, sample_size=2999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69c75e98-18b6-4c54-8893-24fb23a71304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0    1    2    3    4    5    6    7    8    9    ...  269  270  271  272  \\\n",
      "0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
      "1  1.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  ...  1.0  0.0  1.0  1.0   \n",
      "2  1.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  ...  0.0  1.0  1.0  1.0   \n",
      "3  1.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  ...  1.0  0.0  1.0  1.0   \n",
      "4  1.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  ...  1.0  0.0  0.0  0.0   \n",
      "\n",
      "   273  274  275  276  277  278  \n",
      "0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1  1.0  1.0  1.0  1.0  1.0  1.0  \n",
      "2  0.0  1.0  1.0  1.0  0.0  0.0  \n",
      "3  0.0  1.0  1.0  1.0  0.0  0.0  \n",
      "4  0.0  1.0  1.0  1.0  0.0  0.0  \n",
      "\n",
      "[5 rows x 279 columns]\n",
      "[(False, 2.43013716) (False, 0.34082222) (False, 2.12383461)]\n"
     ]
    }
   ],
   "source": [
    "print(dataset_test[0].head())\n",
    "print(dataset_test[1][:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b4b706-2fb3-4964-92f5-a90f9bb0eb7d",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e1a947d-84ac-4178-994e-0a6e45b224e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"CVD\" # \"Hypertension\"\n",
    "competing_risk = False\n",
    "sample_sizes = [int(np.exp(_log_n)) for _log_n in np.linspace(np.log(3000), np.log(500000), 10)]      # [3000, 12500, 30000, 60000, 100000]: # 600, 1200, \n",
    "# sample_sizes = [None]\n",
    "\n",
    "# the time grid which we generate over\n",
    "t_eval = np.linspace(0, 1, 1000) \n",
    "# the time grid which we calculate scores over\n",
    "time_grid = np.linspace(start=0, stop=1 , num=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97471d71-c98d-498a-ab6c-4ea786597e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training dataset from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/benchmark_data/N=2999.pickle\n",
      "Loading validation/test datasets from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/benchmark_data/all.pickle\n",
      "4.401644706726074\n",
      "(array([0, 1]), array([31472,  4286]))\n",
      "(2999, 279)\n",
      "(2999,)\n",
      "(2999,)\n",
      "Fitting Random Survival Forest\n",
      "Evaluating performance by splitting (35758, 279) test samples into batches of size 512\n",
      "Random Survival Forest (SR):N=2999.        Ctd: 0.5817231098874919. IBS: 0.03399845492489739. INBLL: 0.14861047476033662\n",
      "Loading training dataset from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/benchmark_data/N=5296.pickle\n",
      "Loading validation/test datasets from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/benchmark_data/all.pickle\n",
      "4.402191162109375\n",
      "(array([0, 1]), array([31472,  4286]))\n",
      "(5296, 279)\n",
      "(5296,)\n",
      "(5296,)\n",
      "Fitting Random Survival Forest\n",
      "Evaluating performance by splitting (35758, 279) test samples into batches of size 512\n",
      "Random Survival Forest (SR):N=5296.        Ctd: 0.5726886646549809. IBS: 0.03396416779973878. INBLL: 0.149062513427203\n",
      "Loading training dataset from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/benchmark_data/N=9351.pickle\n",
      "Loading validation/test datasets from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/benchmark_data/all.pickle\n",
      "4.402191162109375\n",
      "(array([0, 1]), array([31472,  4286]))\n",
      "(9351, 279)\n",
      "(9351,)\n",
      "(9351,)\n",
      "Fitting Random Survival Forest\n",
      "Evaluating performance by splitting (35758, 279) test samples into batches of size 512\n",
      "Random Survival Forest (SR):N=9351.        Ctd: 0.5883719280021017. IBS: 0.03384484456385947. INBLL: 0.14750556866346265\n",
      "Loading training dataset from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/benchmark_data/N=16509.pickle\n",
      "Loading validation/test datasets from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/benchmark_data/all.pickle\n",
      "4.402191162109375\n",
      "(array([0, 1]), array([31472,  4286]))\n",
      "(16509, 279)\n",
      "(16509,)\n",
      "(16509,)\n",
      "Fitting Random Survival Forest\n",
      "Evaluating performance by splitting (35758, 279) test samples into batches of size 512\n",
      "Random Survival Forest (SR):N=16509.       Ctd: 0.5960278990576237. IBS: 0.0337225214683957. INBLL: 0.14647954529430643\n",
      "Loading training dataset from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/benchmark_data/N=29148.pickle\n",
      "Loading validation/test datasets from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/benchmark_data/all.pickle\n",
      "4.402191162109375\n",
      "(array([0, 1]), array([31472,  4286]))\n",
      "(29148, 279)\n",
      "(29148,)\n",
      "(29148,)\n",
      "Fitting Random Survival Forest\n",
      "Evaluating performance by splitting (35758, 279) test samples into batches of size 512\n",
      "Random Survival Forest (SR):N=29148.       Ctd: 0.5967549074819676. IBS: 0.033762424117441375. INBLL: 0.14621266423294518\n",
      "Loading training dataset from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/benchmark_data/N=51461.pickle\n",
      "Loading validation/test datasets from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/benchmark_data/all.pickle\n",
      "4.402191638946533\n",
      "(array([0, 1]), array([31472,  4286]))\n",
      "(51461, 279)\n",
      "(51461,)\n",
      "(51461,)\n",
      "Fitting Random Survival Forest\n",
      "Evaluating performance by splitting (35758, 279) test samples into batches of size 512\n",
      "Random Survival Forest (SR):N=51461.       Ctd: 0.6081889934667545. IBS: 0.03375098553961131. INBLL: 0.1457478005690567\n",
      "Loading training dataset from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/benchmark_data/N=90856.pickle\n",
      "Loading validation/test datasets from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/benchmark_data/all.pickle\n",
      "4.402192115783691\n",
      "(array([0, 1]), array([31472,  4286]))\n",
      "(90856, 279)\n",
      "(90856,)\n",
      "(90856,)\n",
      "Fitting Random Survival Forest\n",
      "Evaluating performance by splitting (35758, 279) test samples into batches of size 512\n",
      "Random Survival Forest (SR):N=90856.       Ctd: 0.6091873754214131. IBS: 0.033695391903505144. INBLL: 0.1452056774612423\n",
      "Loading training dataset from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/benchmark_data/N=160407.pickle\n",
      "Loading validation/test datasets from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/benchmark_data/all.pickle\n",
      "4.402192115783691\n",
      "(array([0, 1]), array([31472,  4286]))\n",
      "(160407, 279)\n",
      "(160407,)\n",
      "(160407,)\n",
      "Fitting Random Survival Forest\n",
      "Evaluating performance by splitting (35758, 279) test samples into batches of size 512\n",
      "Random Survival Forest (SR):N=160407.      Ctd: 0.6068623782034415. IBS: 0.033734096545360415. INBLL: 0.14558305134538546\n",
      "Loading training dataset from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/benchmark_data/N=283203.pickle\n",
      "Loading validation/test datasets from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/benchmark_data/all.pickle\n",
      "4.402192115783691\n",
      "(array([0, 1]), array([31472,  4286]))\n",
      "(283203, 279)\n",
      "(283203,)\n",
      "(283203,)\n",
      "Fitting Random Survival Forest\n",
      "Evaluating performance by splitting (35758, 279) test samples into batches of size 512\n",
      "Random Survival Forest (SR):N=283203.      Ctd: 0.6140593631057283. IBS: 0.03370825445709043. INBLL: 0.14507898552247775\n",
      "Loading training dataset from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/benchmark_data/N=500000.pickle\n",
      "Loading validation/test datasets from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/benchmark_data/all.pickle\n",
      "4.402192115783691\n",
      "(array([0, 1]), array([31472,  4286]))\n",
      "(500000, 279)\n",
      "(500000,)\n",
      "(500000,)\n",
      "Fitting Random Survival Forest\n",
      "Evaluating performance by splitting (35758, 279) test samples into batches of size 512\n",
      "Random Survival Forest (SR):N=500000.      Ctd: 0.6118567664875652. IBS: 0.033727277767864286. INBLL: 0.14548991693533334\n"
     ]
    }
   ],
   "source": [
    "# Loop over different dataset sizes, train the Random Survival Forest with bootstapping and report results\n",
    "\n",
    "\n",
    "for sample_size in sample_sizes:\n",
    "\n",
    "    # Get dataset for given sample size\n",
    "    dataset_train, dataset_val, dataset_test = get_dataloaders(dataset, competing_risk, sample_size=sample_size)\n",
    "\n",
    "    # Create RSF model with default bootstrap values due to memory constraints\n",
    "    print(f\"Fitting Random Survival Forest\")\n",
    "    rsf = RandomSurvivalForest(\n",
    "        bootstrap=True,\n",
    "        max_samples=1000,    \n",
    "        random_state=42,\n",
    "        low_memory=False\n",
    "    )\n",
    "    rsf.fit(dataset_train[0], dataset_train[1])\n",
    "\n",
    "    # Test\n",
    "    bsz = 512\n",
    "    print(f\"Evaluating performance by splitting {dataset_test[0].shape} test samples into batches of size {bsz}\")\n",
    "    \n",
    "    ctd = []\n",
    "    ibs = []\n",
    "    inbll = []\n",
    "    for batch_idx in range(0, dataset_test[0].shape[0], bsz):\n",
    "    \n",
    "        batch_dataset_test = (dataset_test[0][batch_idx:batch_idx + bsz], dataset_test[1][batch_idx:batch_idx + bsz])\n",
    "        actual_bsz = batch_dataset_test[0].shape[0]\n",
    "    \n",
    "        # Predict survival functionfor batch\n",
    "        surv = rsf.predict_survival_function(batch_dataset_test[0], return_array=True)\n",
    "        \n",
    "        # Find the indices in rsf.unique_times_ that are closest to values in t_eval, so we can evaluate the RSF if the same way as other benchmarks\n",
    "        closest_indices = [np.abs(rsf.unique_times_ - v).argmin() for v in t_eval]\n",
    "        surv_reduced = surv[:, closest_indices]\n",
    "    \n",
    "        # Format appropriately\n",
    "        df_surv = pd.DataFrame(np.transpose(surv_reduced), index=t_eval)\n",
    "        \n",
    "        lbls_test = np.zeros((actual_bsz,))\n",
    "        t_test = np.zeros((actual_bsz,))\n",
    "        for sample_idx in range(actual_bsz):\n",
    "            lbls_test[sample_idx] = 1 if batch_dataset_test[1][sample_idx][0] == True else 0\n",
    "            t_test[sample_idx] = batch_dataset_test[1][sample_idx][1]\n",
    "    \n",
    "        # Same treatment as in SurvivEHR\n",
    "        ev = EvalSurv(df_surv, t_test, lbls_test, censor_surv='km')\n",
    "        ctd.append(ev.concordance_td())\n",
    "        ibs.append(ev.integrated_brier_score(time_grid))\n",
    "        inbll.append(ev.integrated_nbll(time_grid))\n",
    "    \n",
    "        # print(f\"Scores up to sample {batch_idx+bsz}:\".ljust(50) + f\"Ctd: {np.mean(ctd):.3f}. IBS: {np.mean(ibs):.4f}. INBLL: {np.mean(inbll):.3f}\")\n",
    "    \n",
    "    # display(f\"Ctd:   {np.mean(ctd):.3f}\")\n",
    "    # display(f\"IBS:   {np.mean(ibs):.4f}\")\n",
    "    # display(f\"INBLL: {np.mean(inbll):.3f}\")\n",
    "    print(f\"\\tRandom Survival Forest ({'CR' if competing_risk else 'SR'}):\".ljust(20) + f\"N={sample_size}.\".ljust(15) + f\"Ctd: {np.mean(ctd)}. IBS: {np.mean(ibs)}. INBLL: {np.mean(inbll)}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca96c2f6-8b29-4853-9e34-12bd75e4be56",
   "metadata": {},
   "source": [
    "# Output across different setups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bee043-a472-4590-b431-4843cebb7ee7",
   "metadata": {},
   "source": [
    "Cardiovascular disease Single Risk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76125f29-7611-40a8-a86e-df84cf30c7e2",
   "metadata": {},
   "source": [
    "```\n",
    "Random Survival Forest (SR):N=2999.        Ctd: 0.5817231098874919. IBS: 0.03399845492489739. INBLL: 0.14861047476033662\n",
    "Random Survival Forest (SR):N=5296.        Ctd: 0.5726886646549809. IBS: 0.03396416779973878. INBLL: 0.149062513427203\n",
    "Random Survival Forest (SR):N=9351.        Ctd: 0.5883719280021017. IBS: 0.03384484456385947. INBLL: 0.14750556866346265\n",
    "Random Survival Forest (SR):N=16509.       Ctd: 0.5960278990576237. IBS: 0.0337225214683957. INBLL: 0.14647954529430643\n",
    "Random Survival Forest (SR):N=29148.       Ctd: 0.5967549074819676. IBS: 0.033762424117441375. INBLL: 0.14621266423294518\n",
    "Random Survival Forest (SR):N=51461.       Ctd: 0.6081889934667545. IBS: 0.03375098553961131. INBLL: 0.1457478005690567\n",
    "Random Survival Forest (SR):N=90856.       Ctd: 0.6091873754214131. IBS: 0.033695391903505144. INBLL: 0.1452056774612423\n",
    "Random Survival Forest (SR):N=160407.      Ctd: 0.6068623782034415. IBS: 0.033734096545360415. INBLL: 0.14558305134538546\n",
    "Random Survival Forest (SR):N=283203.      Ctd: 0.6140593631057283. IBS: 0.03370825445709043. INBLL: 0.14507898552247775\n",
    "Random Survival Forest (SR):N=500000.      Ctd: 0.6118567664875652. IBS: 0.033727277767864286. INBLL: 0.14548991693533334\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "896a726a-205d-4f4f-82a3-68a4ead97b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ctd:   0.605'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'IBS:   0.0337'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'INBLL: 0.146'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e858e452-4681-4a2a-9c82-01c2ed342f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n"
     ]
    }
   ],
   "source": [
    "print(len(ctd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6070f084-3565-49c3-9263-a537f1e90041",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "sys_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
