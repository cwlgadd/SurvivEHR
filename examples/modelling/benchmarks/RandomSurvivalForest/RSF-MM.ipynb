{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d7eb476-b0e9-4c43-be3c-3e82260f1a61",
   "metadata": {},
   "source": [
    "# Demo Notebook:\n",
    "## Random Survival Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62502ac8-4238-415d-ab1e-bb60322fd55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added path '/rds/homes/g/gaddcz/Projects/CPRD/virtual-envTorch2.0-icelake/lib/python3.10/site-packages' at start of search paths.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "node_type = os.getenv('BB_CPU')\n",
    "venv_dir = f'/rds/homes/g/gaddcz/Projects/CPRD/virtual-envTorch2.0-{node_type}'\n",
    "venv_site_pkgs = Path(venv_dir) / 'lib' / f'python{sys.version_info.major}.{sys.version_info.minor}' / 'site-packages'\n",
    "if venv_site_pkgs.exists():\n",
    "    sys.path.insert(0, str(venv_site_pkgs))\n",
    "    print(f\"Added path '{venv_site_pkgs}' at start of search paths.\")\n",
    "else:\n",
    "    print(f\"Path '{venv_site_pkgs}' not found. Check that it exists and/or that it exists for node-type '{node_type}'.\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e38ef008-1d1b-4f0f-b289-de7c5274d67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from FastEHR.dataloader import FoundationalDataModule\n",
    "\n",
    "from pycox.datasets import support\n",
    "from pycox.evaluation import EvalSurv\n",
    "from scipy.integrate import trapz\n",
    "\n",
    "from sklearn import set_config\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sksurv.datasets import load_gbsg2\n",
    "from sksurv.preprocessing import OneHotEncoder\n",
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "\n",
    "from CPRD.examples.modelling.benchmarks.DeSurv.make_desurv_loader import get_dataloaders\n",
    "from CPRD.examples.modelling.SurvivEHR.custom_outcome_methods import custom_mm_outcomes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbfe7b8b-d667-40f7-80c4-6cc3889af5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_config(display=\"text\")  # displays text representation of estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25fb523-c304-47fe-978a-100e46903d9d",
   "metadata": {},
   "source": [
    "# Extract the indicies which relate to the diagnoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "857281d4-f0f1-4ce3-867f-95e3f0d1d710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 31, 33, 34, 35, 36, 37, 38, 41, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 58, 59, 60, 61, 63, 64, 67, 71, 72, 73, 75, 77, 80, 82, 85, 89, 90, 93, 94, 96, 97, 98, 104, 106, 108, 109, 110, 112, 115, 119, 121, 129, 135, 138, 141, 144, 148, 149, 151]\n"
     ]
    }
   ],
   "source": [
    "with initialize(version_base=None, config_path=\"../../SurvivEHR/confs\", job_name=\"desurv-mm-notebook\"):\n",
    "    cfg = compose(config_name=\"config_CompetingRisk11M\")\n",
    "\n",
    "dm = FoundationalDataModule(path_to_db=cfg.data.path_to_db,\n",
    "                            path_to_ds=\"/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_MultiMorbidity50+/\",\n",
    "                            overwrite_meta_information=cfg.data.meta_information_path,\n",
    "                            load=True)\n",
    "\n",
    "# Get the indicies for the diagnoses used to stratify patient groups (under the SurvivEHR setup)\n",
    "conditions = custom_mm_outcomes(dm)\n",
    "encoded_conditions = dm.tokenizer.encode(conditions)                    # The indicies of the MM events in the xsectional dataset (not adjusted for UNK/PAD/static data)\n",
    "\n",
    "# Get the number of baseline static variables (after one-hot encoding etc), and the vocab size excluding PAD and UNK tokens\n",
    "num_cov = dm.train_set[0][\"static_covariates\"].shape[0]\n",
    "num_context_tokens = dm.tokenizer._event_counts.shape[0] - 1            # Removing UNK token, which is not included in xsectional datasets\n",
    "\n",
    "# Convert the `encoded_conditions` indicies to the equivalent in the xsectional dataset\n",
    "encoded_conditions_xsec = [_ind + num_cov - 1 for _ind in encoded_conditions]\n",
    "print(encoded_conditions_xsec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35e18d8-42cf-4fe9-a6e9-c21667798e20",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfaa037-2ad3-4105-99f4-23184dd0abaf",
   "metadata": {},
   "source": [
    "## Example/test dataloader usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c45f158-945d-411a-bc12-fd713aa4df42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training dataset from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_MultiMorbidity50+/benchmark_data/N=20000_seed1.pickle\n",
      "Loading validation/test datasets from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_MultiMorbidity50+/benchmark_data/all.pickle\n",
      "279\n"
     ]
    }
   ],
   "source": [
    "# dataset_train, dataset_val, dataset_test = get_dataloaders(\"Hypertension\", False, sample_size=2999, seed=1)\n",
    "\n",
    "dataset_train, dataset_val, dataset_test = get_dataloaders(\"MultiMorbidity50+\", False, benchmark=\"sklearn_RSF\", sample_size=20000, seed=1)\n",
    "\n",
    "num_xsectional_in_dims = dataset_train[0].shape[-1]\n",
    "print(num_xsectional_in_dims)\n",
    "assert num_xsectional_in_dims == num_cov + num_context_tokens, f\"{num_xsectional_in_dims} != {num_cov} + {num_context_tokens}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69c75e98-18b6-4c54-8893-24fb23a71304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset_test[0].head())\n",
    "# print(dataset_test[1][:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b4b706-2fb3-4964-92f5-a90f9bb0eb7d",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e1a947d-84ac-4178-994e-0a6e45b224e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sizes = [20000]\n",
    "\n",
    "# the time grid which we generate over\n",
    "t_eval = np.linspace(0, 1, 1000) \n",
    "# the time grid which we calculate scores over\n",
    "time_grid = np.linspace(start=0, stop=1 , num=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97471d71-c98d-498a-ab6c-4ea786597e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training dataset from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_MultiMorbidity50+/benchmark_data/N=20000_seed1.pickle\n",
      "Loading validation/test datasets from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_MultiMorbidity50+/benchmark_data/all.pickle\n",
      "\n",
      "\n",
      "RandomSurvivalForest-SR-MultiMorbidity50+-Ns20000-seed1\n",
      "Training\n",
      "Evaluating performance by splitting (107557, 279) test samples into batches of size 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 421/421 [04:05<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomSurvivalForest-SR-MultiMorbidity50+-Ns20000-seed1:N=20000.       Ctd: 0.5834264896399975. IBS: 0.15459947151900016. INBLL: 0.46990139389813723\n",
      "Loading training dataset from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_MultiMorbidity50+/benchmark_data/N=20000_seed2.pickle\n",
      "Loading validation/test datasets from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_MultiMorbidity50+/benchmark_data/all.pickle\n",
      "\n",
      "\n",
      "RandomSurvivalForest-SR-MultiMorbidity50+-Ns20000-seed2\n",
      "Training\n",
      "Evaluating performance by splitting (107557, 279) test samples into batches of size 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 421/421 [04:34<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomSurvivalForest-SR-MultiMorbidity50+-Ns20000-seed2:N=20000.       Ctd: 0.5853854477013098. IBS: 0.15429969800844914. INBLL: 0.4690151566676098\n",
      "Loading training dataset from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_MultiMorbidity50+/benchmark_data/N=20000_seed3.pickle\n",
      "Loading validation/test datasets from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_MultiMorbidity50+/benchmark_data/all.pickle\n",
      "\n",
      "\n",
      "RandomSurvivalForest-SR-MultiMorbidity50+-Ns20000-seed3\n",
      "Training\n",
      "Evaluating performance by splitting (107557, 279) test samples into batches of size 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 421/421 [03:32<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomSurvivalForest-SR-MultiMorbidity50+-Ns20000-seed3:N=20000.       Ctd: 0.5851260399538806. IBS: 0.15412524528024846. INBLL: 0.46867952099052923\n",
      "Loading training dataset from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_MultiMorbidity50+/benchmark_data/N=20000_seed4.pickle\n",
      "Loading validation/test datasets from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_MultiMorbidity50+/benchmark_data/all.pickle\n",
      "\n",
      "\n",
      "RandomSurvivalForest-SR-MultiMorbidity50+-Ns20000-seed4\n",
      "Training\n",
      "Evaluating performance by splitting (107557, 279) test samples into batches of size 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 421/421 [04:28<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomSurvivalForest-SR-MultiMorbidity50+-Ns20000-seed4:N=20000.       Ctd: 0.5838205168142997. IBS: 0.1542414519560594. INBLL: 0.46888269152341966\n",
      "Loading training dataset from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_MultiMorbidity50+/benchmark_data/N=20000_seed5.pickle\n",
      "Loading validation/test datasets from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_MultiMorbidity50+/benchmark_data/all.pickle\n",
      "\n",
      "\n",
      "RandomSurvivalForest-SR-MultiMorbidity50+-Ns20000-seed5\n",
      "Training\n",
      "Evaluating performance by splitting (107557, 279) test samples into batches of size 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 421/421 [04:54<00:00,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomSurvivalForest-SR-MultiMorbidity50+-Ns20000-seed5:N=20000.       Ctd: 0.5840655791806779. IBS: 0.154206618938596. INBLL: 0.46882119779176207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_names, all_ctd, all_ibs, all_inbll = [], [], [], []\n",
    "all_obs_RMST, all_pred_RMST = [], []\n",
    "\n",
    "for sample_size in sample_sizes:\n",
    "\n",
    "    seeds = [1,2,3,4,5]\n",
    "\n",
    "    for seed in seeds:\n",
    "        # Load dataset\n",
    "        dataset_train, dataset_val, dataset_test = get_dataloaders(\"MultiMorbidity50+\", False, benchmark=\"sklearn_RSF\", sample_size=sample_size, seed=seed)\n",
    "\n",
    "        # Create RSF model with default bootstrap values due to memory constraints\n",
    "        model_name = f\"RandomSurvivalForest-SR-MultiMorbidity50+-Ns{sample_size}-seed{seed}\"\n",
    "        rsf = RandomSurvivalForest(\n",
    "            bootstrap=True,\n",
    "            max_samples=1000,    \n",
    "            random_state=seed,\n",
    "            low_memory=False\n",
    "        )\n",
    "        print(f\"\\n\\n{model_name}\")\n",
    "\n",
    "        # Train model\n",
    "        print(\"Training\")\n",
    "        rsf.fit(dataset_train[0], dataset_train[1])\n",
    "    \n",
    "        # Test\n",
    "        bsz = 2**8\n",
    "        print(f\"Evaluating performance by splitting {dataset_test[0].shape} test samples into batches of size {bsz}\")\n",
    "        ctd = []\n",
    "        ibs = []\n",
    "        inbll = []\n",
    "        obs_RMST_by_number_of_preexisting_conditions = [[] for _ in range(len(encoded_conditions_xsec))]\n",
    "        pred_RMST_by_number_of_preexisting_conditions = [[] for _ in range(len(encoded_conditions_xsec))]\n",
    "        test_generator = range(0, dataset_test[0].shape[0], bsz)\n",
    "        for batch_idx in tqdm(test_generator, total=(len(test_generator)), desc=\"Testing\"):\n",
    "        \n",
    "            batch_dataset_test = (dataset_test[0][batch_idx:batch_idx + bsz], dataset_test[1][batch_idx:batch_idx + bsz])\n",
    "            actual_bsz = batch_dataset_test[0].shape[0]\n",
    "        \n",
    "            # Predict survival functionfor batch\n",
    "            surv = rsf.predict_survival_function(batch_dataset_test[0], return_array=True)\n",
    "            \n",
    "            # Find the indices in rsf.unique_times_ that are closest to values in t_eval, so we can evaluate the RSF if the same way as other benchmarks\n",
    "            closest_indices = [np.abs(rsf.unique_times_ - v).argmin() for v in t_eval]\n",
    "            surv_reduced = surv[:, closest_indices]\n",
    "\n",
    "            lbls_test = np.zeros((actual_bsz,))\n",
    "            t_test = np.zeros((actual_bsz,))\n",
    "            for sample_idx in range(actual_bsz):\n",
    "                lbls_test[sample_idx] = 1 if batch_dataset_test[1][sample_idx][0] == True else 0\n",
    "                t_test[sample_idx] = batch_dataset_test[1][sample_idx][1]\n",
    "                \n",
    "            ###########################\n",
    "            # Get RMST Survival times #\n",
    "            ###########################                \n",
    "            for sample in range(surv_reduced.shape[0]):\n",
    "                x_test = batch_dataset_test[0].to_numpy()[sample, :]\n",
    "                e_test = batch_dataset_test[1][sample][0]\n",
    "                \n",
    "                # Get the number of pre-existing conditions\n",
    "                sample_stratification_label = np.sum(x_test[encoded_conditions_xsec] == 1)\n",
    "                # Get the RMST predicted under the survival curve\n",
    "                sample_predicted_rmst = trapz(surv_reduced[sample,:], t_eval)\n",
    "                pred_RMST_by_number_of_preexisting_conditions[sample_stratification_label].append(sample_predicted_rmst)\n",
    "\n",
    "                # Get the observed RMST - warning: this can never properly account for censoring\n",
    "                if e_test:\n",
    "                    # If outcome was observed\n",
    "                    sample_approx_obs_rmst = np.min((1, t_test[sample]))\n",
    "                else:\n",
    "                    sample_approx_obs_rmst = 1\n",
    "                obs_RMST_by_number_of_preexisting_conditions[sample_stratification_label].append(sample_approx_obs_rmst)\n",
    "\n",
    "        \n",
    "            ########################\n",
    "            # Get survival metrics #\n",
    "            ########################\n",
    "            df_surv = pd.DataFrame(np.transpose(surv_reduced), index=t_eval)\n",
    "            ev = EvalSurv(df_surv, t_test, lbls_test, censor_surv='km')             # Same treatment as in SurvivEHR\n",
    "            # Log overall scores\n",
    "            ctd.append(ev.concordance_td())\n",
    "            ibs.append(ev.integrated_brier_score(time_grid))\n",
    "            inbll.append(ev.integrated_nbll(time_grid))\n",
    "        \n",
    "            # print(f\"Scores up to sample {batch_idx+bsz}:\".ljust(50) + f\"Ctd: {np.mean(ctd):.3f}. IBS: {np.mean(ibs):.4f}. INBLL: {np.mean(inbll):.3f}\")\n",
    "        \n",
    "        ctd = np.mean(ctd)\n",
    "        ibs = np.mean(ibs)\n",
    "        inbll = np.mean(inbll)\n",
    "        \n",
    "        print(f\"{model_name}:\".ljust(20) + f\"N={sample_size}.\".ljust(15) + f\"Ctd: {ctd}. IBS: {ibs}. INBLL: {inbll}\")\n",
    "        model_names.append(model_name)\n",
    "        all_ctd.append(ctd)\n",
    "        all_ibs.append(ibs)\n",
    "        all_inbll.append(inbll)\n",
    "        all_pred_RMST.append(pred_RMST_by_number_of_preexisting_conditions)\n",
    "        all_obs_RMST.append(obs_RMST_by_number_of_preexisting_conditions)\n",
    "\n",
    "        # print(f\"\\tRandom Survival Forest ({'CR' if competing_risk else 'SR'}):\".ljust(20) + f\"N={sample_size}.\".ljust(15) + f\"Ctd: {ctd}. IBS: {ibs}. INBLL: {inbll}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20f5d3f1-8b77-4ecb-a589-8c617881802b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pred_RMST = [[np.mean(_i) if len(_i) > 0 else np.nan for _i in _pred_RMST] for _pred_RMST in all_pred_RMST]\n",
    "mean_obs_RMST = [[np.mean(_i) if len(_i) > 0 else np.nan for _i in _obs_RMST] for _obs_RMST in all_obs_RMST]\n",
    "\n",
    "num_pre_existing = np.arange(len(all_obs_RMST[0]))\n",
    "\n",
    "plt.close()\n",
    "for _mean_pred_RMST in mean_pred_RMST:\n",
    "    plt.plot(num_pre_existing[:10], _mean_pred_RMST[:10], color='b')\n",
    "    \n",
    "plt.plot(num_pre_existing[:10], mean_obs_RMST[0][:10], color='k')   # these are evaluated on the `all` the test data - which is shared across subsampled datasets \n",
    "plt.xlabel(\"Number of pre-existing conditions\")\n",
    "plt.ylabel(\"Survival time\")\n",
    "plt.savefig(\"calibration_rsf.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70edb77c-a46b-45c5-90c9-e0b046d9c930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model names: \n",
      "\t ['RandomSurvivalForest-SR-MultiMorbidity50+-Ns20000-seed1', 'RandomSurvivalForest-SR-MultiMorbidity50+-Ns20000-seed2', 'RandomSurvivalForest-SR-MultiMorbidity50+-Ns20000-seed3', 'RandomSurvivalForest-SR-MultiMorbidity50+-Ns20000-seed4', 'RandomSurvivalForest-SR-MultiMorbidity50+-Ns20000-seed5']\n",
      "\n",
      "Concordance (time-dependent): \n",
      "\t [0.5834264896399975, 0.5853854477013098, 0.5851260399538806, 0.5838205168142997, 0.5840655791806779]\n",
      "\n",
      "Integrated Brier Score: \n",
      "\t [0.15459947151900016, 0.15429969800844914, 0.15412524528024846, 0.1542414519560594, 0.154206618938596]\n",
      "\n",
      "INBLL: \n",
      "\t [0.46990139389813723, 0.4690151566676098, 0.46867952099052923, 0.46888269152341966, 0.46882119779176207]\n",
      "\n",
      "Naive observed RMST: \n",
      "\t [0.8224276887173136, 0.7990325691775028, 0.7849306412327886, 0.7729643687595084, 0.7592866692082013, 0.7325878823314543, 0.7207588323375635, 0.6876065383604424, 0.757309837544218, 0.6978854766258826]\n",
      "\n",
      "Predicted RMST:\n",
      "\t [0.798877246538506, 0.7751059552144429, 0.7619923814891338, 0.7483963312715685, 0.7315717945674856, 0.7130996259233288, 0.7013503368540817, 0.6908386636326109, 0.684355001664467, 0.6561782885516352]\n",
      "\t [0.7966658688703977, 0.7724005905643426, 0.7591017753660674, 0.7444357099298107, 0.7252300973768188, 0.7050902336002204, 0.6898616650573133, 0.68161036277018, 0.6735177776422248, 0.647405417338822]\n",
      "\t [0.7883498577599556, 0.7666469985565976, 0.7551163277749944, 0.7423532739961959, 0.7251831036916294, 0.7075527488653113, 0.6928072684037747, 0.6840235499872772, 0.6789949321860265, 0.6503548095911713]\n",
      "\t [0.793607041603463, 0.7710818457481804, 0.7604463837607597, 0.7456884883856281, 0.7271150565734984, 0.7045681728508075, 0.6868666021620541, 0.6744247279510839, 0.666460206963079, 0.6431398699065201]\n",
      "\t [0.7962048343574931, 0.7727071119339748, 0.7609104533546555, 0.7480487674596503, 0.7314286182411025, 0.7155995211947412, 0.7020947793310219, 0.694850689955102, 0.6901415052925994, 0.6684672113035478]\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nModel names: \\n\\t {model_names}\")\n",
    "print(f\"\\nConcordance (time-dependent): \\n\\t {all_ctd}\")\n",
    "print(f\"\\nIntegrated Brier Score: \\n\\t {all_ibs}\")\n",
    "print(f\"\\nINBLL: \\n\\t {all_inbll}\")\n",
    "print(f\"\\nNaive observed RMST: \\n\\t {mean_obs_RMST[0][:10]}\")\n",
    "print(f\"\\nPredicted RMST:\")\n",
    "for _mean_pred_RMST in mean_pred_RMST:\n",
    "    print(f\"\\t {_mean_pred_RMST[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca96c2f6-8b29-4853-9e34-12bd75e4be56",
   "metadata": {},
   "source": [
    "# Output across different setups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bee043-a472-4590-b431-4843cebb7ee7",
   "metadata": {},
   "source": [
    "Hypertension Single Risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6070f084-3565-49c3-9263-a537f1e90041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5843648146580331\n",
      "(0.5835322030735344, 0.5851974262425319, 0.0008326115844988198)\n",
      "0.15429449714047064\n",
      "(0.15411632696594102, 0.15447266731500026, 0.00017817017452961926)\n",
      "0.4690599921742916\n",
      "(0.46858412446163566, 0.46953585988694746, 0.0004758677126559015)\n"
     ]
    }
   ],
   "source": [
    "from statistics import NormalDist\n",
    "\n",
    "def confidence_interval(data, confidence=0.95):\n",
    "  dist = NormalDist.from_samples(data)\n",
    "  z = NormalDist().inv_cdf((1 + confidence) / 2.)\n",
    "  h = dist.stdev * z / ((len(data) - 1) ** .5)\n",
    "  return dist.mean - h, dist.mean + h, h\n",
    "\n",
    "data = [0.5834264896399975, 0.5853854477013098, 0.5851260399538806, 0.5838205168142997, 0.5840655791806779]\n",
    "print(np.mean(data))\n",
    "print(confidence_interval(data))\n",
    "\n",
    "data = [0.15459947151900016, 0.15429969800844914, 0.15412524528024846, 0.1542414519560594, 0.154206618938596]\n",
    "print(np.mean(data))\n",
    "print(confidence_interval(data))\n",
    "\n",
    "data =  [0.46990139389813723, 0.4690151566676098, 0.46867952099052923, 0.46888269152341966, 0.46882119779176207]\n",
    "print(np.mean(data))\n",
    "print(confidence_interval(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f87c29-0626-4108-9d94-a34ba132be5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "sys_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
