{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJpXpmjEYC_T"
   },
   "source": [
    "# Demo Notebook:\n",
    "## DeepHit - Hypertension\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added path '/rds/homes/g/gaddcz/Projects/CPRD/virtual-envTorch2.0-icelake/lib/python3.10/site-packages' at start of search paths.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "node_type = os.getenv('BB_CPU')\n",
    "venv_dir = f'/rds/homes/g/gaddcz/Projects/CPRD/virtual-envTorch2.0-{node_type}'\n",
    "venv_site_pkgs = Path(venv_dir) / 'lib' / f'python{sys.version_info.major}.{sys.version_info.minor}' / 'site-packages'\n",
    "if venv_site_pkgs.exists():\n",
    "    sys.path.insert(0, str(venv_site_pkgs))\n",
    "    print(f\"Added path '{venv_site_pkgs}' at start of search paths.\")\n",
    "else:\n",
    "    print(f\"Path '{venv_site_pkgs}' not found. Check that it exists and/or that it exists for node-type '{node_type}'.\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from CPRD.examples.modelling.benchmarks.make_method_loaders import get_dataloaders\n",
    "from CPRD.examples.modelling.benchmarks.DeepHit.train_deephit import run_experiment\n",
    "\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = \"cpu\"    # if more informative debugging statements are needed\n",
    "print(f\"Using device: {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training dataset from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/benchmark_data/N=9351_seed1.pickle\n",
      "Loading validation/test datasets from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/benchmark_data/all.pickle\n",
      "lr_finder best lr: 0.0001\n",
      "setting to lr: 0.001\n",
      "0:\t[3s / 3s],\t\ttrain_loss: 0.2911,\tval_loss: 0.2801\n",
      "1:\t[3s / 7s],\t\ttrain_loss: 0.2773,\tval_loss: 0.2715\n",
      "2:\t[3s / 11s],\t\ttrain_loss: 0.2645,\tval_loss: 0.2621\n",
      "3:\t[3s / 15s],\t\ttrain_loss: 0.2539,\tval_loss: 0.2559\n",
      "4:\t[3s / 18s],\t\ttrain_loss: 0.2457,\tval_loss: 0.2514\n",
      "5:\t[3s / 22s],\t\ttrain_loss: 0.2390,\tval_loss: 0.2502\n",
      "6:\t[3s / 26s],\t\ttrain_loss: 0.2338,\tval_loss: 0.2498\n",
      "7:\t[3s / 30s],\t\ttrain_loss: 0.2292,\tval_loss: 0.2504\n",
      "8:\t[3s / 34s],\t\ttrain_loss: 0.2257,\tval_loss: 0.2503\n",
      "9:\t[3s / 37s],\t\ttrain_loss: 0.2222,\tval_loss: 0.2527\n",
      "10:\t[3s / 41s],\t\ttrain_loss: 0.2190,\tval_loss: 0.2538\n",
      "11:\t[3s / 45s],\t\ttrain_loss: 0.2147,\tval_loss: 0.2567\n",
      "12:\t[3s / 48s],\t\ttrain_loss: 0.2123,\tval_loss: 0.2592\n",
      "13:\t[3s / 52s],\t\ttrain_loss: 0.2086,\tval_loss: 0.2594\n",
      "14:\t[3s / 56s],\t\ttrain_loss: 0.2066,\tval_loss: 0.2605\n",
      "15:\t[3s / 59s],\t\ttrain_loss: 0.2059,\tval_loss: 0.2640\n",
      "16:\t[3s / 1m:3s],\t\ttrain_loss: 0.2008,\tval_loss: 0.2651\n",
      "{'ctd': 0.5976818508410504, 'ibs': 0.034656694624276015, 'inbll': 0.1520562648292715}\n",
      "Loading training dataset from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/benchmark_data/N=9351_seed2.pickle\n",
      "Loading validation/test datasets from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/benchmark_data/all.pickle\n",
      "lr_finder best lr: 0.020092330025650584\n",
      "setting to lr: 0.01\n",
      "0:\t[2s / 2s],\t\ttrain_loss: 0.2588,\tval_loss: 0.2492\n",
      "1:\t[4s / 7s],\t\ttrain_loss: 0.2341,\tval_loss: 0.2484\n",
      "2:\t[1s / 8s],\t\ttrain_loss: 0.2273,\tval_loss: 0.2525\n",
      "3:\t[3s / 12s],\t\ttrain_loss: 0.2209,\tval_loss: 0.2523\n",
      "4:\t[1s / 13s],\t\ttrain_loss: 0.2149,\tval_loss: 0.2614\n",
      "5:\t[0s / 14s],\t\ttrain_loss: 0.2127,\tval_loss: 0.2614\n",
      "6:\t[3s / 17s],\t\ttrain_loss: 0.2086,\tval_loss: 0.2626\n",
      "7:\t[3s / 20s],\t\ttrain_loss: 0.2024,\tval_loss: 0.2663\n",
      "8:\t[3s / 24s],\t\ttrain_loss: 0.1986,\tval_loss: 0.2773\n",
      "9:\t[4s / 28s],\t\ttrain_loss: 0.1931,\tval_loss: 0.2824\n",
      "10:\t[0s / 29s],\t\ttrain_loss: 0.1955,\tval_loss: 0.2776\n",
      "11:\t[2s / 32s],\t\ttrain_loss: 0.1901,\tval_loss: 0.2835\n",
      "{'ctd': 0.5957035926271645, 'ibs': 0.033895934706036725, 'inbll': 0.14646253900933245}\n",
      "Loading training dataset from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/benchmark_data/N=9351_seed3.pickle\n",
      "Loading validation/test datasets from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/benchmark_data/all.pickle\n",
      "lr_finder best lr: 0.0001\n",
      "setting to lr: 0.001\n",
      "0:\t[2s / 2s],\t\ttrain_loss: 0.2783,\tval_loss: 0.2823\n",
      "1:\t[1s / 4s],\t\ttrain_loss: 0.2655,\tval_loss: 0.2735\n",
      "2:\t[5s / 9s],\t\ttrain_loss: 0.2511,\tval_loss: 0.2642\n",
      "3:\t[5s / 15s],\t\ttrain_loss: 0.2413,\tval_loss: 0.2571\n",
      "4:\t[3s / 18s],\t\ttrain_loss: 0.2310,\tval_loss: 0.2531\n",
      "5:\t[5s / 23s],\t\ttrain_loss: 0.2247,\tval_loss: 0.2504\n",
      "6:\t[3s / 27s],\t\ttrain_loss: 0.2185,\tval_loss: 0.2500\n",
      "7:\t[3s / 30s],\t\ttrain_loss: 0.2143,\tval_loss: 0.2499\n",
      "8:\t[2s / 32s],\t\ttrain_loss: 0.2119,\tval_loss: 0.2501\n",
      "9:\t[2s / 34s],\t\ttrain_loss: 0.2068,\tval_loss: 0.2508\n",
      "10:\t[5s / 40s],\t\ttrain_loss: 0.2031,\tval_loss: 0.2540\n",
      "11:\t[1s / 41s],\t\ttrain_loss: 0.1994,\tval_loss: 0.2540\n",
      "12:\t[4s / 46s],\t\ttrain_loss: 0.1984,\tval_loss: 0.2559\n",
      "13:\t[3s / 49s],\t\ttrain_loss: 0.1947,\tval_loss: 0.2559\n",
      "14:\t[11s / 1m:1s],\t\ttrain_loss: 0.1922,\tval_loss: 0.2568\n",
      "15:\t[7s / 1m:8s],\t\ttrain_loss: 0.1892,\tval_loss: 0.2614\n",
      "16:\t[0s / 1m:9s],\t\ttrain_loss: 0.1874,\tval_loss: 0.2635\n",
      "17:\t[9s / 1m:19s],\t\ttrain_loss: 0.1850,\tval_loss: 0.2647\n",
      "{'ctd': 0.5876146639781018, 'ibs': 0.03435783418452331, 'inbll': 0.15044463914067238}\n",
      "Loading training dataset from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/benchmark_data/N=9351_seed4.pickle\n",
      "Loading validation/test datasets from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/benchmark_data/all.pickle\n",
      "lr_finder best lr: 0.029150530628251937\n",
      "setting to lr: 0.01\n",
      "0:\t[3s / 3s],\t\ttrain_loss: 0.2614,\tval_loss: 0.2449\n",
      "1:\t[6s / 10s],\t\ttrain_loss: 0.2341,\tval_loss: 0.2445\n",
      "2:\t[2s / 12s],\t\ttrain_loss: 0.2288,\tval_loss: 0.2539\n",
      "3:\t[6s / 18s],\t\ttrain_loss: 0.2215,\tval_loss: 0.2487\n",
      "4:\t[11s / 30s],\t\ttrain_loss: 0.2156,\tval_loss: 0.2551\n",
      "5:\t[2s / 33s],\t\ttrain_loss: 0.2118,\tval_loss: 0.2542\n",
      "6:\t[5s / 38s],\t\ttrain_loss: 0.2085,\tval_loss: 0.2605\n",
      "7:\t[3s / 41s],\t\ttrain_loss: 0.2037,\tval_loss: 0.2637\n",
      "8:\t[1s / 43s],\t\ttrain_loss: 0.2008,\tval_loss: 0.2665\n",
      "9:\t[1s / 45s],\t\ttrain_loss: 0.1966,\tval_loss: 0.2652\n",
      "10:\t[3s / 49s],\t\ttrain_loss: 0.1958,\tval_loss: 0.2768\n",
      "11:\t[5s / 54s],\t\ttrain_loss: 0.1876,\tval_loss: 0.2755\n",
      "{'ctd': 0.5939509101764509, 'ibs': 0.03395375599102466, 'inbll': 0.14698375198665134}\n",
      "Loading training dataset from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/benchmark_data/N=9351_seed5.pickle\n",
      "Loading validation/test datasets from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/benchmark_data/all.pickle\n",
      "lr_finder best lr: 0.0001\n",
      "setting to lr: 0.001\n",
      "0:\t[2s / 2s],\t\ttrain_loss: 0.2846,\tval_loss: 0.2822\n",
      "1:\t[3s / 5s],\t\ttrain_loss: 0.2708,\tval_loss: 0.2717\n",
      "2:\t[0s / 6s],\t\ttrain_loss: 0.2601,\tval_loss: 0.2631\n",
      "3:\t[5s / 11s],\t\ttrain_loss: 0.2493,\tval_loss: 0.2560\n",
      "4:\t[6s / 18s],\t\ttrain_loss: 0.2416,\tval_loss: 0.2519\n",
      "5:\t[2s / 20s],\t\ttrain_loss: 0.2330,\tval_loss: 0.2501\n",
      "6:\t[0s / 21s],\t\ttrain_loss: 0.2288,\tval_loss: 0.2490\n",
      "7:\t[2s / 24s],\t\ttrain_loss: 0.2242,\tval_loss: 0.2507\n",
      "8:\t[4s / 29s],\t\ttrain_loss: 0.2226,\tval_loss: 0.2494\n",
      "9:\t[2s / 31s],\t\ttrain_loss: 0.2177,\tval_loss: 0.2516\n",
      "10:\t[1s / 32s],\t\ttrain_loss: 0.2150,\tval_loss: 0.2510\n",
      "11:\t[4s / 37s],\t\ttrain_loss: 0.2117,\tval_loss: 0.2529\n",
      "12:\t[4s / 42s],\t\ttrain_loss: 0.2078,\tval_loss: 0.2548\n",
      "13:\t[1s / 43s],\t\ttrain_loss: 0.2054,\tval_loss: 0.2568\n",
      "14:\t[2s / 46s],\t\ttrain_loss: 0.2031,\tval_loss: 0.2573\n",
      "15:\t[3s / 49s],\t\ttrain_loss: 0.1997,\tval_loss: 0.2610\n",
      "16:\t[6s / 56s],\t\ttrain_loss: 0.1986,\tval_loss: 0.2634\n",
      "{'ctd': 0.5717836325479015, 'ibs': 0.03422987411303425, 'inbll': 0.14915927290647582}\n"
     ]
    }
   ],
   "source": [
    "# Study params\n",
    "seeds = [1,2,3,4,5]\n",
    "sample_sizes = [9351]  # 2999, 5296, 9351, 16509, 29148, 51461, 90856, 160407, 283203, 500000,None\n",
    "bins=200\n",
    "\n",
    "model_names = []\n",
    "all_ctd = []\n",
    "all_ibs = []\n",
    "all_inbll = []\n",
    "for sample_size in sample_sizes:\n",
    "\n",
    "    seed_model_names = []\n",
    "    seed_ctd = []\n",
    "    seed_ibs = []\n",
    "    seed_inbll = []\n",
    "    for seed in seeds:\n",
    "\n",
    "        # Load data\n",
    "        dataset_train, dataset_val, dataset_test, meta_information = get_dataloaders(\"CVD\", False, \"deephit\", sample_size=sample_size, seed=seed, bins=bins)\n",
    "\n",
    "        # Train benchmark\n",
    "        result_dict = run_experiment(dataset_train, dataset_val, dataset_test, meta_information)\n",
    "        print(result_dict)\n",
    "    \n",
    "        # Record\n",
    "        seed_model_names.append(f\"DeepHit-SR-Hypertension-Ns{sample_size}-seed{seed}\")\n",
    "        seed_ctd.append(result_dict[\"ctd\"])\n",
    "        seed_ibs.append(result_dict[\"ibs\"])\n",
    "        seed_inbll.append(result_dict[\"inbll\"])\n",
    "\n",
    "    # Record\n",
    "    model_names.append(seed_model_names)\n",
    "    all_ctd.append(seed_ctd)\n",
    "    all_ibs.append(seed_ibs)\n",
    "    all_inbll.append(seed_inbll)\n",
    "        \n",
    "        # # Ensure loaded datasets align with original data\n",
    "        # num_xsectional_in_dims = dataset_train[0].shape[1]\n",
    "        # assert num_xsectional_in_dims == num_cov + num_context_tokens, f\"{num_xsectional_in_dims} != {num_cov} + {num_context_tokens}\"\n",
    "        # largest_dm_tokenizer_ind = dm.tokenizer._event_counts.shape[0]\n",
    "        # assert largest_dm_tokenizer_ind + num_cov -1 == num_xsectional_in_dims\n",
    "    \n",
    "        # # Train with off-the-shelf parameter setup\n",
    "        # net = tt.practical.MLPVanilla(num_xsectional_in_dims, num_nodes, bins, batch_norm, dropout)\n",
    "        # model = DeepHitSingle(net, tt.optim.Adam, alpha=0.2, sigma=0.1, duration_index=meta_information[\"cuts\"])\n",
    "    \n",
    "        # # Learning rate\n",
    "        # lr_finder = model.lr_finder(dataset_train[0], dataset_train[1], batch_size, tolerance=3)\n",
    "        # print(lr_finder.get_best_lr())\n",
    "        # #\n",
    "        # # Documentation states this over-estimates the best LR, so set it slightly smaller (as per example), but keep it within reasonable bounds\n",
    "        # lr_exponent = math.floor(math.log10(lr_finder.get_best_lr()))\n",
    "        # lr_exponent = min(max(lr_exponent, -3), -2)\n",
    "        # print(10**lr_exponent)\n",
    "        # model.optimizer.set_lr(10 ** lr_exponent)              \n",
    "    \n",
    "    \n",
    "        # callbacks = [tt.callbacks.EarlyStopping()]\n",
    "        # log = model.fit(dataset_train[0], dataset_train[1], batch_size, epochs, callbacks, val_data=dataset_val)\n",
    "        \n",
    "        # surv = model.predict_surv_df(dataset_test[0])\n",
    "    \n",
    "        # if False:\n",
    "        #     _ = log.plot()\n",
    "        #     plt.savefig(f\"figs/loss{seed}.png\")\n",
    "        \n",
    "        #     surv.iloc[:, :5].plot(drawstyle='steps-post')\n",
    "        #     plt.ylabel('S(t | x)')\n",
    "        #     _ = plt.xlabel('Time')\n",
    "        #     plt.savefig(f\"figs/step{seed}.png\")\n",
    "            \n",
    "        #     surv = model.interpolate(bins).predict_surv_df(dataset_test[0])\n",
    "            \n",
    "        #     surv.iloc[:, :5].plot(drawstyle='steps-post')\n",
    "        #     plt.ylabel('S(t | x)')\n",
    "        #     _ = plt.xlabel('Time')\n",
    "        #     plt.savefig(f\"figs/km{seed}.png\")\n",
    "    \n",
    "        # ###########################\n",
    "        # # Get RMST Survival times #\n",
    "        # ###########################              \n",
    "        # obs_RMST_by_number_of_preexisting_conditions = [[] for _ in range(len(encoded_conditions_xsec))]\n",
    "        # pred_RMST_by_number_of_preexisting_conditions = [[] for _ in range(len(encoded_conditions_xsec))]\n",
    "        # for sample in range(surv.shape[1]):\n",
    "        #     # Get the number of pre-existing conditions\n",
    "        #     sample_stratification_label = np.sum(dataset_test[0][sample][encoded_conditions_xsec] == 1)\n",
    "        #     # Get the RMST predicted under the survival curve\n",
    "        #     samples_below_cutoff = surv.index.to_numpy() < 1        # to ensure fair comparison to other methods, we evaluate up until standardised time 1\n",
    "        #     # \n",
    "        #     sample_predicted_rmst = trapz(surv[sample].to_numpy()[samples_below_cutoff], surv.index.to_numpy()[samples_below_cutoff])\n",
    "        #     pred_RMST_by_number_of_preexisting_conditions[sample_stratification_label].append(sample_predicted_rmst)\n",
    "            \n",
    "        #     if dataset_test[1][1][sample] != 0:\n",
    "        #         # Get the observed RMST - warning: this is IGNORING CENSORING\n",
    "        #         t_obs = dataset_test[1][0][sample]\n",
    "        #         obs_RMST_by_number_of_preexisting_conditions[sample_stratification_label].append(t_obs)\n",
    "    \n",
    "        # # Get metrics\n",
    "        # ev = EvalSurv(surv, dataset_test[1][0], dataset_test[1][1], censor_surv='km')\n",
    "        # ctd = ev.concordance_td()\n",
    "        # ibs = ev.integrated_brier_score(time_grid)\n",
    "        # inbll= ev.integrated_nbll(time_grid)\n",
    "    \n",
    "        # model_names.append(model_name)\n",
    "        # all_ctd.append(ctd)\n",
    "        # all_ibs.append(ibs)\n",
    "        # all_inbll.append(inbll)\n",
    "        # all_pred_RMST.append(pred_RMST_by_number_of_preexisting_conditions)\n",
    "        # all_obs_RMST.append(obs_RMST_by_number_of_preexisting_conditions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in data_loader_test:\n",
    "    x_train = batch[0]\n",
    "    print(batch[0].shape)\n",
    "    print(torch.mean(batch[1]))\n",
    "    print(batch[2])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model\n",
    "```\n",
    "Hypertension\n",
    "\n",
    "SR\n",
    "[0.768850848388541]\n",
    "[0.08287342061953376]\n",
    "[0.2629869386534056]\n",
    "\n",
    "CVD\n",
    "\n",
    "SR\n",
    "[0.6660374450461991]\n",
    "[0.03355264167471281]\n",
    "[0.14207857084831382]\n",
    "\n",
    "CR\n",
    "[0.6622136902603167]\n",
    "[0.033540062894645686]\n",
    "[0.142287892974895]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"Hypertension\" \n",
    "competing_risk = False\n",
    "# sample_sizes = [int(np.exp(_log_n)) for _log_n in np.linspace(np.log(3000), np.log(500000), 10)]      # [3000, 12500, 30000, 60000, 100000]: # 600, 1200, \n",
    "sample_sizes = [2999, 5296, 9351, 16509, 29148, 51461, 90856, 160407, 283203, 500000, None]\n",
    "sample_sizes = [None]\n",
    "# sample_sizes = [500000]\n",
    "# sample_sizes = sample_sizes[-1:]\n",
    "\n",
    "# training = True\n",
    "lr = 1e-3\n",
    "xdim = x_train.shape[1]\n",
    "\n",
    "\n",
    "# the time grid which we generate over\n",
    "t_eval = np.linspace(0, 1, 1000) \n",
    "# the time grid which we calculate scores over\n",
    "time_grid = np.linspace(start=0, stop=1 , num=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training dataset from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_Hypertension/benchmark_data/all.pickle\n",
      "(572096, 279)\n",
      "Loading validation/test datasets from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_Hypertension/benchmark_data/all.pickle\n",
      "\n",
      "\n",
      "DeSurv-Hypertension-NsNone-seed1 with 10081 parameters\n",
      "Training\n",
      "\tEpoch:  0. Total loss:   262342.64\n",
      "best_epoch: 0\n",
      "\tEpoch:  0. Total val loss:    14993.43\n",
      "\tEpoch:  1. Total loss:   247637.81\n",
      "best_epoch: 1\n",
      "\tEpoch:  1. Total val loss:    14772.03\n",
      "\tEpoch:  2. Total loss:   245857.33\n",
      "\tEpoch:  2. Total val loss:    14809.54\n",
      "\tEpoch:  3. Total loss:   244750.78\n",
      "best_epoch: 3\n",
      "\tEpoch:  3. Total val loss:    14764.87\n",
      "\tEpoch:  4. Total loss:   243993.23\n",
      "best_epoch: 4\n",
      "\tEpoch:  4. Total val loss:    14751.96\n",
      "\tEpoch:  5. Total loss:   243276.68\n",
      "best_epoch: 5\n",
      "\tEpoch:  5. Total val loss:    14706.59\n",
      "\tEpoch:  6. Total loss:   242696.47\n",
      "\tEpoch:  6. Total val loss:    14747.26\n",
      "\tEpoch:  7. Total loss:   242295.63\n",
      "\tEpoch:  7. Total val loss:    14729.99\n",
      "\tEpoch:  8. Total loss:   241866.22\n",
      "finished training\n",
      "Testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 140/140 [00:14<00:00,  9.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeSurv-Hypertension-NsNone-seed1:N=None.        Ctd: 0.7708035555642572. IBS: 0.08267526462773596. INBLL: 0.2625306123346124\n",
      "Loading training dataset from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_Hypertension/benchmark_data/all.pickle\n",
      "(572096, 279)\n",
      "Loading validation/test datasets from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_Hypertension/benchmark_data/all.pickle\n",
      "\n",
      "\n",
      "DeSurv-Hypertension-NsNone-seed2 with 10081 parameters\n",
      "Training\n",
      "\tEpoch:  0. Total loss:   264250.41\n",
      "best_epoch: 0\n",
      "\tEpoch:  0. Total val loss:    14995.57\n",
      "\tEpoch:  1. Total loss:   249065.96\n",
      "best_epoch: 1\n",
      "\tEpoch:  1. Total val loss:    14806.92\n",
      "\tEpoch:  2. Total loss:   246848.68\n",
      "\tEpoch:  2. Total val loss:    14929.38\n",
      "\tEpoch:  3. Total loss:   245412.42\n",
      "best_epoch: 3\n",
      "\tEpoch:  3. Total val loss:    14699.10\n",
      "\tEpoch:  4. Total loss:   244466.25\n",
      "\tEpoch:  4. Total val loss:    14812.86\n",
      "\tEpoch:  5. Total loss:   243831.36\n",
      "\tEpoch:  5. Total val loss:    14706.39\n",
      "\tEpoch:  6. Total loss:   243325.18\n",
      "finished training\n",
      "Testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 140/140 [00:14<00:00,  9.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeSurv-Hypertension-NsNone-seed2:N=None.        Ctd: 0.7695751200897563. IBS: 0.08273968181756607. INBLL: 0.262755230788484\n",
      "Loading training dataset from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_Hypertension/benchmark_data/all.pickle\n",
      "(572096, 279)\n",
      "Loading validation/test datasets from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_Hypertension/benchmark_data/all.pickle\n",
      "\n",
      "\n",
      "DeSurv-Hypertension-NsNone-seed3 with 10081 parameters\n",
      "Training\n",
      "\tEpoch:  0. Total loss:   263525.15\n",
      "best_epoch: 0\n",
      "\tEpoch:  0. Total val loss:    15057.56\n",
      "\tEpoch:  1. Total loss:   248587.60\n",
      "best_epoch: 1\n",
      "\tEpoch:  1. Total val loss:    14861.83\n",
      "\tEpoch:  2. Total loss:   246639.46\n",
      "best_epoch: 2\n",
      "\tEpoch:  2. Total val loss:    14745.99\n",
      "\tEpoch:  3. Total loss:   245216.52\n",
      "\tEpoch:  3. Total val loss:    14906.11\n",
      "\tEpoch:  4. Total loss:   244196.80\n",
      "best_epoch: 4\n",
      "\tEpoch:  4. Total val loss:    14715.75\n",
      "\tEpoch:  5. Total loss:   243366.60\n",
      "\tEpoch:  5. Total val loss:    14817.76\n",
      "\tEpoch:  6. Total loss:   242818.69\n",
      "\tEpoch:  6. Total val loss:    14751.50\n",
      "\tEpoch:  7. Total loss:   242379.21\n",
      "finished training\n",
      "Testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 140/140 [00:14<00:00,  9.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeSurv-Hypertension-NsNone-seed3:N=None.        Ctd: 0.7724139601735452. IBS: 0.08270569036836782. INBLL: 0.2625610527660121\n",
      "Loading training dataset from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_Hypertension/benchmark_data/all.pickle\n",
      "(572096, 279)\n",
      "Loading validation/test datasets from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_Hypertension/benchmark_data/all.pickle\n",
      "\n",
      "\n",
      "DeSurv-Hypertension-NsNone-seed4 with 10081 parameters\n",
      "Training\n",
      "\tEpoch:  0. Total loss:   265191.16\n",
      "best_epoch: 0\n",
      "\tEpoch:  0. Total val loss:    15200.15\n",
      "\tEpoch:  1. Total loss:   249490.30\n",
      "best_epoch: 1\n",
      "\tEpoch:  1. Total val loss:    14855.25\n",
      "\tEpoch:  2. Total loss:   246943.33\n",
      "best_epoch: 2\n",
      "\tEpoch:  2. Total val loss:    14765.10\n",
      "\tEpoch:  3. Total loss:   245495.67\n",
      "best_epoch: 3\n",
      "\tEpoch:  3. Total val loss:    14728.11\n",
      "\tEpoch:  4. Total loss:   244464.41\n",
      "best_epoch: 4\n",
      "\tEpoch:  4. Total val loss:    14718.48\n",
      "\tEpoch:  5. Total loss:   243684.61\n",
      "\tEpoch:  5. Total val loss:    14782.91\n",
      "\tEpoch:  6. Total loss:   243141.17\n",
      "\tEpoch:  6. Total val loss:    14799.96\n",
      "\tEpoch:  7. Total loss:   242588.22\n",
      "finished training\n",
      "Testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 140/140 [00:14<00:00,  9.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeSurv-Hypertension-NsNone-seed4:N=None.        Ctd: 0.7729993169568111. IBS: 0.08248514899737762. INBLL: 0.2621711782943788\n",
      "Loading training dataset from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_Hypertension/benchmark_data/all.pickle\n",
      "(572096, 279)\n",
      "Loading validation/test datasets from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_Hypertension/benchmark_data/all.pickle\n",
      "\n",
      "\n",
      "DeSurv-Hypertension-NsNone-seed5 with 10081 parameters\n",
      "Training\n",
      "\tEpoch:  0. Total loss:   266318.23\n",
      "best_epoch: 0\n",
      "\tEpoch:  0. Total val loss:    14920.48\n",
      "\tEpoch:  1. Total loss:   247918.10\n",
      "best_epoch: 1\n",
      "\tEpoch:  1. Total val loss:    14816.83\n",
      "\tEpoch:  2. Total loss:   245815.22\n",
      "best_epoch: 2\n",
      "\tEpoch:  2. Total val loss:    14786.59\n",
      "\tEpoch:  3. Total loss:   244699.37\n",
      "best_epoch: 3\n",
      "\tEpoch:  3. Total val loss:    14723.82\n",
      "\tEpoch:  4. Total loss:   243835.01\n",
      "best_epoch: 4\n",
      "\tEpoch:  4. Total val loss:    14681.30\n",
      "\tEpoch:  5. Total loss:   243240.67\n",
      "\tEpoch:  5. Total val loss:    14835.21\n",
      "\tEpoch:  6. Total loss:   242698.33\n",
      "\tEpoch:  6. Total val loss:    14708.85\n"
     ]
    }
   ],
   "source": [
    "model_names, all_ctd, all_ibs, all_inbll = [], [], [], []\n",
    "\n",
    "for sample_size in sample_sizes:\n",
    "\n",
    "    seeds = [1,2,3,4,5]\n",
    "\n",
    "    for seed in seeds:\n",
    "        # Load dataset\n",
    "        data_loader_train, data_loader_val, data_loader_test = get_dataloaders(dataset, competing_risk, sample_size=sample_size, seed=seed)\n",
    "    \n",
    "        # Initialise model\n",
    "        model_name = f\"DeSurv-{dataset}-Ns{sample_size}-seed{seed}\"\n",
    "        if competing_risk is False:\n",
    "            model = ODESurvSingle(xdim, [32, 32], device=device)\n",
    "        else:\n",
    "            model = ODESurvMultiple(xdim, [32, 32], num_risks=5)\n",
    "        print(f\"\\n\\n{model_name} with {sum(p.numel() for p in model.parameters() if p.requires_grad)} parameters\")\n",
    "\n",
    "        # Load or train model\n",
    "        torch.manual_seed(seed)\n",
    "        try:\n",
    "            state_dict = torch.load(\"outputs/Hypertension/\" + model_name + \"tst_model\")\n",
    "            model.load_state_dict(state_dict)\n",
    "            print(f\"Loaded previously trained model\")\n",
    "        except:\n",
    "            print(f\"Training\")\n",
    "            model.optimize(data_loader_train, n_epochs=20, logging_freq=1, data_loader_val=data_loader_val, max_wait=2)\n",
    "            print(\"finished training\")\n",
    "            torch.save(model.state_dict(), \"outputs/Hypertension/\" + model_name + \"tst_model\")\n",
    "            model.eval()\n",
    "    \n",
    "            # state_dict = torch.load(\"outputs/Hypertension/\" + model_name + \"tst_model\")\n",
    "            # model.load_state_dict(state_dict)\n",
    "           \n",
    "        \n",
    "        # argsortttest = np.argsort(t_test)\n",
    "        # t_test = t_test[argsortttest]\n",
    "        # e_test = e_test[argsortttest]\n",
    "        # x_test = x_test[argsortttest,:]\n",
    "        \n",
    "        print(f\"Testing\")    \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "    \n",
    "            ctd = []\n",
    "            ibs = []\n",
    "            inbll = []\n",
    "            for batch in tqdm(data_loader_test, total=(len(data_loader_test)), desc=\"Testing\"):\n",
    "    \n",
    "                x_test = batch[0].numpy()\n",
    "                t_test = batch[1].numpy()\n",
    "                e_test = batch[2].numpy()\n",
    "    \n",
    "                # The normalised grid over which to predict\n",
    "                t_test_grid = torch.tensor(np.concatenate([t_eval] * x_test.shape[0], 0), dtype=torch.float32)\n",
    "                x_test_grid = torch.tensor(x_test, dtype=torch.float32).repeat_interleave(t_eval.size, 0)\n",
    "                \n",
    "                pred_bsz = 51200\n",
    "                pred = []\n",
    "                for x_test_batched, t_test_batched in zip(torch.split(x_test_grid, pred_bsz), torch.split(t_test_grid, pred_bsz)):\n",
    "                    \n",
    "                    if competing_risk is False:\n",
    "                        pred_ = model.predict(x_test_batched, t_test_batched)          # shape: (x_test.batched.shape[0],)\n",
    "                    else:\n",
    "                        pred_, pi_  = model.predict(x_test_batched, t_test_batched)    # shape: (x_test.batched.shape[0], num_outcomes)\n",
    "                    pred.append(pred_)\n",
    "                        \n",
    "                pred = torch.concat(pred)\n",
    "            \n",
    "                pred = pred.reshape((x_test.shape[0], t_eval.size, -1)).cpu().detach().numpy()\n",
    "                preds = [pred[:, :, _i] for _i in range(pred.shape[-1])]\n",
    "                # print([_.shape for _ in preds])\n",
    "        \n",
    "                # Merge (additively) each outcome risk curve into a single CDF, and update label for if outcome occurred or not\n",
    "                cdf = np.zeros_like(preds[0])\n",
    "                lbls = np.zeros_like(e_test)     \n",
    "                for _outcome_token in np.unique(e_test)[1:]:\n",
    "                    # print(f\"{_outcome_token} of {np.unique(e_test)[1:]} included from {len(preds)} surv CDFs\")\n",
    "                    # print(_outcome_token)\n",
    "                    cdf += preds[_outcome_token - 1] \n",
    "                    lbls += (e_test == _outcome_token)\n",
    "                \n",
    "                surv = pd.DataFrame(np.transpose((1 - cdf.reshape((x_test.shape[0],t_eval.size)))), index=t_eval)\n",
    "                # print(surv)\n",
    "    \n",
    "                # Evaluate surv curve with unscaled index with unscaled test times to event \n",
    "                ev = EvalSurv(surv, t_test, lbls, censor_surv='km')\n",
    "                # try:\n",
    "                    # Same treatment as in SurvivEHR\n",
    "                ctd.append(ev.concordance_td())\n",
    "                ibs.append(ev.integrated_brier_score(time_grid))\n",
    "                inbll.append(ev.integrated_nbll(time_grid))\n",
    "                # print(f\"{ctd} {ibs}, {inbll}\")\n",
    "                # except:\n",
    "                #     pass\n",
    "                \n",
    "            ctd = np.mean(ctd)\n",
    "            ibs = np.mean(ibs)\n",
    "            inbll = np.mean(inbll)\n",
    "    \n",
    "            print(f\"{model_name}:\".ljust(20) + f\"N={sample_size}.\".ljust(15) + f\"Ctd: {ctd}. IBS: {ibs}. INBLL: {inbll}\")\n",
    "\n",
    "        model_names.append(model_name)\n",
    "        all_ctd.append(ctd)\n",
    "        all_ibs.append(ibs)\n",
    "        all_inbll.append(inbll)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_names)\n",
    "print(all_ctd)\n",
    "print(all_ibs)\n",
    "print(all_inbll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(surv.head())\n",
    "print(surv.shape)\n",
    "print(preds[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_sizes[-1] = 572096\n",
    "print(sample_sizes)\n",
    "print(all_ctd)\n",
    "print(all_ibs)\n",
    "print(all_inbll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output across different setups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypertension Single Risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypertension Competing Risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypertension_cr:N=2999.        Ctd: 0.6815094745489083. IBS: 0.09055914251601428. INBLL: 0.31029164842950396\n",
    "# Hypertension_cr:N=5296.        Ctd: 0.7124953181062986. IBS: 0.08950243267368668. INBLL: 0.2998138831024105\n",
    "# Hypertension_cr:N=9351.        Ctd: 0.7274689831170883. IBS: 0.08788381776138605. INBLL: 0.29219335002924224\n",
    "# Hypertension_cr:N=16509.       Ctd: 0.730682386913791. IBS: 0.08713170539614512. INBLL: 0.2915679214433027\n",
    "# Hypertension_cr:N=29148.       Ctd: 0.7389774246331557. IBS: 0.08589961516053798. INBLL: 0.2797340688171351\n",
    "# Hypertension_cr:N=51461.       Ctd: 0.7411341306621856. IBS: 0.08509218669879787. INBLL: 0.27232015638634294\n",
    "# Hypertension_cr:N=90856.       Ctd: 0.7515646895266107. IBS: 0.08443103008855017. INBLL: 0.27128357563201616\n",
    "# Hypertension_cr:N=160407.      Ctd: 0.7600540925292503. IBS: 0.08451878180772691. INBLL: 0.2692630746813842\n",
    "# Hypertension_cr:N=283203.      Ctd: 0.7620305063798369. IBS: 0.08404512042043363. INBLL: 0.2666228576617026\n",
    "# Hypertension_cr:N=500000.      Ctd: 0.761360566628242. IBS: 0.08371696663855675. INBLL: 0.26584980102348854"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cardiovascular disease Single Risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cardiovascular disease Competing Risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CVD_cr:          N=2999.        Ctd: 0.5625189849883911. IBS: 0.03435665860871859. INBLL: 0.14990771301641295\n",
    "# CVD_cr:          N=5296.        Ctd: 0.5869489534497859. IBS: 0.03411933684032459. INBLL: 0.14823684700350284\n",
    "# CVD_cr:          N=9351.        Ctd: 0.5993921885610577. IBS: 0.03442101009915632. INBLL: 0.14955773914208445\n",
    "# CVD_cr:          N=16509.       Ctd: 0.6101382510977712. IBS: 0.03387871579676817. INBLL: 0.14660058710195034\n",
    "# CVD_cr:          N=29148.       Ctd: 0.6200633894187596. IBS: 0.033806085396407184. INBLL: 0.1458525721893707\n",
    "# CVD_cr:          N=51461.       Ctd: 0.624818164536866. IBS: 0.033786555455045275. INBLL: 0.14535719048483275\n",
    "# CVD_cr:          N=90856.       Ctd: 0.6355840601226502. IBS: 0.03364728847608413. INBLL: 0.14414393636167722\n",
    "# CVD_cr:          N=160407.      Ctd: 0.6469820170508909. IBS: 0.033556625283347644. INBLL: 0.14308219809140393\n",
    "# CVD_cr:          N=283203.      Ctd: 0.6557505764681258. IBS: 0.033524955828030355. INBLL: 0.1423563785027365\n",
    "# CVD_cr:          N=500000.      Ctd: 0.6582288879213832. IBS: 0.033605688713984706. INBLL: 0.14273257727425628\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "sys_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
