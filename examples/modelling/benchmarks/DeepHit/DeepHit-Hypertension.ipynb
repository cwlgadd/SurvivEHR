{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJpXpmjEYC_T"
   },
   "source": [
    "# Demo Notebook:\n",
    "## DeepHit - Hypertension\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added path '/rds/homes/g/gaddcz/Projects/CPRD/virtual-envTorch2.0-icelake/lib/python3.10/site-packages' at start of search paths.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "node_type = os.getenv('BB_CPU')\n",
    "venv_dir = f'/rds/homes/g/gaddcz/Projects/CPRD/virtual-envTorch2.0-{node_type}'\n",
    "venv_site_pkgs = Path(venv_dir) / 'lib' / f'python{sys.version_info.major}.{sys.version_info.minor}' / 'site-packages'\n",
    "if venv_site_pkgs.exists():\n",
    "    sys.path.insert(0, str(venv_site_pkgs))\n",
    "    print(f\"Added path '{venv_site_pkgs}' at start of search paths.\")\n",
    "else:\n",
    "    print(f\"Path '{venv_site_pkgs}' not found. Check that it exists and/or that it exists for node-type '{node_type}'.\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from CPRD.examples.modelling.benchmarks.make_method_loaders import get_dataloaders\n",
    "from CPRD.examples.modelling.benchmarks.DeepHit.train_deephit import run_experiment\n",
    "\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = \"cpu\"    # if more informative debugging statements are needed\n",
    "print(f\"Using device: {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Seed 4, sample size None, learning_rate 0.001\n",
      "Loading training dataset from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_Hypertension/benchmark_data/all.pickle\n",
      "Loading validation/test datasets from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_Hypertension/benchmark_data/all.pickle\n",
      "0:\t[8s / 8s],\t\ttrain_loss: 0.3298,\tval_loss: 0.3227\n",
      "1:\t[8s / 17s],\t\ttrain_loss: 0.3207,\tval_loss: 0.3223\n",
      "2:\t[8s / 26s],\t\ttrain_loss: 0.3200,\tval_loss: 0.3216\n",
      "3:\t[8s / 34s],\t\ttrain_loss: 0.3196,\tval_loss: 0.3216\n",
      "4:\t[8s / 43s],\t\ttrain_loss: 0.3192,\tval_loss: 0.3217\n",
      "5:\t[8s / 51s],\t\ttrain_loss: 0.3190,\tval_loss: 0.3217\n",
      "6:\t[8s / 1m:0s],\t\ttrain_loss: 0.3189,\tval_loss: 0.3216\n",
      "7:\t[8s / 1m:9s],\t\ttrain_loss: 0.3187,\tval_loss: 0.3218\n",
      "8:\t[8s / 1m:17s],\t\ttrain_loss: 0.3185,\tval_loss: 0.3218\n",
      "9:\t[8s / 1m:26s],\t\ttrain_loss: 0.3183,\tval_loss: 0.3221\n",
      "10:\t[8s / 1m:34s],\t\ttrain_loss: 0.3183,\tval_loss: 0.3216\n",
      "11:\t[8s / 1m:43s],\t\ttrain_loss: 0.3181,\tval_loss: 0.3219\n",
      "12:\t[8s / 1m:52s],\t\ttrain_loss: 0.3180,\tval_loss: 0.3217\n",
      "13:\t[8s / 2m:0s],\t\ttrain_loss: 0.3179,\tval_loss: 0.3219\n",
      "{'ctd': 0.7567114353936188, 'ibs': 0.08493137618812542, 'inbll': 0.2708546666342921}\n",
      "\n",
      "Seed 5, sample size None, learning_rate 0.001\n",
      "Loading training dataset from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_Hypertension/benchmark_data/all.pickle\n",
      "Loading validation/test datasets from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_Hypertension/benchmark_data/all.pickle\n",
      "0:\t[8s / 8s],\t\ttrain_loss: 0.3298,\tval_loss: 0.3227\n",
      "1:\t[8s / 17s],\t\ttrain_loss: 0.3209,\tval_loss: 0.3221\n",
      "2:\t[8s / 25s],\t\ttrain_loss: 0.3200,\tval_loss: 0.3216\n",
      "3:\t[8s / 34s],\t\ttrain_loss: 0.3196,\tval_loss: 0.3214\n",
      "4:\t[8s / 43s],\t\ttrain_loss: 0.3193,\tval_loss: 0.3219\n",
      "5:\t[8s / 52s],\t\ttrain_loss: 0.3190,\tval_loss: 0.3214\n",
      "6:\t[8s / 1m:0s],\t\ttrain_loss: 0.3188,\tval_loss: 0.3229\n",
      "7:\t[8s / 1m:9s],\t\ttrain_loss: 0.3187,\tval_loss: 0.3213\n",
      "8:\t[8s / 1m:18s],\t\ttrain_loss: 0.3185,\tval_loss: 0.3216\n",
      "9:\t[8s / 1m:26s],\t\ttrain_loss: 0.3184,\tval_loss: 0.3211\n",
      "10:\t[8s / 1m:35s],\t\ttrain_loss: 0.3182,\tval_loss: 0.3215\n",
      "11:\t[8s / 1m:44s],\t\ttrain_loss: 0.3181,\tval_loss: 0.3215\n",
      "12:\t[9s / 1m:53s],\t\ttrain_loss: 0.3180,\tval_loss: 0.3212\n",
      "13:\t[8s / 2m:1s],\t\ttrain_loss: 0.3179,\tval_loss: 0.3221\n",
      "14:\t[8s / 2m:10s],\t\ttrain_loss: 0.3178,\tval_loss: 0.3218\n",
      "15:\t[8s / 2m:19s],\t\ttrain_loss: 0.3178,\tval_loss: 0.3225\n",
      "16:\t[8s / 2m:27s],\t\ttrain_loss: 0.3176,\tval_loss: 0.3216\n",
      "17:\t[8s / 2m:36s],\t\ttrain_loss: 0.3176,\tval_loss: 0.3224\n",
      "18:\t[8s / 2m:45s],\t\ttrain_loss: 0.3175,\tval_loss: 0.3223\n",
      "19:\t[8s / 2m:54s],\t\ttrain_loss: 0.3174,\tval_loss: 0.3227\n",
      "{'ctd': 0.7624806391527287, 'ibs': 0.08515887960197109, 'inbll': 0.2722541784393107}\n"
     ]
    }
   ],
   "source": [
    "# Study params\n",
    "seeds = [4,5]\n",
    "sample_sizes = [None]  # 2999, 5296, 9351, 16509, 29148, 51461, 90856, 160407, 283203, 500000, None\n",
    "bins=200\n",
    "\n",
    "model_names = []\n",
    "all_ctd = []\n",
    "all_ibs = []\n",
    "all_inbll = []\n",
    "for sample_size in sample_sizes:\n",
    "\n",
    "    seed_model_names = []\n",
    "    seed_ctd = []\n",
    "    seed_ibs = []\n",
    "    seed_inbll = []\n",
    "    for seed in seeds:\n",
    "        for lr in [0.001]:\n",
    "            print(f\"\\nSeed {seed}, sample size {sample_size}, learning_rate {lr}\")\n",
    "    \n",
    "            # Seed\n",
    "            torch.manual_seed(seed)\n",
    "    \n",
    "            # Load data\n",
    "            dataset_train, dataset_val, dataset_test, meta_information = get_dataloaders(\"Hypertension\", False, \"deephit\", sample_size=sample_size, seed=seed, bins=bins)\n",
    "    \n",
    "            # Train benchmark\n",
    "            result_dict = run_experiment(dataset_train, dataset_val, dataset_test, meta_information, learning_rate=lr)\n",
    "            print(result_dict)\n",
    "        \n",
    "            # Record\n",
    "            seed_model_names.append(f\"DeepHit-SR-Hypertension-Ns{sample_size}-seed{seed}-lr{lr}\")\n",
    "            seed_ctd.append(result_dict[\"ctd\"])\n",
    "            seed_ibs.append(result_dict[\"ibs\"])\n",
    "            seed_inbll.append(result_dict[\"inbll\"])\n",
    "\n",
    "    # Record\n",
    "    model_names.append(seed_model_names)\n",
    "    all_ctd.append(seed_ctd)\n",
    "    all_ibs.append(seed_ibs)\n",
    "    all_inbll.append(seed_inbll)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_names)\n",
    "print(all_ctd)\n",
    "print(all_ibs)\n",
    "print(all_inbll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in data_loader_test:\n",
    "    x_train = batch[0]\n",
    "    print(batch[0].shape)\n",
    "    print(torch.mean(batch[1]))\n",
    "    print(batch[2])\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "sys_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
