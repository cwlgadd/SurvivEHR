is_decoder: True

data:
  batch_size: 64                    # Batch size
  unk_freq_threshold: 0.0           # Frequency required for token to be included. Tokens below threshold are transformed to <UNK>
  min_workers: 20                   # The number of workers to create for dataloader
  global_diagnoses: False           # Whether we enforce diagnoses which occurred before context block to prepend the block
  path_to_db: /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/                         # Path to SQL database
  path_to_ds: /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/                # Path to Polars dataset
  # Path to meta_information. This is created when building datasets. It can then be modified, for example here we updated the default outlier bounds to custom values
  meta_information_path: /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/meta_information_QuantJenny.pickle
  

experiment:
  type: 'pre-train'                  # Options: ['pre-train', 'zero-shot', 'fine-tune']
  project_name: SurvStreamGPT_${head.SurvLayer}
  run_id: PreTrain_${head.SurvLayer}_11M_${experiment.seed}
  train: True
  test: True
  verbose: True
  seed: 1337
  log: True
  log_dir: /rds/projects/s/subramaa-mum-predict/CharlesGadd_Oxford/FoundationModelOutput/                    # Where to place any log files
  ckpt_dir: /rds/projects/s/subramaa-mum-predict/CharlesGadd_Oxford/FoundationModelOutput/checkpoints/       # Where to save checkpoints during training 
  # If we are evaluating the pre-trained model (with zero-shot,or fine-tuning), which outcomes do we consider
  fine_tune_outcomes: None                                                     
  
optim: 
  num_epochs: 1                      # Number of batches. By default we train on each patient one time
  learning_rate: 3e-4                # AdamW learning rate (initial)
  scheduler: CAWarmRestarts          # Type of scheduler to use (CAWarmRestarts, CosineAnnealingLR, default: ReduceLROnPlateau)
  scheduler_periods: 5000            # If scheduler has periodicity (i.e. how long is warmup, how long between warmups in CA)
  scheduler_warmup: True             # Whether to add warmup to learning rate scheduler 
  lr_cosine_decay_period: 1e7        # (after warm up) how long to decay learning rate over
  val_check_interval: 1000           # How many training batches to perform between validation check
  early_stop: False                  # If we should do early stopping
  early_stop_patience: 5             # How many val_check intervals to continue without seeing an improvement
  log_every_n_steps: 20              # Logging frequency
  limit_val_batches: 0.05            # The fraction of patients we validate on each time (out of entire validation set)
  limit_test_batches: 0.05           # The fraction of patients we test on each time (out of entire test set)

transformer:
  block_type: "Neo"                  # Architecture (Neo, or Nano)
  block_size: 128                    # what is the maximum context length for predictions?
  n_layer: 6                         # Number of MHA layers
  n_head: 6                          # Number of attention heads
  n_embd: 384                        # Embedding dimension
  layer_norm_bias: False             # Whether to add bias to layer norm (inspired by gpt 2 having = False)
  attention_type: "global"           # Type of attention. Neo: ["global", "local"], Nano ["global"]
  bias: True                         # Bias in the MLP layer inside the blocks
  dropout: 0.0                       # Dropout before attention blocks, and after MLP layer inside blocks
  attention_dropout: 0.0             # Dropout inside of attention
  resid_dropout: 0.0                 # Dropout in residual connection

head:
  # survival head
  SurvLayer: "cr"                    # Survival head type, ["competing-risk" | "cr"] or ["single-risk" | "sr"]
  surv_weight: 0.5                   # How much weight should be given to this aspect of the head
  # value head
  tokens_for_univariate_regression: None   # These are the tokens for which we create a value prediction head. This must be set after tokenization has been initialised
  value_weight: 0.5                  # How much weight should be given to this aspect of the head
