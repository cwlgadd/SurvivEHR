{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJpXpmjEYC_T"
   },
   "source": [
    "# Demo Notebook:\n",
    "## Competing Risk Survival Transformer For Causal Sequence Modelling \n",
    "\n",
    "Including time, tabular values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added path '/rds/homes/g/gaddcz/Projects/CPRD/virtual-envTorch2.0-icelake/lib/python3.10/site-packages' at start of search paths.\n",
      "/rds/homes/g/gaddcz/Projects/CPRD/examples/modelling/SurvStreamGPT/notebooks/CompetingRisk\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "node_type = os.getenv('BB_CPU')\n",
    "venv_dir = f'/rds/homes/g/gaddcz/Projects/CPRD/virtual-envTorch2.0-{node_type}'\n",
    "venv_site_pkgs = Path(venv_dir) / 'lib' / f'python{sys.version_info.major}.{sys.version_info.minor}' / 'site-packages'\n",
    "if venv_site_pkgs.exists():\n",
    "    sys.path.insert(0, str(venv_site_pkgs))\n",
    "    print(f\"Added path '{venv_site_pkgs}' at start of search paths.\")\n",
    "else:\n",
    "    print(f\"Path '{venv_site_pkgs}' not found. Check that it exists and/or that it exists for node-type '{node_type}'.\")\n",
    "\n",
    "!pwd\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from pycox.evaluation import EvalSurv\n",
    "from tqdm import tqdm\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "from CPRD.examples.modelling.SurvStreamGPT.experiment import run\n",
    "from CPRD.data.foundational_loader import FoundationalDataModule\n",
    "from CPRD.src.models.survival.task_heads.causal import SurvStreamGPTForCausalModelling\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = \"cpu\"    # if more informative debugging statements are needed\n",
    "print(f\"Using device: {device}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo Version of SurvStreamGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the configuration file, override any settings \n",
    "with initialize(version_base=None, config_path=\"../../confs\", job_name=\"testing_notebook\"):\n",
    "    cfg = compose(config_name=\"config_CompetingRisk11M\", overrides=[])\n",
    "\n",
    "\n",
    "# cfg.data.batch_size = 16\n",
    "# cfg.transformer.block_size = 32\n",
    "# # cfg.transformer.n_layer = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_decoder: true\n",
      "data:\n",
      "  batch_size: 64\n",
      "  unk_freq_threshold: 0.0\n",
      "  min_workers: 20\n",
      "  global_diagnoses: false\n",
      "  meta_information_path: /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/polars/meta_information.pickle\n",
      "experiment:\n",
      "  project_name: SurvStreamGPT_${head.SurvLayer}\n",
      "  run_id: PreTrain_${head.SurvLayer}_11M_${experiment.seed}\n",
      "  train: true\n",
      "  test: true\n",
      "  verbose: true\n",
      "  seed: 1337\n",
      "  log: true\n",
      "  log_dir: /rds/projects/s/subramaa-mum-predict/CharlesGadd_Oxford/FoundationModelOutput/\n",
      "  ckpt_dir: /rds/projects/s/subramaa-mum-predict/CharlesGadd_Oxford/FoundationModelOutput/checkpoints/\n",
      "optim:\n",
      "  num_epochs: 1\n",
      "  learning_rate: 0.0003\n",
      "  val_check_interval: 1000\n",
      "  early_stop: false\n",
      "  early_stop_patience: 5\n",
      "  log_every_n_steps: 20\n",
      "  limit_val_batches: 0.05\n",
      "  limit_test_batches: 0.05\n",
      "transformer:\n",
      "  block_type: Neo\n",
      "  block_size: 128\n",
      "  n_layer: 6\n",
      "  n_head: 6\n",
      "  n_embd: 384\n",
      "  layer_norm_bias: false\n",
      "  attention_type: global\n",
      "  bias: true\n",
      "  dropout: 0.0\n",
      "  attention_dropout: 0.0\n",
      "  resid_dropout: 0.0\n",
      "head:\n",
      "  SurvLayer: cr\n",
      "  surv_weight: 0.5\n",
      "  tokens_for_univariate_regression: None\n",
      "  value_weight: 0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: SLURM_NTASKS_PER_NODE=28\n"
     ]
    }
   ],
   "source": [
    " # TODO: define an env variable to fix for a local hpc environment issue, this shouldn't be needed\n",
    "%env SLURM_NTASKS_PER_NODE=28      \n",
    "\n",
    "# TODO: with above this trains, but due to widgets issue on hpc it does not print progress to notebook\n",
    "# cfg.experiment.train = False\n",
    "# cfg.experiment.test = False\n",
    "# cfg.experiment.log = False\n",
    "# model, dm = run(cfg)     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or define training process by hand\n",
    "### Create data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using meta information from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/polars/meta_information.pickle\n",
      "INFO:root:Using train file-row count dictionary from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/polars/file_row_count_dict_train.pickle\n",
      "INFO:root:Using test file-row count dictionary from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/polars/file_row_count_dict_test.pickle\n",
      "INFO:root:Using val file-row count dictionary from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/polars/file_row_count_dict_val.pickle\n",
      "INFO:root:Tokenzier created based on 3584.43M tokens\n",
      "INFO:root:Using tabular tokenizer, created from meta information and containing 184 tokens\n",
      "INFO:root:Loaded /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/polars/split=train/ dataset, with 23,343,104 samples\n",
      "INFO:root:Loaded /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/polars/split=test/ dataset, with 1,263,168 samples\n",
      "INFO:root:Loaded /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/polars/split=val/ dataset, with 1,233,601 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184 vocab elements\n"
     ]
    }
   ],
   "source": [
    "# Build \n",
    "path_to_db = \"/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/\"\n",
    "dm = FoundationalDataModule(path_to_db=path_to_db,\n",
    "                            load=True,\n",
    "                            tokenizer=\"tabular\",\n",
    "                            batch_size=cfg.data.batch_size,\n",
    "                            max_seq_length=cfg.transformer.block_size,\n",
    "                            global_diagnoses=cfg.data.global_diagnoses,\n",
    "                            freq_threshold=cfg.data.unk_freq_threshold,\n",
    "                            min_workers=cfg.data.min_workers,\n",
    "                            overwrite_meta_information=cfg.data.meta_information_path,\n",
    "                           )\n",
    "\n",
    "vocab_size = dm.train_set.tokenizer.vocab_size\n",
    "print(f\"{vocab_size} vocab elements\")\n",
    "\n",
    "# list of univariate measurements to model with Normal distribution\n",
    "# Extract the measurements, using the fact that the diagnoses are all up upper case.\n",
    "measurements_for_univariate_regression = [record for record in dm.tokenizer._event_counts[\"EVENT\"] if record.upper() != record]\n",
    "cfg.head.tokens_for_univariate_regression = dm.encode(measurements_for_univariate_regression) \n",
    "# display(measurements_for_univariate_regression)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dm.train_set.view_sample(1000, report_time=True) # max_dynamic_events=120,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hoelkOrFY8bN",
    "outputId": "961304cd-e379-40d4-dd56-8de0b91d2861",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = SurvStreamGPTForCausalModelling(cfg, vocab_size).to(device)\n",
    "model = model.to(device)\n",
    "\n",
    "loss_curves_train = []\n",
    "loss_curves_train_surv = []\n",
    "loss_curves_train_values = []\n",
    "\n",
    "loss_curves_val = []\n",
    "loss_curves_val_surv = []\n",
    "loss_curves_val_values = []    \n",
    "print(f\"Training model with {sum(p.numel() for p in model.parameters())/1e6} M parameters\")\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.optim.learning_rate)\n",
    "\n",
    "best_val, epochs_since_best = np.inf, 0\n",
    "for epoch in range(10):\n",
    "    \n",
    "    epoch_loss, epoch_surv_loss, epoch_values_loss = 0, 0, 0\n",
    "    model.train()\n",
    "    for i, batch in tqdm(enumerate(dm.train_dataloader()), desc=f\"Training epoch {epoch}\", total=len(dm.train_dataloader())):\n",
    "        \n",
    "            \n",
    "        # evaluate the loss\n",
    "        _, loss_dict, hidden_states = model(tokens=batch['tokens'].to(device),\n",
    "                                ages=batch['ages'].to(device),\n",
    "                                values=batch['values'].to(device),\n",
    "                                covariates=batch[\"static_covariates\"].to(device),\n",
    "                                attention_mask=batch['attention_mask'].to(device)\n",
    "                               )\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss_dict[\"loss\"].backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # record\n",
    "        epoch_loss += loss_dict[\"loss\"].item()            \n",
    "        epoch_surv_loss += loss_dict[\"loss_desurv\"].item()\n",
    "        epoch_values_loss += loss_dict[\"loss_values\"].item()\n",
    "\n",
    "        if i > 1000:\n",
    "            break\n",
    "    \n",
    "    epoch_loss /= i\n",
    "    epoch_surv_loss /= i\n",
    "    epoch_values_loss /= i\n",
    "    loss_curves_train.append(epoch_loss)\n",
    "    loss_curves_train_surv.append(epoch_surv_loss)\n",
    "    loss_curves_train_values.append(epoch_values_loss)\n",
    "\n",
    "    # evaluate the loss on val set\n",
    "    with torch.no_grad(): \n",
    "        model.eval()\n",
    "        if epoch % 1 == 0 or epoch == cfg.optim.num_epochs - 1:\n",
    "            val_loss, val_surv_loss, val_values_loss = 0, 0, 0\n",
    "            for j, batch in tqdm(enumerate(dm.val_dataloader()), desc=f\"Validation epoch {epoch}\", total=len(dm.val_dataloader())):\n",
    "                if j > 100:\n",
    "                    break\n",
    "                _, loss_dict, _ = model(tokens=batch['tokens'].to(device),\n",
    "                                        ages=batch['ages'].to(device),\n",
    "                                        values=batch['values'].to(device),\n",
    "                                        covariates=batch[\"static_covariates\"].to(device),\n",
    "                                        attention_mask=batch['attention_mask'].to(device)\n",
    "                                       )\n",
    "                # record\n",
    "                val_loss += loss_dict[\"loss\"].item()                    \n",
    "                val_surv_loss += loss_dict[\"loss_desurv\"].item()\n",
    "                val_values_loss += loss_dict[\"loss_values\"].item()\n",
    "                \n",
    "            val_loss /= j\n",
    "            val_surv_loss /= j\n",
    "            val_values_loss /= j\n",
    "            loss_curves_val.append(val_loss)\n",
    "            loss_curves_val_surv.append(val_surv_loss)\n",
    "            loss_curves_val_values.append(val_values_loss)\n",
    "\n",
    "            print(f\"Epoch {epoch}:\\tTrain loss {epoch_loss:.2f}: ({epoch_surv_loss:.2f}, {epoch_values_loss:.2f}). Val loss {val_loss:.2f}: ({val_surv_loss:.2f}, {val_values_loss:.2f})\")          \n",
    "            # TODO: Note not fully accurate as last batch is likely not the same size, will be fixed with lightning\n",
    "\n",
    "        if val_loss >= best_val:\n",
    "            epochs_since_best += 1\n",
    "            if epochs_since_best >= 5:\n",
    "                break\n",
    "        else:\n",
    "            best_val = val_loss\n",
    "            epochs_since_best = 0\n",
    "\n",
    "            # Save best seen model\n",
    "            # torch.save(model.state_dict(), path_to_db + \"polars/CR.pt\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss\n",
    "plt.figure()\n",
    "# Training\n",
    "iterations = np.linspace(0, len(loss_curves_train), len(loss_curves_train)) * opt.eval_interval\n",
    "plt.plot(iterations, loss_curves_train, label=\"train\")\n",
    "# Validation\n",
    "iterations = np.linspace(0, len(loss_curves_val), len(loss_curves_val)) * opt.eval_interval\n",
    "plt.plot(iterations, loss_curves_val, label=\"val\", linestyle='dashed')\n",
    "plt.legend()\n",
    "plt.savefig(f\"figs/loss.png\")\n",
    "\n",
    "# Plot DeSurv loss\n",
    "plt.figure()\n",
    "# Training\n",
    "iterations = np.linspace(0, len(loss_curves_train_surv), len(loss_curves_train_surv)) * opt.eval_interval\n",
    "plt.plot(iterations, loss_curves_train_surv, label=\"train\")\n",
    "# Validation\n",
    "iterations = np.linspace(0, len(loss_curves_val_surv), len(loss_curves_val_surv)) * opt.eval_interval\n",
    "plt.plot(iterations, loss_curves_val_surv, label=\"val\", linestyle='dashed')\n",
    "plt.legend()\n",
    "plt.savefig(f\"figs/loss_desurv.png\")\n",
    "\n",
    "# Plot value loss\n",
    "plt.figure()\n",
    "# Training\n",
    "iterations = np.linspace(0, len(loss_curves_train_values), len(loss_curves_train_values)) * opt.eval_interval\n",
    "plt.plot(iterations, loss_curves_train_values, label=\"train\", )\n",
    "# Validation\n",
    "iterations = np.linspace(0, len(loss_curves_val_values), len(loss_curves_val_values)) * opt.eval_interval\n",
    "plt.plot(iterations, loss_curves_val_values, label=\"val\", linestyle='dashed')\n",
    "plt.legend()\n",
    "plt.savefig(f\"figs/loss_val.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: model architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to html --no-input competing_risk.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "sys_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
