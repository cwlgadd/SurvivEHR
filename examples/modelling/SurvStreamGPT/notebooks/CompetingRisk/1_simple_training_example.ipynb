{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJpXpmjEYC_T"
   },
   "source": [
    "# Demo Notebook:\n",
    "## A simple notebook example of training the Competing Risk Survival Transformer For Causal Sequence Modelling.\n",
    "\n",
    "Note! This is for demonstrative purposes - the CausalExperiment class provides significantly more versatility. Refer to: \n",
    "\n",
    "```bash\n",
    "    examples/modelling/SurvStreamGPT/setup_causal_experiment.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added path '/rds/homes/g/gaddcz/Projects/CPRD/virtual-envTorch2.0-icelake/lib/python3.10/site-packages' at start of search paths.\n",
      "/rds/homes/g/gaddcz/Projects/CPRD/examples/modelling/SurvStreamGPT/notebooks/CompetingRisk\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "node_type = os.getenv('BB_CPU')\n",
    "venv_dir = f'/rds/homes/g/gaddcz/Projects/CPRD/virtual-envTorch2.0-{node_type}'\n",
    "venv_site_pkgs = Path(venv_dir) / 'lib' / f'python{sys.version_info.major}.{sys.version_info.minor}' / 'site-packages'\n",
    "if venv_site_pkgs.exists():\n",
    "    sys.path.insert(0, str(venv_site_pkgs))\n",
    "    print(f\"Added path '{venv_site_pkgs}' at start of search paths.\")\n",
    "else:\n",
    "    print(f\"Path '{venv_site_pkgs}' not found. Check that it exists and/or that it exists for node-type '{node_type}'.\")\n",
    "\n",
    "!pwd\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from pycox.evaluation import EvalSurv\n",
    "from tqdm import tqdm\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "# from CPRD.examples.modelling.SurvStreamGPT.run_experiment import run\n",
    "from CPRD.data.foundational_loader import FoundationalDataModule\n",
    "from CPRD.src.models.survival.task_heads.causal import SurvStreamGPTForCausalModelling\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = \"cpu\"    # if more informative debugging statements are needed\n",
    "print(f\"Using device: {device}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo Version of SurvStreamGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the configuration file, override any settings \n",
    "with initialize(version_base=None, config_path=\"../../confs\", job_name=\"testing_notebook\"):\n",
    "    cfg = compose(config_name=\"config_CompetingRisk11M\", overrides=[])\n",
    "\n",
    "\n",
    "# cfg.data.batch_size = 16\n",
    "# cfg.transformer.block_size = 32\n",
    "# # cfg.transformer.n_layer = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_decoder: true\n",
      "data:\n",
      "  batch_size: 64\n",
      "  unk_freq_threshold: 0.0\n",
      "  min_workers: 20\n",
      "  global_diagnoses: false\n",
      "  path_to_db: /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/\n",
      "  path_to_ds: /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/\n",
      "  meta_information_path: /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/meta_information_QuantJenny.pickle\n",
      "experiment:\n",
      "  type: pre-train\n",
      "  project_name: SurvStreamGPT_${head.SurvLayer}\n",
      "  run_id: PreTrain_${head.SurvLayer}_11M_${experiment.seed}\n",
      "  train: true\n",
      "  test: true\n",
      "  verbose: true\n",
      "  seed: 1337\n",
      "  log: true\n",
      "  log_dir: /rds/projects/s/subramaa-mum-predict/CharlesGadd_Oxford/FoundationModelOutput/\n",
      "  ckpt_dir: /rds/projects/s/subramaa-mum-predict/CharlesGadd_Oxford/FoundationModelOutput/checkpoints/\n",
      "  fine_tune_outcomes: None\n",
      "optim:\n",
      "  num_epochs: 1\n",
      "  learning_rate: 0.0003\n",
      "  scheduler: CAWarmRestarts\n",
      "  scheduler_periods: 5000\n",
      "  scheduler_warmup: true\n",
      "  lr_cosine_decay_period: 10000000.0\n",
      "  val_check_interval: 1000\n",
      "  early_stop: false\n",
      "  early_stop_patience: 5\n",
      "  log_every_n_steps: 20\n",
      "  limit_val_batches: 0.05\n",
      "  limit_test_batches: 0.05\n",
      "transformer:\n",
      "  block_type: Neo\n",
      "  block_size: 128\n",
      "  n_layer: 6\n",
      "  n_head: 6\n",
      "  n_embd: 384\n",
      "  layer_norm_bias: false\n",
      "  attention_type: global\n",
      "  bias: true\n",
      "  dropout: 0.0\n",
      "  attention_dropout: 0.0\n",
      "  resid_dropout: 0.0\n",
      "head:\n",
      "  SurvLayer: cr\n",
      "  surv_weight: 0.5\n",
      "  tokens_for_univariate_regression: None\n",
      "  value_weight: 0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: SLURM_NTASKS_PER_NODE=28\n"
     ]
    }
   ],
   "source": [
    " # TODO: define an env variable to fix for a local hpc environment issue, this shouldn't be needed\n",
    "%env SLURM_NTASKS_PER_NODE=28      \n",
    "\n",
    "# TODO: with above this trains, but due to widgets issue on hpc it does not print progress to notebook\n",
    "# cfg.experiment.train = False\n",
    "# cfg.experiment.test = False\n",
    "# cfg.experiment.log = False\n",
    "# model, dm = run(cfg)     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or define training process by hand\n",
    "### Create data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using meta information from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/meta_information_QuantJenny.pickle\n",
      "INFO:root:Using train file-row count dictionary from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/file_row_count_dict_train.pickle\n",
      "INFO:root:Using test file-row count dictionary from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/file_row_count_dict_test.pickle\n",
      "INFO:root:Using val file-row count dictionary from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/file_row_count_dict_val.pickle\n",
      "INFO:root:Tokenzier created based on 7,555,415,275 tokens\n",
      "INFO:root:Using tabular tokenizer, created from meta information and containing 265 tokens\n",
      "INFO:root:Loaded /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/split=train/ dataset, with 23,613,894 samples\n",
      "INFO:root:Loaded /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/split=test/ dataset, with 1,508,320 samples\n",
      "INFO:root:Loaded /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/split=val/ dataset, with 1,426,714 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265 vocab elements\n"
     ]
    }
   ],
   "source": [
    "# Build \n",
    "dm = FoundationalDataModule(path_to_db=cfg.data.path_to_db,\n",
    "                            path_to_ds=cfg.data.path_to_ds,\n",
    "                            load=True,\n",
    "                            tokenizer=\"tabular\",\n",
    "                            batch_size=cfg.data.batch_size,\n",
    "                            max_seq_length=cfg.transformer.block_size,\n",
    "                            global_diagnoses=cfg.data.global_diagnoses,\n",
    "                            freq_threshold=cfg.data.unk_freq_threshold,\n",
    "                            min_workers=cfg.data.min_workers,\n",
    "                            overwrite_meta_information=cfg.data.meta_information_path,\n",
    "                           )\n",
    "\n",
    "vocab_size = dm.train_set.tokenizer.vocab_size\n",
    "print(f\"{vocab_size} vocab elements\")\n",
    "\n",
    "# list of univariate measurements to model with Normal distribution\n",
    "# Extract the measurements, using the fact that the diagnoses are all up upper case.\n",
    "measurements_for_univariate_regression = [record for record in dm.tokenizer._event_counts[\"EVENT\"] if record.upper() != record]\n",
    "cfg.head.tokens_for_univariate_regression = dm.encode(measurements_for_univariate_regression) \n",
    "# display(measurements_for_univariate_regression)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dm.train_set.view_sample(1000, report_time=True) # max_dynamic_events=120,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple boilerplate training example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hoelkOrFY8bN",
    "outputId": "961304cd-e379-40d4-dd56-8de0b91d2861",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using Temporal Positional Encoding. This module uses the patient's age at an event within their time series.\n",
      "INFO:root:Using Competing-Risk DeSurv head.\n",
      "INFO:root:Internally scaling time in survival head by 1825 days\n",
      "INFO:root:In generation forwarding DeSurv on the grid between [0.0, 1825.0] with 1826 time-points of delta=1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with 11.495664 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rds/bear-apps/2022a/EL8-ice/software/PyTorch/2.0.1-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 20 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "Training epoch 0:   0%|          | 0/368968 [00:13<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'surv': {'k': [tensor([187, 248, 179,  ..., 204, 262, 207], device='cuda:0')], 'tte_deltas': tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'), 'surv_CDF': None}, 'values_dist': Normal(loc: torch.Size([64, 127]), scale: torch.Size([64, 127]))}\n",
      "{'loss': tensor(17.4001, device='cuda:0', grad_fn=<AddBackward0>), 'loss_desurv': tensor(6.1184, device='cuda:0', grad_fn=<SumBackward0>), 'loss_values': tensor(28.6819, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "tensor([[[ 1.2471, -0.5819,  1.7113,  ..., -0.9841, -0.0025, -0.9285],\n",
      "         [ 0.7609, -0.6914,  1.8770,  ..., -0.3029, -0.2056, -1.2073],\n",
      "         [ 1.5128, -1.0191,  1.4974,  ..., -0.4149,  0.0650, -0.3332],\n",
      "         ...,\n",
      "         [-0.0077,  0.9734,  0.6062,  ..., -0.5477, -0.9804,  0.9182],\n",
      "         [ 0.1318,  0.9799,  0.4084,  ..., -1.2155, -1.0101, -0.1689],\n",
      "         [ 0.1977,  1.3529,  0.3337,  ..., -0.3589, -0.4911,  0.5974]],\n",
      "\n",
      "        [[-0.4083, -0.5222, -0.6363,  ..., -0.0324,  0.9524, -0.4055],\n",
      "         [ 0.3632, -0.2745, -0.2921,  ..., -0.3074,  0.5497, -0.5785],\n",
      "         [-0.4974, -0.0029,  0.8803,  ...,  0.5723,  0.3577, -0.0773],\n",
      "         ...,\n",
      "         [-0.5187, -0.2188, -1.0364,  ...,  0.7733, -0.0716,  0.3398],\n",
      "         [ 1.7241, -1.8860, -1.0407,  ...,  1.2553,  0.5730,  0.1465],\n",
      "         [ 1.1621, -1.4826, -0.5278,  ...,  0.8967,  0.6339,  0.1089]],\n",
      "\n",
      "        [[-1.0565, -0.5355,  1.0782,  ...,  0.9524,  0.3484,  0.7794],\n",
      "         [-1.5585, -0.2885,  0.8521,  ...,  0.6689,  0.3360,  0.4000],\n",
      "         [-0.7000, -0.9988,  1.4902,  ...,  0.3929,  0.1999,  0.8094],\n",
      "         ...,\n",
      "         [-0.0533,  1.4228, -0.4001,  ...,  0.7300, -0.9022, -1.2751],\n",
      "         [ 1.0225,  0.5629,  0.1140,  ...,  0.6643, -1.3845, -0.4076],\n",
      "         [ 0.8656,  0.7911,  0.1685,  ...,  0.4617, -1.8183, -0.6628]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.6279, -1.0843,  1.0184,  ...,  0.1195,  0.4773, -0.1961],\n",
      "         [-1.1033, -0.1317,  0.0547,  ...,  0.7231,  0.6913, -0.3310],\n",
      "         [-0.4062, -1.8819, -0.1257,  ...,  0.1703,  0.8063, -0.1470],\n",
      "         ...,\n",
      "         [-0.0597, -0.9998, -0.6083,  ...,  1.1301, -1.3739, -0.6983],\n",
      "         [-0.0597, -0.9998, -0.6083,  ...,  1.1301, -1.3739, -0.6983],\n",
      "         [-0.0597, -0.9998, -0.6083,  ...,  1.1301, -1.3739, -0.6983]],\n",
      "\n",
      "        [[ 0.6596,  1.3371,  1.1945,  ...,  0.7700, -0.7656, -0.4015],\n",
      "         [ 0.9965,  1.1154,  1.4376,  ...,  1.4676, -0.2319, -0.3267],\n",
      "         [ 1.5329,  0.6888,  0.9998,  ...,  1.0022, -0.9066, -0.2567],\n",
      "         ...,\n",
      "         [ 0.1986,  1.8469, -0.6856,  ...,  1.1945, -2.2090, -0.1592],\n",
      "         [ 0.1986,  1.8469, -0.6856,  ...,  1.1945, -2.2090, -0.1592],\n",
      "         [ 0.1986,  1.8469, -0.6856,  ...,  1.1945, -2.2090, -0.1592]],\n",
      "\n",
      "        [[-0.4959, -1.6830, -1.4771,  ..., -0.0223, -1.2412,  0.7305],\n",
      "         [-0.5763, -1.3467, -1.8255,  ...,  0.4759, -0.7358,  0.7902],\n",
      "         [-0.9969, -0.2967, -1.6439,  ...,  0.5130, -1.8263,  0.4685],\n",
      "         ...,\n",
      "         [-0.9969, -0.2967, -1.6439,  ...,  0.5130, -1.8263,  0.4685],\n",
      "         [-0.9969, -0.2967, -1.6439,  ...,  0.5130, -1.8263,  0.4685],\n",
      "         [-0.9969, -0.2967, -1.6439,  ...,  0.5130, -1.8263,  0.4685]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss_dict)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(hidden_states)\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[1;32m     36\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     37\u001b[0m loss_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = SurvStreamGPTForCausalModelling(cfg, vocab_size).to(device)\n",
    "model = model.to(device)\n",
    "\n",
    "loss_curves_train = []\n",
    "loss_curves_train_surv = []\n",
    "loss_curves_train_values = []\n",
    "\n",
    "loss_curves_val = []\n",
    "loss_curves_val_surv = []\n",
    "loss_curves_val_values = []    \n",
    "print(f\"Training model with {sum(p.numel() for p in model.parameters())/1e6} M parameters\")\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.optim.learning_rate)\n",
    "\n",
    "best_val, epochs_since_best = np.inf, 0\n",
    "for epoch in range(10):\n",
    "    \n",
    "    epoch_loss, epoch_surv_loss, epoch_values_loss = 0, 0, 0\n",
    "    model.train()\n",
    "    for i, batch in tqdm(enumerate(dm.train_dataloader()), desc=f\"Training epoch {epoch}\", total=len(dm.train_dataloader())):\n",
    "        \n",
    "            \n",
    "        # evaluate the loss\n",
    "        _, loss_dict, hidden_states = model(tokens=batch['tokens'].to(device),\n",
    "                                            ages=batch['ages'].to(device),\n",
    "                                            values=batch['values'].to(device),\n",
    "                                            covariates=batch[\"static_covariates\"].to(device),\n",
    "                                            attention_mask=batch['attention_mask'].to(device)\n",
    "                                            )\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss_dict[\"loss\"].backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # record\n",
    "        epoch_loss += loss_dict[\"loss\"].item()            \n",
    "        epoch_surv_loss += loss_dict[\"loss_desurv\"].item()\n",
    "        epoch_values_loss += loss_dict[\"loss_values\"].item()\n",
    "\n",
    "        if i > 1000:\n",
    "            break\n",
    "    \n",
    "    epoch_loss /= i\n",
    "    epoch_surv_loss /= i\n",
    "    epoch_values_loss /= i\n",
    "    loss_curves_train.append(epoch_loss)\n",
    "    loss_curves_train_surv.append(epoch_surv_loss)\n",
    "    loss_curves_train_values.append(epoch_values_loss)\n",
    "\n",
    "    # evaluate the loss on val set\n",
    "    with torch.no_grad(): \n",
    "        model.eval()\n",
    "        if epoch % 1 == 0 or epoch == cfg.optim.num_epochs - 1:\n",
    "            val_loss, val_surv_loss, val_values_loss = 0, 0, 0\n",
    "            for j, batch in tqdm(enumerate(dm.val_dataloader()), desc=f\"Validation epoch {epoch}\", total=len(dm.val_dataloader())):\n",
    "                if j > 100:\n",
    "                    break\n",
    "                _, loss_dict, _ = model(tokens=batch['tokens'].to(device),\n",
    "                                        ages=batch['ages'].to(device),\n",
    "                                        values=batch['values'].to(device),\n",
    "                                        covariates=batch[\"static_covariates\"].to(device),\n",
    "                                        attention_mask=batch['attention_mask'].to(device)\n",
    "                                       )\n",
    "                # record\n",
    "                val_loss += loss_dict[\"loss\"].item()                    \n",
    "                val_surv_loss += loss_dict[\"loss_desurv\"].item()\n",
    "                val_values_loss += loss_dict[\"loss_values\"].item()\n",
    "                \n",
    "            val_loss /= j\n",
    "            val_surv_loss /= j\n",
    "            val_values_loss /= j\n",
    "            loss_curves_val.append(val_loss)\n",
    "            loss_curves_val_surv.append(val_surv_loss)\n",
    "            loss_curves_val_values.append(val_values_loss)\n",
    "\n",
    "            print(f\"Epoch {epoch}:\\tTrain loss {epoch_loss:.2f}: ({epoch_surv_loss:.2f}, {epoch_values_loss:.2f}). Val loss {val_loss:.2f}: ({val_surv_loss:.2f}, {val_values_loss:.2f})\")          \n",
    "            # TODO: Note not fully accurate as last batch is likely not the same size, will be fixed with lightning\n",
    "\n",
    "        if val_loss >= best_val:\n",
    "            epochs_since_best += 1\n",
    "            if epochs_since_best >= 5:\n",
    "                break\n",
    "        else:\n",
    "            best_val = val_loss\n",
    "            epochs_since_best = 0\n",
    "\n",
    "            # Save best seen model\n",
    "            # torch.save(model.state_dict(), path_to_db + \"polars/CR.pt\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss\n",
    "plt.figure()\n",
    "# Training\n",
    "iterations = np.linspace(0, len(loss_curves_train), len(loss_curves_train)) * opt.eval_interval\n",
    "plt.plot(iterations, loss_curves_train, label=\"train\")\n",
    "# Validation\n",
    "iterations = np.linspace(0, len(loss_curves_val), len(loss_curves_val)) * opt.eval_interval\n",
    "plt.plot(iterations, loss_curves_val, label=\"val\", linestyle='dashed')\n",
    "plt.legend()\n",
    "plt.savefig(f\"figs/loss.png\")\n",
    "\n",
    "# Plot DeSurv loss\n",
    "plt.figure()\n",
    "# Training\n",
    "iterations = np.linspace(0, len(loss_curves_train_surv), len(loss_curves_train_surv)) * opt.eval_interval\n",
    "plt.plot(iterations, loss_curves_train_surv, label=\"train\")\n",
    "# Validation\n",
    "iterations = np.linspace(0, len(loss_curves_val_surv), len(loss_curves_val_surv)) * opt.eval_interval\n",
    "plt.plot(iterations, loss_curves_val_surv, label=\"val\", linestyle='dashed')\n",
    "plt.legend()\n",
    "plt.savefig(f\"figs/loss_desurv.png\")\n",
    "\n",
    "# Plot value loss\n",
    "plt.figure()\n",
    "# Training\n",
    "iterations = np.linspace(0, len(loss_curves_train_values), len(loss_curves_train_values)) * opt.eval_interval\n",
    "plt.plot(iterations, loss_curves_train_values, label=\"train\", )\n",
    "# Validation\n",
    "iterations = np.linspace(0, len(loss_curves_val_values), len(loss_curves_val_values)) * opt.eval_interval\n",
    "plt.plot(iterations, loss_curves_val_values, label=\"val\", linestyle='dashed')\n",
    "plt.legend()\n",
    "plt.savefig(f\"figs/loss_val.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: model architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to html --no-input competing_risk.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "sys_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
