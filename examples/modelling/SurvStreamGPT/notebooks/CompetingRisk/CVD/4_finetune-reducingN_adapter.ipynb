{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4921342e-8ff8-45d4-b1f6-9fe480c2ee1d",
   "metadata": {},
   "source": [
    "# CPRD Notebook:\n",
    "## Evaluation of fine-tuning the pre-trained SurvivEHR-CR model on a supervised cohort study.\n",
    "\n",
    "Cohort study: predicting Cardiovascular Disease in a Type 2 Diabetes Mellitus population.\n",
    "\n",
    "This notebook quantifies the performance obtained when fine-tuning the pre-trained model to a sub-population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a671c59b-4428-4e63-a138-7244418a87c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added path '/rds/homes/g/gaddcz/Projects/CPRD/virtual-envTorch2.0-icelake/lib/python3.10/site-packages' at start of search paths.\n",
      "/rds/homes/g/gaddcz/Projects/CPRD/examples/modelling/SurvStreamGPT/notebooks/CompetingRisk/CVD\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "node_type = os.getenv('BB_CPU')\n",
    "venv_dir = f'/rds/homes/g/gaddcz/Projects/CPRD/virtual-envTorch2.0-{node_type}'\n",
    "venv_site_pkgs = Path(venv_dir) / 'lib' / f'python{sys.version_info.major}.{sys.version_info.minor}' / 'site-packages'\n",
    "if venv_site_pkgs.exists():\n",
    "    sys.path.insert(0, str(venv_site_pkgs))\n",
    "    print(f\"Added path '{venv_site_pkgs}' at start of search paths.\")\n",
    "else:\n",
    "    print(f\"Path '{venv_site_pkgs}' not found. Check that it exists and/or that it exists for node-type '{node_type}'.\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d866c7f0-eaa8-4129-b3b1-d4d6b504d0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda.\n",
      "env: SLURM_NTASKS_PER_NODE=28\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "from CPRD.examples.modelling.SurvStreamGPT.run_experiment import run\n",
    "from CPRD.data.foundational_loader import FoundationalDataModule\n",
    "from CPRD.src.models.survival.task_heads.causal import SurvStreamGPTForCausalModelling\n",
    "\n",
    "import time\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "import polars as pl\n",
    "pl.Config.set_tbl_rows(10000)\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 10000\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = \"cpu\"    # if more informative debugging statements are needed\n",
    "print(f\"Using device: {device}.\")\n",
    "\n",
    " # TODO: define an env variable to fix for a local hpc environment issue, this shouldn't be needed\n",
    "%env SLURM_NTASKS_PER_NODE=28   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c237f8-679a-4fe4-a570-2c8c7b61da9b",
   "metadata": {},
   "source": [
    "# Fine-tuning on full dataset\n",
    "The default configuration is for pre-training. Here we modify as necesssary\n",
    "\n",
    "Here we choose to load in the configuration for a small **pre-trained** 11.4M parameter model, named \"CR_11M\". We specfiy the `fine-tune` experiment type, which will lead to running the ```SupervisedExperiment```. \n",
    "\n",
    "We tell this experiment that we want to perform training (true by default). Additionally, we do choose to perform testing (true by default). As this is a supervised model, this tests the ability to predict the outcomes of interest. In this notebook, this is chosen to be those of the cohort study for predicting Cardiovascular Disease in a Type 2 Diabetes Mellitus population, and we add the folder containing this dataset to the configuration. \n",
    "\n",
    "```Note: As this is a supervised dataset, we need to tell the DataModule that the last event observed is a target and must be stripped. This is done by passing a list of targets to the configuration, overriding the null default. This lets the DataModule know that it should process batches as supervised.```\n",
    "\n",
    "We set the number of workers to be appropriate for the number of CPUs available to reduce bottlenecking, and tell the experiment that we do not want to limit the number of testing batches. In addition, we specify where we want any checkpoints to be saved to avoid bloating the repository.\n",
    "\n",
    "We design a new optimisation strategy for fine-tuning. Pre-training was achieved with a warmup and cosine annealing, with rates which are no appropriate for much smaller dataset sizes seen in clinical prediction models (CPMs). We here choose a simpler strategy: of ReduceOnPlateau with no warmup, increasing the number of epochs (default is 1) and reduced validation intervals, and the addition of early stopping. Additionally, as this is not a causal model we can increase the batch size. Finally, as this CPM is not trying to predict the value of any outcomes, we set the value weight to zero allowing the model to focus entirely on optimising survival outcome prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68e4721-b2ca-44c8-9760-55442aa8803f",
   "metadata": {},
   "source": [
    "```\n",
    "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
    "┃             Test metric             ┃            DataLoader 0             ┃\n",
    "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
    "│  Test:OutcomePerformanceMetricsctd  │         0.8166841864585876          │\n",
    "│  Test:OutcomePerformanceMetricsibs  │         0.07901122285789881         │\n",
    "│ Test:OutcomePerformanceMetricsinbll │         0.25131127201650066         │\n",
    "│              test_loss              │         0.4277091324329376          │\n",
    "└─────────────────────────────────────┴─────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2e42878-b97d-4cba-a4c9-d35f745a1eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trained_model_ids = ['SurvivEHR-cr-small', 'SurvivEHR-cr-small-v1', 'SurvivEHR-cr-384', 'SurvivEHR-cr-384-v1', 'crPreTrain_small_1337']\n",
    "experiments = [\"cvd\", \"hypertension\"] \n",
    "experiment_types = [ \"fine-tune-cr\", \"fine-tune-sr\"] \n",
    "adapter = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c662c49b-6a1d-4a35-82dc-f00c77d0cfe9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    for pre_trained_model in pre_trained_model_ids[1:2]:\n",
    "        print(pre_trained_model)\n",
    "    \n",
    "        for experiment, experiment_type in zip(experiments, experiment_types):\n",
    "        \n",
    "            wandb.finish()\n",
    "            # load the configuration file, override any settings \n",
    "            with initialize(version_base=None, config_path=\"../../../confs\", job_name=\"testing_notebook\"):\n",
    "                cfg = compose(config_name=\"config_CompetingRisk11M\", \n",
    "                              overrides=[# Experiment setup\n",
    "                                         f\"experiment.type='{experiment_type}'\",\n",
    "                                         f\"experiment.run_id='{pre_trained_model}'\",\n",
    "                                         f\"experiment.fine_tune_id='{experiment}-{experiment_type}-A{adapter}-notebook'\",\n",
    "                                         \"experiment.train=True\",\n",
    "                                         \"experiment.test=True\",\n",
    "                                         \"experiment.notes=Table result\",\n",
    "                                         # Dataloader\n",
    "                                         \"data.batch_size=128\",\n",
    "                                         \"data.meta_information_path=/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/meta_information_QuantJenny.pickle\",\n",
    "                                         \"data.min_workers=3\",\n",
    "                                         \"data.global_diagnoses=True\",\n",
    "                                         # Optimiser\n",
    "                                         \"optim.num_epochs=20\",\n",
    "                                         \"optim.limit_test_batches=null\",\n",
    "                                         \"optim.scheduler=ReduceOnPlateau\",\n",
    "                                         \"optim.scheduler_warmup=False\",\n",
    "                                         \"optim.learning_rate=1e-3\",\n",
    "                                         \"optim.val_check_interval=50\",\n",
    "                                         \"optim.early_stop=True\",\n",
    "                                         \"optim.early_stop_patience=4\",\n",
    "                                         \"optim.limit_val_batches=0.035\",\n",
    "                                         \"optim.accumulate_grad_batches=4\",\n",
    "                                         # Model\n",
    "                                         # \"transformer.n_embd=384\",\n",
    "                                         f\"transformer.use_fine_tune_adapter={False if adapter is False else True}\",\n",
    "                                         f\"transformer.adapter_dim={8 if adapter is False else adapter}\",\n",
    "                                         \"transformer.block_size=512\", \n",
    "                                         \"transformer.dropout=0.0\",\n",
    "                                         \"transformer.resid_dropout=0.0\",\n",
    "                                         \"transformer.attention_dropout=0.0\",                                  \n",
    "                                        ]\n",
    "                             )\n",
    "            \n",
    "            match experiment.lower():\n",
    "                case \"cvd\":\n",
    "                    cfg.data.path_to_ds=\"/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/\"\n",
    "                    cfg.experiment.fine_tune_outcomes=[\"IHDINCLUDINGMI_OPTIMALV2\", \"ISCHAEMICSTROKE_V2\", \"MINFARCTION\", \"STROKEUNSPECIFIED_V2\", \"STROKE_HAEMRGIC\"]\n",
    "                case \"hypertension\":\n",
    "                    cfg.data.path_to_ds=\"/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_Hypertension/\"\n",
    "                    cfg.experiment.fine_tune_outcomes=[\"HYPERTENSION\"]\n",
    "            \n",
    "            \n",
    "            model, dm = run(cfg)\n",
    "            print(f\"Loaded model with {sum(p.numel() for p in model.parameters())/1e6} M parameters\")\n",
    "            wandb.finish()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65799ee6-9462-42a0-a293-dd4cac545d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [_i if _i == _i.upper() else 0 for _i in dm.train_set.tokenizer._stoi.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62557fe3-6772-411b-b16a-14edc0d366de",
   "metadata": {},
   "source": [
    "# Fine-tuning on sub-set of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "394bdffe-a788-4b32-8563-044daaf283b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2999, 5296, 9351, 16509, 29148, 51461, 90856, 160407, 283203, 500000]\n"
     ]
    }
   ],
   "source": [
    "sample_sizes = [int(np.exp(_log_n)) for _log_n in np.linspace(np.log(3000), np.log(500000), 10)]\n",
    "print(sample_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac4b72e2-a938-46b0-aea4-f56517283a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Running cr on 72 CPUs and 1 GPUs\n",
      "INFO:root:====================================================================================================\n",
      "INFO:root:# Loading DataModule for dataset /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/. This will be loaded in supervised form.\n",
      "INFO:root:====================================================================================================\n",
      "INFO:root:Creating supervised collator for DataModule\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SurvivEHR-cr-small-v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using meta information from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/meta_information_QuantJenny.pickle\n",
      "INFO:root:Using train file-row count dictionary from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/file_row_count_dict_train.pickle\n",
      "INFO:root:Using test file-row count dictionary from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/file_row_count_dict_test.pickle\n",
      "INFO:root:Using val file-row count dictionary from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/file_row_count_dict_val.pickle\n",
      "INFO:root:Tokenzier created based on 7,555,415,275 tokens\n",
      "INFO:root:Using tabular tokenizer, created from meta information and containing 265 tokens\n",
      "INFO:root:Loaded /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/split=train/ dataset, with 572,096 samples\n",
      "INFO:root:Loaded /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/split=test/ dataset, with 35,758 samples\n",
      "INFO:root:Loaded /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/split=val/ dataset, with 33,280 samples\n",
      "INFO:root:is_decoder: true\n",
      "data:\n",
      "  batch_size: 128\n",
      "  unk_freq_threshold: 0.0\n",
      "  min_workers: 3\n",
      "  global_diagnoses: true\n",
      "  repeating_events: false\n",
      "  path_to_db: /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/\n",
      "  path_to_ds: /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/\n",
      "  meta_information_path: /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/meta_information_QuantJenny.pickle\n",
      "  subsample_training: null\n",
      "experiment:\n",
      "  type: fine-tune-cr\n",
      "  project_name: SurvivEHR\n",
      "  run_id: SurvivEHR-cr-small-v1\n",
      "  fine_tune_id: cvd-fine-tune-cr-A8-EffecBsz640-NsNone-Bs512NRs-notebook\n",
      "  notes: Ablation on increasing cohort study size result\n",
      "  tags: null\n",
      "  train: false\n",
      "  test: true\n",
      "  verbose: true\n",
      "  seed: 1337\n",
      "  log: true\n",
      "  log_dir: /rds/projects/s/subramaa-mum-predict/CharlesGadd_Oxford/FoundationModelOutput/\n",
      "  ckpt_dir: /rds/projects/s/subramaa-mum-predict/CharlesGadd_Oxford/FoundationModelOutput/checkpoints/\n",
      "  fine_tune_outcomes:\n",
      "  - IHDINCLUDINGMI_OPTIMALV2\n",
      "  - ISCHAEMICSTROKE_V2\n",
      "  - MINFARCTION\n",
      "  - STROKEUNSPECIFIED_V2\n",
      "  - STROKE_HAEMRGIC\n",
      "optim:\n",
      "  num_epochs: 500\n",
      "  learning_rate: 0.001\n",
      "  scheduler_warmup: false\n",
      "  scheduler: ReduceOnPlateau\n",
      "  scheduler_periods: 10000\n",
      "  learning_rate_decay: 0.8\n",
      "  val_check_interval: 0.25\n",
      "  early_stop: true\n",
      "  early_stop_patience: 4\n",
      "  log_every_n_steps: 20\n",
      "  limit_val_batches: 0.035\n",
      "  limit_test_batches: null\n",
      "  accumulate_grad_batches: 5\n",
      "transformer:\n",
      "  block_type: Neo\n",
      "  block_size: 512\n",
      "  n_layer: 6\n",
      "  n_head: 6\n",
      "  n_embd: 384\n",
      "  layer_norm_bias: false\n",
      "  attention_type: global\n",
      "  bias: true\n",
      "  dropout: 0.0\n",
      "  attention_dropout: 0.0\n",
      "  resid_dropout: 0.0\n",
      "  private_heads: 0\n",
      "  use_fine_tune_adapter: true\n",
      "  adapter_dim: 8\n",
      "head:\n",
      "  SurvLayer: cr\n",
      "  surv_weight: 1\n",
      "  tokens_for_univariate_regression:\n",
      "  - 84\n",
      "  - 86\n",
      "  - 107\n",
      "  - 154\n",
      "  - 42\n",
      "  - 231\n",
      "  - 63\n",
      "  - 85\n",
      "  - 247\n",
      "  - 51\n",
      "  - 103\n",
      "  - 146\n",
      "  - 66\n",
      "  - 172\n",
      "  - 152\n",
      "  - 261\n",
      "  - 235\n",
      "  - 193\n",
      "  - 184\n",
      "  - 101\n",
      "  - 223\n",
      "  - 135\n",
      "  - 226\n",
      "  - 202\n",
      "  - 145\n",
      "  - 244\n",
      "  - 165\n",
      "  - 64\n",
      "  - 185\n",
      "  - 238\n",
      "  - 220\n",
      "  - 232\n",
      "  - 240\n",
      "  - 237\n",
      "  - 17\n",
      "  - 239\n",
      "  - 217\n",
      "  - 118\n",
      "  - 219\n",
      "  - 251\n",
      "  - 25\n",
      "  - 150\n",
      "  - 111\n",
      "  - 102\n",
      "  - 15\n",
      "  - 110\n",
      "  - 130\n",
      "  - 128\n",
      "  - 122\n",
      "  - 99\n",
      "  - 77\n",
      "  - 96\n",
      "  - 140\n",
      "  - 50\n",
      "  - 80\n",
      "  - 108\n",
      "  - 138\n",
      "  - 27\n",
      "  - 139\n",
      "  - 125\n",
      "  - 117\n",
      "  - 105\n",
      "  - 109\n",
      "  - 243\n",
      "  - 236\n",
      "  - 208\n",
      "  - 98\n",
      "  - 176\n",
      "  - 209\n",
      "  - 194\n",
      "  - 61\n",
      "  - 73\n",
      "  - 214\n",
      "  - 206\n",
      "  - 229\n",
      "  - 225\n",
      "  - 183\n",
      "  - 186\n",
      "  - 215\n",
      "  - 187\n",
      "  - 248\n",
      "  - 179\n",
      "  - 163\n",
      "  - 180\n",
      "  - 181\n",
      "  - 153\n",
      "  - 245\n",
      "  - 54\n",
      "  - 246\n",
      "  - 112\n",
      "  - 212\n",
      "  - 141\n",
      "  - 204\n",
      "  - 227\n",
      "  - 173\n",
      "  - 55\n",
      "  - 119\n",
      "  - 262\n",
      "  - 127\n",
      "  - 71\n",
      "  - 59\n",
      "  - 144\n",
      "  - 113\n",
      "  - 170\n",
      "  - 241\n",
      "  - 168\n",
      "  - 47\n",
      "  - 159\n",
      "  value_weight: 0.1\n",
      "\n",
      "INFO:root:====================================================================================================\n",
      "INFO:root:# Fine-tune learning experiment with a new competing-risk head\n",
      "INFO:root:====================================================================================================\n",
      "INFO:root:Loading a fine-tuned model with the checkpoint path /rds/projects/s/subramaa-mum-predict/CharlesGadd_Oxford/FoundationModelOutput/checkpoints/SurvivEHR-cr-small-v1_cvd-fine-tune-cr-A8-EffecBsz640-NsNone-Bs512NRs-notebook.ckpt. Evaluating supervised performance\n",
      "INFO:root:Running competing-risk fine-tuning experiment with outcomes {'IHDINCLUDINGMI_OPTIMALV2': 95, 'ISCHAEMICSTROKE_V2': 41, 'MINFARCTION': 67, 'STROKEUNSPECIFIED_V2': 65, 'STROKE_HAEMRGIC': 28}\n",
      "INFO:root:Loading fine-tuned checkpoint from /rds/projects/s/subramaa-mum-predict/CharlesGadd_Oxford/FoundationModelOutput/checkpoints/SurvivEHR-cr-small-v1_cvd-fine-tune-cr-A8-EffecBsz640-NsNone-Bs512NRs-notebook.ckpt\n",
      "INFO:torch.distributed.nn.jit.instantiator:Created a temporary directory at /tmp/tmp40fdq2ga\n",
      "INFO:torch.distributed.nn.jit.instantiator:Writing /tmp/tmp40fdq2ga/_remote_module_non_scriptable.py\n",
      "INFO:root:Using Temporal Positional Encoding. This module uses the patient's age at an event within their time series.\n",
      "INFO:root:Using Competing-Risk DeSurv head.\n",
      "INFO:root:In generation forwarding DeSurv on the grid between [0.0, 1.0] with 1000 intervals\n",
      "INFO:root:Fixing Transformer parameters and fine-tuning using an Adapter mechanism.\n",
      "INFO:root:Using Competing-Risk DeSurv head.\n",
      "INFO:root:In generation forwarding DeSurv on the grid between [0.0, 1.0] with 1000 intervals\n",
      "INFO:root:Created Performance metric callback. Calculating metrics for dict_keys([95, 41, 67, 65, 28]) with map {95: 0, 41: 1, 67: 2, 65: 3, 28: 4}\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "INFO:root:Testing model.\n",
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcwlgadd\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37f073c63fbf416bb0c44e09ec262f56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668105400943507, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/rds/projects/s/subramaa-mum-predict/CharlesGadd_Oxford/FoundationModelOutput/wandb/run-20250126_165512-2q41lwwe</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/cwlgadd/SurvivEHR/runs/2q41lwwe\" target=\"_blank\">SurvivEHR-cr-small-v1_cvd-fine-tune-cr-A8-EffecBsz640-NsNone-Bs512NRs-notebook</a></strong> to <a href=\"https://wandb.ai/cwlgadd/SurvivEHR\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9671b8b18a1e480fb6be7047b00dec95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">             Test metric             </span>┃<span style=\"font-weight: bold\">            DataLoader 0             </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  Test:OutcomePerformanceMetricsctd  </span>│<span style=\"color: #800080; text-decoration-color: #800080\">         0.6792826056480408          </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  Test:OutcomePerformanceMetricsibs  </span>│<span style=\"color: #800080; text-decoration-color: #800080\">         0.03339820813009103         </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Test:OutcomePerformanceMetricsinbll </span>│<span style=\"color: #800080; text-decoration-color: #800080\">         0.14070308365715697         </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">              test_loss              </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          0.558636486530304          </span>│\n",
       "└─────────────────────────────────────┴─────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m            Test metric            \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m           DataLoader 0            \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m Test:OutcomePerformanceMetricsctd \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m        0.6792826056480408         \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m Test:OutcomePerformanceMetricsibs \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m        0.03339820813009103        \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mTest:OutcomePerformanceMetricsinbll\u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m        0.14070308365715697        \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m             test_loss             \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         0.558636486530304         \u001b[0m\u001b[35m \u001b[0m│\n",
       "└─────────────────────────────────────┴─────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model with 11.316878 M parameters\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test:OutcomePerformanceMetricsctd</td><td>▁</td></tr><tr><td>Test:OutcomePerformanceMetricsibs</td><td>▁</td></tr><tr><td>Test:OutcomePerformanceMetricsinbll</td><td>▁</td></tr><tr><td>epoch</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>trainer/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test:OutcomePerformanceMetricsctd</td><td>0.67928</td></tr><tr><td>Test:OutcomePerformanceMetricsibs</td><td>0.0334</td></tr><tr><td>Test:OutcomePerformanceMetricsinbll</td><td>0.1407</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>test_loss</td><td>0.55864</td></tr><tr><td>trainer/global_step</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">SurvivEHR-cr-small-v1_cvd-fine-tune-cr-A8-EffecBsz640-NsNone-Bs512NRs-notebook</strong>: <a href=\"https://wandb.ai/cwlgadd/SurvivEHR/runs/2q41lwwe\" target=\"_blank\">https://wandb.ai/cwlgadd/SurvivEHR/runs/2q41lwwe</a><br/>Synced 5 W&B file(s), 281 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/rds/projects/s/subramaa-mum-predict/CharlesGadd_Oxford/FoundationModelOutput/wandb/run-20250126_165512-2q41lwwe/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Running cr on 72 CPUs and 1 GPUs\n",
      "INFO:root:====================================================================================================\n",
      "INFO:root:# Loading DataModule for dataset /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_Hypertension/. This will be loaded in supervised form.\n",
      "INFO:root:====================================================================================================\n",
      "INFO:root:Creating supervised collator for DataModule\n",
      "INFO:root:Using meta information from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/meta_information_QuantJenny.pickle\n",
      "INFO:root:Using train file-row count dictionary from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_Hypertension/file_row_count_dict_train.pickle\n",
      "INFO:root:Using test file-row count dictionary from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_Hypertension/file_row_count_dict_test.pickle\n",
      "INFO:root:Using val file-row count dictionary from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_Hypertension/file_row_count_dict_val.pickle\n",
      "INFO:root:Tokenzier created based on 7,555,415,275 tokens\n",
      "INFO:root:Using tabular tokenizer, created from meta information and containing 265 tokens\n",
      "INFO:root:Loaded /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_Hypertension/split=train/ dataset, with 572,096 samples\n",
      "INFO:root:Loaded /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_Hypertension/split=test/ dataset, with 35,758 samples\n",
      "INFO:root:Loaded /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_Hypertension/split=val/ dataset, with 33,280 samples\n",
      "INFO:root:is_decoder: true\n",
      "data:\n",
      "  batch_size: 128\n",
      "  unk_freq_threshold: 0.0\n",
      "  min_workers: 3\n",
      "  global_diagnoses: true\n",
      "  repeating_events: false\n",
      "  path_to_db: /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/\n",
      "  path_to_ds: /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_Hypertension/\n",
      "  meta_information_path: /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/meta_information_QuantJenny.pickle\n",
      "  subsample_training: null\n",
      "experiment:\n",
      "  type: fine-tune-sr\n",
      "  project_name: SurvivEHR\n",
      "  run_id: SurvivEHR-cr-small-v1\n",
      "  fine_tune_id: hypertension-fine-tune-sr-A8-EffecBsz640-NsNone-Bs512NRs-notebook\n",
      "  notes: Ablation on increasing cohort study size result\n",
      "  tags: null\n",
      "  train: false\n",
      "  test: true\n",
      "  verbose: true\n",
      "  seed: 1337\n",
      "  log: true\n",
      "  log_dir: /rds/projects/s/subramaa-mum-predict/CharlesGadd_Oxford/FoundationModelOutput/\n",
      "  ckpt_dir: /rds/projects/s/subramaa-mum-predict/CharlesGadd_Oxford/FoundationModelOutput/checkpoints/\n",
      "  fine_tune_outcomes:\n",
      "  - HYPERTENSION\n",
      "optim:\n",
      "  num_epochs: 500\n",
      "  learning_rate: 0.001\n",
      "  scheduler_warmup: false\n",
      "  scheduler: ReduceOnPlateau\n",
      "  scheduler_periods: 10000\n",
      "  learning_rate_decay: 0.8\n",
      "  val_check_interval: 0.25\n",
      "  early_stop: true\n",
      "  early_stop_patience: 4\n",
      "  log_every_n_steps: 20\n",
      "  limit_val_batches: 0.035\n",
      "  limit_test_batches: null\n",
      "  accumulate_grad_batches: 5\n",
      "transformer:\n",
      "  block_type: Neo\n",
      "  block_size: 512\n",
      "  n_layer: 6\n",
      "  n_head: 6\n",
      "  n_embd: 384\n",
      "  layer_norm_bias: false\n",
      "  attention_type: global\n",
      "  bias: true\n",
      "  dropout: 0.0\n",
      "  attention_dropout: 0.0\n",
      "  resid_dropout: 0.0\n",
      "  private_heads: 0\n",
      "  use_fine_tune_adapter: true\n",
      "  adapter_dim: 8\n",
      "head:\n",
      "  SurvLayer: cr\n",
      "  surv_weight: 1\n",
      "  tokens_for_univariate_regression:\n",
      "  - 84\n",
      "  - 86\n",
      "  - 107\n",
      "  - 154\n",
      "  - 42\n",
      "  - 231\n",
      "  - 63\n",
      "  - 85\n",
      "  - 247\n",
      "  - 51\n",
      "  - 103\n",
      "  - 146\n",
      "  - 66\n",
      "  - 172\n",
      "  - 152\n",
      "  - 261\n",
      "  - 235\n",
      "  - 193\n",
      "  - 184\n",
      "  - 101\n",
      "  - 223\n",
      "  - 135\n",
      "  - 226\n",
      "  - 202\n",
      "  - 145\n",
      "  - 244\n",
      "  - 165\n",
      "  - 64\n",
      "  - 185\n",
      "  - 238\n",
      "  - 220\n",
      "  - 232\n",
      "  - 240\n",
      "  - 237\n",
      "  - 17\n",
      "  - 239\n",
      "  - 217\n",
      "  - 118\n",
      "  - 219\n",
      "  - 251\n",
      "  - 25\n",
      "  - 150\n",
      "  - 111\n",
      "  - 102\n",
      "  - 15\n",
      "  - 110\n",
      "  - 130\n",
      "  - 128\n",
      "  - 122\n",
      "  - 99\n",
      "  - 77\n",
      "  - 96\n",
      "  - 140\n",
      "  - 50\n",
      "  - 80\n",
      "  - 108\n",
      "  - 138\n",
      "  - 27\n",
      "  - 139\n",
      "  - 125\n",
      "  - 117\n",
      "  - 105\n",
      "  - 109\n",
      "  - 243\n",
      "  - 236\n",
      "  - 208\n",
      "  - 98\n",
      "  - 176\n",
      "  - 209\n",
      "  - 194\n",
      "  - 61\n",
      "  - 73\n",
      "  - 214\n",
      "  - 206\n",
      "  - 229\n",
      "  - 225\n",
      "  - 183\n",
      "  - 186\n",
      "  - 215\n",
      "  - 187\n",
      "  - 248\n",
      "  - 179\n",
      "  - 163\n",
      "  - 180\n",
      "  - 181\n",
      "  - 153\n",
      "  - 245\n",
      "  - 54\n",
      "  - 246\n",
      "  - 112\n",
      "  - 212\n",
      "  - 141\n",
      "  - 204\n",
      "  - 227\n",
      "  - 173\n",
      "  - 55\n",
      "  - 119\n",
      "  - 262\n",
      "  - 127\n",
      "  - 71\n",
      "  - 59\n",
      "  - 144\n",
      "  - 113\n",
      "  - 170\n",
      "  - 241\n",
      "  - 168\n",
      "  - 47\n",
      "  - 159\n",
      "  value_weight: 0.1\n",
      "\n",
      "INFO:root:====================================================================================================\n",
      "INFO:root:# Fine-tune learning experiment with a new single-risk head\n",
      "INFO:root:====================================================================================================\n",
      "INFO:root:Creating new fine-tuned model at the path /rds/projects/s/subramaa-mum-predict/CharlesGadd_Oxford/FoundationModelOutput/checkpoints/SurvivEHR-cr-small-v1_hypertension-fine-tune-sr-A8-EffecBsz640-NsNone-Bs512NRs-notebook.ckpt.\n",
      "INFO:root:This is trained from a checkpointed pre-trained causal experiment, which can be found at /rds/projects/s/subramaa-mum-predict/CharlesGadd_Oxford/FoundationModelOutput/checkpoints/SurvivEHR-cr-small-v1.ckpt.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "If you are not training a new fine-tuned model, please load a valid checkpoint. /rds/projects/s/subramaa-mum-predict/CharlesGadd_Oxford/FoundationModelOutput/checkpoints/SurvivEHR-cr-small-v1.ckpt is not valid.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 61\u001b[0m\n\u001b[1;32m     17\u001b[0m     cfg \u001b[38;5;241m=\u001b[39m compose(config_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig_CompetingRisk11M\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     18\u001b[0m                   overrides\u001b[38;5;241m=\u001b[39m[\u001b[38;5;66;03m# Experiment setup\u001b[39;00m\n\u001b[1;32m     19\u001b[0m                              \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexperiment.type=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexperiment_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m                             ]\n\u001b[1;32m     49\u001b[0m                  )\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mmatch\u001b[39;00m experiment\u001b[38;5;241m.\u001b[39mlower():\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcvd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     54\u001b[0m         cfg\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mpath_to_ds\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m         cfg\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mpath_to_ds\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_Hypertension/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     58\u001b[0m         cfg\u001b[38;5;241m.\u001b[39mexperiment\u001b[38;5;241m.\u001b[39mfine_tune_outcomes\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHYPERTENSION\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 61\u001b[0m model, dm \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded model with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1e6\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m M parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     63\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "File \u001b[0;32m/rds/bear-apps/2022a/EL8-ice/software/Hydra/1.3.2-GCCcore-11.3.0/lib/python3.10/site-packages/hydra/main.py:83\u001b[0m, in \u001b[0;36mmain.<locals>.main_decorator.<locals>.decorated_main\u001b[0;34m(cfg_passthrough)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(task_function)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorated_main\u001b[39m(cfg_passthrough: Optional[DictConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cfg_passthrough \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 83\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg_passthrough\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     85\u001b[0m         args_parser \u001b[38;5;241m=\u001b[39m get_args_parser()\n",
      "File \u001b[0;32m~/Projects/CPRD/examples/modelling/SurvStreamGPT/run_experiment.py:210\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Create experiment\u001b[39;00m\n\u001b[1;32m     67\u001b[0m experiment_type \u001b[38;5;241m=\u001b[39m cfg\u001b[38;5;241m.\u001b[39mexperiment\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mmatch\u001b[39;00m experiment_type:\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# (TODO: LBYL)\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpretrain\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcausal\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mselfsupervised\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     71\u001b[0m         \n\u001b[1;32m     72\u001b[0m         \u001b[38;5;66;03m# Training (or causal evaluation) a pre-trained model\u001b[39;00m\n\u001b[1;32m     73\u001b[0m         logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m     74\u001b[0m         logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m# Pre-training experiment\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     75\u001b[0m         logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m     76\u001b[0m         \n\u001b[1;32m     77\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m Path(pre_trained_ckpt_path)\u001b[38;5;241m.\u001b[39mis_file():\n\u001b[1;32m     78\u001b[0m             \u001b[38;5;66;03m# Load existing experiment from checkpoint\u001b[39;00m\n\u001b[1;32m     79\u001b[0m             logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading a pre-trained model with the checkpoint path \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpre_trained_ckpt_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     80\u001b[0m \n\u001b[1;32m     81\u001b[0m             \u001b[38;5;66;03m# Catch cases where user loads a pre-trained model to pre-train it further\u001b[39;00m\n\u001b[1;32m     82\u001b[0m             \u001b[38;5;66;03m#   (it will result in checkpointing to a new pre_train_ckpt-V2.ckpt file and then re-loading the original after training)\u001b[39;00m\n\u001b[1;32m     83\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mexperiment\u001b[38;5;241m.\u001b[39mtrain:\n\u001b[1;32m     84\u001b[0m                 logging\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFurther training on a checkpoint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpre_trained_ckpt_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m which will create a new checkpoint. Ensure evaluation is not on the original checkpoint.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m                 \n\u001b[1;32m     86\u001b[0m             load_from_checkpoint \u001b[38;5;241m=\u001b[39m pre_trained_ckpt_path\n\u001b[1;32m     87\u001b[0m             new_checkpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# Not implemented a versioning control on iterative checkpointing\u001b[39;00m\n\u001b[1;32m     88\u001b[0m             \n\u001b[1;32m     89\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m             \u001b[38;5;66;03m# Create new experiment\u001b[39;00m\n\u001b[1;32m     91\u001b[0m             logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating new pre-trained model at the path \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpre_trained_ckpt_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     92\u001b[0m             \n\u001b[1;32m     93\u001b[0m             \u001b[38;5;28;01massert\u001b[39;00m dm\u001b[38;5;241m.\u001b[39mis_supervised \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you are training a new pre-trained model, the data module must not be supervised. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdm\u001b[38;5;241m.\u001b[39mis_supervised\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     94\u001b[0m             \u001b[38;5;28;01massert\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mexperiment\u001b[38;5;241m.\u001b[39mtrain \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you are not training a new pre-trained model, please load a valid checkpoint. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpre_trained_ckpt_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not valid.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     95\u001b[0m             \n\u001b[1;32m     96\u001b[0m             logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m# This will create / evaluate a pre-trained Foundation Model on a causal (next-event prediction) modelling task.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     97\u001b[0m             load_from_checkpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     98\u001b[0m             new_checkpoint \u001b[38;5;241m=\u001b[39m pre_trained_ckpt_path\n\u001b[1;32m     99\u001b[0m             \n\u001b[1;32m    100\u001b[0m         experiment_instance, Experiment, trainer \u001b[38;5;241m=\u001b[39m setup_causal_experiment(cfg\u001b[38;5;241m=\u001b[39mcfg, \n\u001b[1;32m    101\u001b[0m                                                                            dm\u001b[38;5;241m=\u001b[39mdm, \n\u001b[1;32m    102\u001b[0m                                                                            vocab_size\u001b[38;5;241m=\u001b[39mvocab_size,\n\u001b[1;32m    103\u001b[0m                                                                            checkpoint\u001b[38;5;241m=\u001b[39mload_from_checkpoint,\n\u001b[1;32m    104\u001b[0m                                                                            logger\u001b[38;5;241m=\u001b[39mlogger\n\u001b[1;32m    105\u001b[0m                                                                           )\n\u001b[1;32m    106\u001b[0m \n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeroshot\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;66;03m# Evaluate an existing pre-trained experiment\u001b[39;00m\n\u001b[1;32m    109\u001b[0m         logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m    110\u001b[0m         logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m# # Zero-shot learning experiment\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    111\u001b[0m         logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m    112\u001b[0m \n\u001b[1;32m    113\u001b[0m         logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading a pre-trained model with the checkpoint path \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpre_trained_ckpt_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    114\u001b[0m         \n\u001b[1;32m    115\u001b[0m         \u001b[38;5;66;03m# Ensure the pre-trained model exists\u001b[39;00m\n\u001b[1;32m    116\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Path(pre_trained_ckpt_path)\u001b[38;5;241m.\u001b[39mis_file():\n\u001b[1;32m    117\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe pre-trained model with the checkpoint path \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpre_trained_ckpt_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    118\u001b[0m         \n\u001b[1;32m    119\u001b[0m         \u001b[38;5;66;03m# Load existing pre-trained checkpoint\u001b[39;00m\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mexperiment\u001b[38;5;241m.\u001b[39mtrain \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe zero-shot experiment evaluates a pre-trained causal model on a supervised task without additional training. Ensure training is set to False\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    121\u001b[0m \n\u001b[1;32m    122\u001b[0m         load_from_checkpoint \u001b[38;5;241m=\u001b[39m pre_trained_ckpt_path\n\u001b[1;32m    123\u001b[0m             \n\u001b[1;32m    124\u001b[0m         experiment_instance, Experiment, trainer \u001b[38;5;241m=\u001b[39m setup_fewshot_experiment(cfg\u001b[38;5;241m=\u001b[39mcfg,\n\u001b[1;32m    125\u001b[0m                                                                             dm\u001b[38;5;241m=\u001b[39mdm, \n\u001b[1;32m    126\u001b[0m                                                                             vocab_size\u001b[38;5;241m=\u001b[39mvocab_size,\n\u001b[1;32m    127\u001b[0m                                                                             checkpoint\u001b[38;5;241m=\u001b[39mload_from_checkpoint,\n\u001b[1;32m    128\u001b[0m                                                                             logger\u001b[38;5;241m=\u001b[39mlogger\n\u001b[1;32m    129\u001b[0m                                                                            )\n\u001b[1;32m    130\u001b[0m         new_checkpoint \u001b[38;5;241m=\u001b[39m pre_trained_ckpt_path\n\u001b[1;32m    131\u001b[0m \n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfewshot\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    133\u001b[0m         \u001b[38;5;66;03m# Create/evaluate a few-shot model\u001b[39;00m\n\u001b[1;32m    134\u001b[0m         logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m    135\u001b[0m         logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m# Few-shot learning experiment.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    136\u001b[0m         logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m    137\u001b[0m \n\u001b[1;32m    138\u001b[0m         \u001b[38;5;66;03m# Get ckpt path for each experiment type (either to be loaded or created)\u001b[39;00m\n\u001b[1;32m    139\u001b[0m         supervised_run_id \u001b[38;5;241m=\u001b[39m  cfg\u001b[38;5;241m.\u001b[39mexperiment\u001b[38;5;241m.\u001b[39mrun_id \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m cfg\u001b[38;5;241m.\u001b[39mexperiment\u001b[38;5;241m.\u001b[39mfine_tune_id   \u001b[38;5;66;03m# run id + dataset folder name (i.e. CR_11M_FineTune_CVD)\u001b[39;00m\n\u001b[1;32m    140\u001b[0m         supervised_ckpt_path \u001b[38;5;241m=\u001b[39m cfg\u001b[38;5;241m.\u001b[39mexperiment\u001b[38;5;241m.\u001b[39mckpt_dir \u001b[38;5;241m+\u001b[39m supervised_run_id \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.ckpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m         \n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# By default, we try to load in any existing model with the same supervised name\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m Path(supervised_ckpt_path)\u001b[38;5;241m.\u001b[39mis_file():\n\u001b[1;32m    144\u001b[0m             \u001b[38;5;66;03m# Load existing fine-tuned experiment from checkpoint\u001b[39;00m\n\u001b[1;32m    145\u001b[0m             logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading a few-shot model with the checkpoint path \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msupervised_ckpt_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    146\u001b[0m \n\u001b[1;32m    147\u001b[0m             \u001b[38;5;66;03m# Catch cases where user loads a fine-tuned model and tries to fine-tune it further, as this edge case is not supported \u001b[39;00m\n\u001b[1;32m    148\u001b[0m             \u001b[38;5;66;03m#   (it will result in checkpointing to a new fine_tune_ckpt-V2.ckpt file and then re-loading the original after training)\u001b[39;00m\n\u001b[1;32m    149\u001b[0m             \u001b[38;5;28;01massert\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mexperiment\u001b[38;5;241m.\u001b[39mtrain \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFurther training on a checkpoint is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    150\u001b[0m \n\u001b[1;32m    151\u001b[0m             load_from_checkpoint \u001b[38;5;241m=\u001b[39m supervised_ckpt_path\n\u001b[1;32m    152\u001b[0m \n\u001b[1;32m    153\u001b[0m         \u001b[38;5;66;03m# Otherwise we create a new model built upon the specified pre-trained model\u001b[39;00m\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;28;01melif\u001b[39;00m Path(pre_trained_ckpt_path)\u001b[38;5;241m.\u001b[39mis_file():\n\u001b[1;32m    155\u001b[0m             \u001b[38;5;66;03m# Create new fine-tuning experiment\u001b[39;00m\n\u001b[1;32m    156\u001b[0m             logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating new few-shot model at the path \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msupervised_ckpt_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \\\n\u001b[1;32m    157\u001b[0m                          \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is initialised from a pre-trained causal model, which can be found at checkpoint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpre_trained_ckpt_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    158\u001b[0m             \n\u001b[1;32m    159\u001b[0m             \u001b[38;5;28;01massert\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mexperiment\u001b[38;5;241m.\u001b[39mtrain \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you are not training a new few-shot model, please load a valid checkpoint. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpre_trained_ckpt_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not valid.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m             \n\u001b[1;32m    161\u001b[0m             load_from_checkpoint \u001b[38;5;241m=\u001b[39m pre_trained_ckpt_path\n\u001b[1;32m    162\u001b[0m \n\u001b[1;32m    163\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    164\u001b[0m             logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating new few-shot model from scratch.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    165\u001b[0m             \n\u001b[1;32m    166\u001b[0m             \u001b[38;5;28;01massert\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mexperiment\u001b[38;5;241m.\u001b[39mtrain \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you are not training a new fine-tuned model, please load a valid checkpoint. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpre_trained_ckpt_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not valid.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    167\u001b[0m             load_from_checkpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \n\u001b[1;32m    169\u001b[0m         experiment_instance, Experiment, trainer \u001b[38;5;241m=\u001b[39m setup_fewshot_experiment(cfg\u001b[38;5;241m=\u001b[39mcfg, \n\u001b[1;32m    170\u001b[0m                                                                             dm\u001b[38;5;241m=\u001b[39mdm, \n\u001b[1;32m    171\u001b[0m                                                                             vocab_size\u001b[38;5;241m=\u001b[39mvocab_size,\n\u001b[1;32m    172\u001b[0m                                                                             checkpoint\u001b[38;5;241m=\u001b[39mload_from_checkpoint,\n\u001b[1;32m    173\u001b[0m                                                                             logger\u001b[38;5;241m=\u001b[39mlogger\n\u001b[1;32m    174\u001b[0m                                                                            )\n\u001b[1;32m    175\u001b[0m         \n\u001b[1;32m    176\u001b[0m         \u001b[38;5;66;03m# Specfiy path we should will find the best attained model after training, so that this can be loaded before testing\u001b[39;00m\n\u001b[1;32m    177\u001b[0m         new_checkpoint \u001b[38;5;241m=\u001b[39m supervised_ckpt_path\n\u001b[1;32m    178\u001b[0m \n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinetune\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinetunesr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinetunecr\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;66;03m# Create/evaluate a fine-tuned model\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \n\u001b[1;32m    182\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m experiment_type[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msr\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    183\u001b[0m             risk_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle-risk\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    184\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m             risk_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompeting-risk\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    186\u001b[0m \n\u001b[1;32m    187\u001b[0m         \u001b[38;5;66;03m# Get ckpt path for each experiment type\u001b[39;00m\n\u001b[1;32m    188\u001b[0m         supervised_run_id \u001b[38;5;241m=\u001b[39m  cfg\u001b[38;5;241m.\u001b[39mexperiment\u001b[38;5;241m.\u001b[39mrun_id \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m cfg\u001b[38;5;241m.\u001b[39mexperiment\u001b[38;5;241m.\u001b[39mfine_tune_id   \u001b[38;5;66;03m# run id + dataset folder name (i.e. CR_11M_FineTune_CVD)\u001b[39;00m\n\u001b[1;32m    189\u001b[0m         supervised_ckpt_path \u001b[38;5;241m=\u001b[39m cfg\u001b[38;5;241m.\u001b[39mexperiment\u001b[38;5;241m.\u001b[39mckpt_dir \u001b[38;5;241m+\u001b[39m supervised_run_id \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.ckpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    190\u001b[0m         \n\u001b[1;32m    191\u001b[0m         logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m    192\u001b[0m         logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m# Fine-tune learning experiment with a new \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrisk_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m head\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    193\u001b[0m         logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m    194\u001b[0m \n\u001b[1;32m    195\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m Path(supervised_ckpt_path)\u001b[38;5;241m.\u001b[39mis_file():\n\u001b[1;32m    196\u001b[0m             \u001b[38;5;66;03m# Load existing fine-tuned experiment from checkpoint\u001b[39;00m\n\u001b[1;32m    197\u001b[0m             logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading a fine-tuned model with the checkpoint path \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msupervised_ckpt_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Evaluating supervised performance\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    198\u001b[0m \n\u001b[1;32m    199\u001b[0m             \u001b[38;5;66;03m# Catch cases where user loads a fine-tuned model and tries to fine-tune it further, as this edge case is not supported \u001b[39;00m\n\u001b[1;32m    200\u001b[0m             \u001b[38;5;66;03m#   (it will result in checkpointing to a new fine_tune_ckpt-V2.ckpt file and then re-loading the original after training)\u001b[39;00m\n\u001b[1;32m    201\u001b[0m             \u001b[38;5;28;01massert\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mexperiment\u001b[38;5;241m.\u001b[39mtrain \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFurther training on a checkpoint is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    202\u001b[0m             ft_ckpt \u001b[38;5;241m=\u001b[39m supervised_ckpt_path\n\u001b[1;32m    203\u001b[0m             mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload_from_finetune\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    204\u001b[0m             \n\u001b[1;32m    205\u001b[0m         \u001b[38;5;28;01melif\u001b[39;00m Path(pre_trained_ckpt_path)\u001b[38;5;241m.\u001b[39mis_file():\n\u001b[1;32m    206\u001b[0m             \u001b[38;5;66;03m# Create new fine-tuning experiment, from a pre-trained model\u001b[39;00m\n\u001b[1;32m    207\u001b[0m             logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating new fine-tuned model at the path \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msupervised_ckpt_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    208\u001b[0m             logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is trained from a checkpointed pre-trained causal experiment, which can be found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpre_trained_ckpt_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    209\u001b[0m             \n\u001b[0;32m--> 210\u001b[0m             \u001b[38;5;28;01massert\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mexperiment\u001b[38;5;241m.\u001b[39mtrain \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you are not training a new fine-tuned model, please load a valid checkpoint. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpre_trained_ckpt_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not valid.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    211\u001b[0m             ft_ckpt \u001b[38;5;241m=\u001b[39m pre_trained_ckpt_path\n\u001b[1;32m    212\u001b[0m             mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload_from_pretrain\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    213\u001b[0m             \n\u001b[1;32m    214\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m             logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating new fine-tuned model from scratch.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    216\u001b[0m             \n\u001b[1;32m    217\u001b[0m             \u001b[38;5;28;01massert\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mexperiment\u001b[38;5;241m.\u001b[39mtrain \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you are not training a new fine-tuned model, please load a valid checkpoint. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpre_trained_ckpt_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not valid.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    218\u001b[0m             ft_ckpt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    219\u001b[0m             mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno_load\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    220\u001b[0m             \n\u001b[1;32m    221\u001b[0m         experiment_instance, Experiment, trainer \u001b[38;5;241m=\u001b[39m setup_finetune_experiment(cfg\u001b[38;5;241m=\u001b[39mcfg,\n\u001b[1;32m    222\u001b[0m                                                                              dm\u001b[38;5;241m=\u001b[39mdm, \n\u001b[1;32m    223\u001b[0m                                                                              mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[1;32m    224\u001b[0m                                                                              risk_model\u001b[38;5;241m=\u001b[39mrisk_model,\n\u001b[1;32m    225\u001b[0m                                                                              checkpoint\u001b[38;5;241m=\u001b[39mft_ckpt,\n\u001b[1;32m    226\u001b[0m                                                                              logger\u001b[38;5;241m=\u001b[39mlogger\n\u001b[1;32m    227\u001b[0m                                                                             )\n\u001b[1;32m    228\u001b[0m         new_checkpoint \u001b[38;5;241m=\u001b[39m supervised_ckpt_path\n\u001b[1;32m    229\u001b[0m         \n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;28;01m_\u001b[39;00m:\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mexperiment\u001b[38;5;241m.\u001b[39mtrain:\n\u001b[1;32m    234\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining model.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: If you are not training a new fine-tuned model, please load a valid checkpoint. /rds/projects/s/subramaa-mum-predict/CharlesGadd_Oxford/FoundationModelOutput/checkpoints/SurvivEHR-cr-small-v1.ckpt is not valid."
     ]
    }
   ],
   "source": [
    "sample_sizes = [int(np.exp(_log_n)) for _log_n in np.linspace(np.log(3000), np.log(500000), 10)]      # [3000, 12500, 30000, 60000, 100000]: # 600, 1200, \n",
    "sample_sizes = [None]\n",
    "\n",
    "accumulate_grad_batches = 5\n",
    "\n",
    "\n",
    "for pre_trained_model in pre_trained_model_ids[1:2]:\n",
    "    print(pre_trained_model)\n",
    "\n",
    "    for experiment, experiment_type in zip(experiments, experiment_types):\n",
    "\n",
    "        for sample_size in sample_sizes:\n",
    "\n",
    "            wandb.finish()\n",
    "            # load the configuration file, override any settings \n",
    "            with initialize(version_base=None, config_path=\"../../../confs\", job_name=\"testing_notebook\"):\n",
    "                cfg = compose(config_name=\"config_CompetingRisk11M\", \n",
    "                              overrides=[# Experiment setup\n",
    "                                         f\"experiment.type='{experiment_type}'\",\n",
    "                                         f\"experiment.run_id='{pre_trained_model}'\",\n",
    "                                         f\"experiment.fine_tune_id='{experiment}-{experiment_type}-A{adapter}-EffecBsz{128*accumulate_grad_batches}-Ns{sample_size}-Bs512NRs-notebook'\",\n",
    "                                         \"experiment.train=False\",\n",
    "                                         \"experiment.test=True\",\n",
    "                                         \"experiment.notes=Ablation on increasing cohort study size result\",\n",
    "                                         # Dataloader\n",
    "                                         \"data.batch_size=128\",\n",
    "                                         \"data.meta_information_path=/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/meta_information_QuantJenny.pickle\",\n",
    "                                         \"data.min_workers=3\",\n",
    "                                         \"data.global_diagnoses=True\",\n",
    "                                         \"data.repeating_events=False\",\n",
    "                                         # f\"data.subsample_training={sample_size}\",\n",
    "                                         # Optimiser\n",
    "                                         \"optim.num_epochs=500\",\n",
    "                                         \"optim.limit_test_batches=null\",\n",
    "                                         \"optim.scheduler=ReduceOnPlateau\",\n",
    "                                         \"optim.scheduler_warmup=False\",\n",
    "                                         \"optim.learning_rate=1e-3\",\n",
    "                                         \"optim.val_check_interval=0.25\",\n",
    "                                         \"optim.early_stop=True\",\n",
    "                                         \"optim.early_stop_patience=4\",\n",
    "                                         \"optim.limit_val_batches=0.035\",\n",
    "                                         f\"optim.accumulate_grad_batches={accumulate_grad_batches}\",\n",
    "                                         # Model\n",
    "                                         # \"transformer.n_embd=384\",\n",
    "                                         f\"transformer.use_fine_tune_adapter={False if adapter is False else True}\",\n",
    "                                         f\"transformer.adapter_dim={8 if adapter is False else adapter}\",\n",
    "                                         \"transformer.block_size=512\", \n",
    "                                        ]\n",
    "                             )\n",
    "            \n",
    "            \n",
    "            match experiment.lower():\n",
    "                case \"cvd\":\n",
    "                    cfg.data.path_to_ds=\"/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/\"\n",
    "                    cfg.experiment.fine_tune_outcomes=[\"IHDINCLUDINGMI_OPTIMALV2\", \"ISCHAEMICSTROKE_V2\", \"MINFARCTION\", \"STROKEUNSPECIFIED_V2\", \"STROKE_HAEMRGIC\"]\n",
    "                case \"hypertension\":\n",
    "                    cfg.data.path_to_ds=\"/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_Hypertension/\"\n",
    "                    cfg.experiment.fine_tune_outcomes=[\"HYPERTENSION\"]\n",
    "            \n",
    "            \n",
    "            model, dm = run(cfg)\n",
    "            print(f\"Loaded model with {sum(p.numel() for p in model.parameters())/1e6} M parameters\")\n",
    "            wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25b2aed-68b9-49b1-a804-2420199947a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b4322e-5fa7-4079-8439-ca79cd6d1674",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.tokenizer._event_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6184dc1d-6429-40cb-941a-5b21db8b8ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_CVD = {\n",
    "    \"SurvivEHR-cr-small-v1\": (\n",
    "        \"SurvivEHR-CR\",\n",
    "        [2999, 5296, 9351, 16509, 29148, 51461, 90856, 160407, 283203, 500000]],\n",
    "        [0.6270919442176819],\n",
    "        [0.03389651543792928],\n",
    "        [0.14657540914939907]\n",
    "        ),\n",
    "    \n",
    "}\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "sys_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
