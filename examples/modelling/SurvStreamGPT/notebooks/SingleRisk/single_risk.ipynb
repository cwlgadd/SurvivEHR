{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJpXpmjEYC_T"
   },
   "source": [
    "# Demo Notebook:\n",
    "## Single Risk Survival Transformer For Causal Sequence Modelling \n",
    "\n",
    "Including time, tabular values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added path '/rds/homes/g/gaddcz/Projects/CPRD/virtual-envTorch2.0-icelake/lib/python3.10/site-packages' at start of search paths.\n",
      "/rds/homes/g/gaddcz/Projects/CPRD/examples/modelling/SurvStreamGPT/notebooks/SingleRisk\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "node_type = os.getenv('BB_CPU')\n",
    "venv_dir = f'/rds/homes/g/gaddcz/Projects/CPRD/virtual-envTorch2.0-{node_type}'\n",
    "venv_site_pkgs = Path(venv_dir) / 'lib' / f'python{sys.version_info.major}.{sys.version_info.minor}' / 'site-packages'\n",
    "if venv_site_pkgs.exists():\n",
    "    sys.path.insert(0, str(venv_site_pkgs))\n",
    "    print(f\"Added path '{venv_site_pkgs}' at start of search paths.\")\n",
    "else:\n",
    "    print(f\"Path '{venv_site_pkgs}' not found. Check that it exists and/or that it exists for node-type '{node_type}'.\")\n",
    "\n",
    "!pwd\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from pycox.evaluation import EvalSurv\n",
    "from tqdm import tqdm\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "from CPRD.examples.modelling.SurvStreamGPT.experiment import run\n",
    "from CPRD.data.foundational_loader import FoundationalDataModule\n",
    "from CPRD.src.models.survival.task_heads.causal import SurvStreamGPTForCausalModelling\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = \"cpu\"    # if more informative debugging statements are needed\n",
    "print(f\"Using device: {device}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo Version of SurvStreamGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the configuration file, override any settings \n",
    "with initialize(version_base=None, config_path=\"../../confs\", job_name=\"testing_notebook\"):\n",
    "    cfg = compose(config_name=\"config_SingleRisk11M\", overrides=[])\n",
    "\n",
    "\n",
    "# cfg.data.batch_size = 16\n",
    "# cfg.transformer.block_size = 32\n",
    "# # cfg.transformer.n_layer = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_decoder: true\n",
      "data:\n",
      "  batch_size: 64\n",
      "  unk_freq_threshold: 0.0\n",
      "  min_workers: 20\n",
      "  global_diagnoses: false\n",
      "  meta_information_path: /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/polars/meta_information.pickle\n",
      "experiment:\n",
      "  project_name: SurvStreamGPT_${head.SurvLayer}\n",
      "  run_id: PreTrain_${head.SurvLayer}_11M_${experiment.seed}\n",
      "  train: true\n",
      "  test: true\n",
      "  verbose: true\n",
      "  seed: 1337\n",
      "  log: true\n",
      "  log_dir: /rds/projects/s/subramaa-mum-predict/CharlesGadd_Oxford/FoundationModelOutput/\n",
      "  ckpt_dir: /rds/projects/s/subramaa-mum-predict/CharlesGadd_Oxford/FoundationModelOutput/checkpoints/\n",
      "optim:\n",
      "  num_epochs: 1\n",
      "  learning_rate: 0.0001\n",
      "  val_check_interval: 1000\n",
      "  early_stop: false\n",
      "  early_stop_patience: 5\n",
      "  log_every_n_steps: 20\n",
      "  limit_val_batches: 0.05\n",
      "  limit_test_batches: 0.05\n",
      "transformer:\n",
      "  block_type: Neo\n",
      "  block_size: 128\n",
      "  n_layer: 6\n",
      "  n_head: 6\n",
      "  n_embd: 384\n",
      "  layer_norm_bias: false\n",
      "  attention_type: global\n",
      "  bias: true\n",
      "  dropout: 0.0\n",
      "  attention_dropout: 0.0\n",
      "  resid_dropout: 0.0\n",
      "head:\n",
      "  SurvLayer: sr\n",
      "  surv_weight: 0.5\n",
      "  tokens_for_univariate_regression: None\n",
      "  value_weight: 0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: SLURM_NTASKS_PER_NODE=28\n"
     ]
    }
   ],
   "source": [
    " # TODO: define an env variable to fix for a local hpc environment issue, this shouldn't be needed\n",
    "%env SLURM_NTASKS_PER_NODE=28      \n",
    "\n",
    "# TODO: with above this trains, but due to widgets issue on hpc it does not print progress to notebook\n",
    "# cfg.experiment.train = False\n",
    "# cfg.experiment.test = False\n",
    "# cfg.experiment.log = False\n",
    "# model, dm = run(cfg)     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or define training process by hand\n",
    "### Create data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using meta information from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/polars/meta_information.pickle\n",
      "INFO:root:Using train file-row count dictionary from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/polars/file_row_count_dict_train.pickle\n",
      "INFO:root:Using test file-row count dictionary from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/polars/file_row_count_dict_test.pickle\n",
      "INFO:root:Using val file-row count dictionary from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/polars/file_row_count_dict_val.pickle\n",
      "INFO:root:Tokenzier created based on 3584.43M tokens\n",
      "INFO:root:Using tabular tokenizer, created from meta information and containing 184 tokens\n",
      "INFO:root:Loaded /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/polars/split=train/ dataset, with 23,343,104 samples\n",
      "INFO:root:Loaded /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/polars/split=test/ dataset, with 1,263,168 samples\n",
      "INFO:root:Loaded /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/polars/split=val/ dataset, with 1,233,601 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184 vocab elements\n"
     ]
    }
   ],
   "source": [
    "# Build \n",
    "path_to_db = \"/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/\"\n",
    "dm = FoundationalDataModule(path_to_db=path_to_db,\n",
    "                            load=True,\n",
    "                            tokenizer=\"tabular\",\n",
    "                            batch_size=cfg.data.batch_size,\n",
    "                            max_seq_length=cfg.transformer.block_size,\n",
    "                            global_diagnoses=cfg.data.global_diagnoses,\n",
    "                            freq_threshold=cfg.data.unk_freq_threshold,\n",
    "                            min_workers=cfg.data.min_workers,\n",
    "                            overwrite_meta_information=cfg.data.meta_information_path,\n",
    "                           )\n",
    "\n",
    "vocab_size = dm.train_set.tokenizer.vocab_size\n",
    "print(f\"{vocab_size} vocab elements\")\n",
    "\n",
    "# list of univariate measurements to model with Normal distribution\n",
    "# Extract the measurements, using the fact that the diagnoses are all up upper case.\n",
    "measurements_for_univariate_regression = [record for record in dm.tokenizer._event_counts[\"EVENT\"] if record.upper() != record]\n",
    "cfg.head.tokens_for_univariate_regression = dm.encode(measurements_for_univariate_regression) \n",
    "# display(measurements_for_univariate_regression)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dm.train_set.view_sample(1000, report_time=True) # max_dynamic_events=120,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using Temporal Positional Encoding. This module uses the patient's age at an event within their time series.\n",
      "INFO:root:Using Single-Risk DeSurvival head. This module predicts a separate survival curve for each possible future event\n",
      "INFO:root:Internally scaling time in survival head by 1825 days\n",
      "INFO:root:In generation forwarding DeSurv on the grid between [0.0, 1825.0]\n",
      "INFO:root:with 1826 intervals of delta=1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with 11.167512 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   0%|          | 1001/364736 [13:37<82:32:18,  1.22it/s]\n",
      "Validation epoch 0:   1%|          | 101/19276 [00:38<2:01:40,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\tTrain loss -1.89: (3.12, -6.90). Val loss -2.59: (2.86, -8.03)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   0%|          | 1001/364736 [09:57<60:20:38,  1.67it/s]\n",
      "Validation epoch 1:   1%|          | 101/19276 [00:38<2:00:26,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\tTrain loss -3.48: (2.62, -9.57). Val loss -3.37: (2.55, -9.29)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 2:   0%|          | 1001/364736 [09:59<60:28:47,  1.67it/s]\n",
      "Validation epoch 2:   1%|          | 101/19276 [00:37<1:58:57,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:\tTrain loss -4.24: (2.29, -10.77). Val loss -3.81: (2.20, -9.82)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 3:   0%|          | 1001/364736 [09:59<60:33:24,  1.67it/s]\n",
      "Validation epoch 3:   1%|          | 101/19276 [00:37<1:58:19,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:\tTrain loss -4.68: (2.05, -11.41). Val loss -4.18: (2.05, -10.41)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 4:   0%|          | 1001/364736 [09:59<60:33:36,  1.67it/s]\n",
      "Validation epoch 4:   1%|          | 101/19276 [00:37<1:58:50,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:\tTrain loss -5.11: (1.90, -12.12). Val loss -4.47: (1.93, -10.86)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 5:   0%|          | 1001/364736 [10:01<60:43:12,  1.66it/s]\n",
      "Validation epoch 5:   1%|          | 101/19276 [00:37<1:58:41,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:\tTrain loss -5.41: (1.82, -12.64). Val loss -4.02: (1.85, -9.89)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 6:   0%|          | 1001/364736 [10:02<60:50:06,  1.66it/s]\n",
      "Validation epoch 6:   1%|          | 101/19276 [00:37<1:59:04,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:\tTrain loss -5.62: (1.75, -12.98). Val loss -4.76: (1.78, -11.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 7:   0%|          | 1001/364736 [10:01<60:45:37,  1.66it/s]\n",
      "Validation epoch 7:   1%|          | 101/19276 [00:37<2:00:13,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:\tTrain loss -5.79: (1.69, -13.27). Val loss -5.00: (1.74, -11.73)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 8:   0%|          | 1001/364736 [10:05<61:09:49,  1.65it/s]\n",
      "Validation epoch 8:   1%|          | 101/19276 [00:38<2:01:38,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:\tTrain loss -5.92: (1.65, -13.50). Val loss -4.97: (1.70, -11.63)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 9:   0%|          | 1001/364736 [10:10<61:37:13,  1.64it/s]\n",
      "Validation epoch 9:   1%|          | 101/19276 [00:38<2:00:36,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:\tTrain loss -6.07: (1.59, -13.72). Val loss -4.98: (1.68, -11.65)\n"
     ]
    }
   ],
   "source": [
    "model = SurvStreamGPTForCausalModelling(cfg, vocab_size).to(device)\n",
    "model = model.to(device)\n",
    "\n",
    "loss_curves_train = []\n",
    "loss_curves_train_surv = []\n",
    "loss_curves_train_values = []\n",
    "\n",
    "loss_curves_val = []\n",
    "loss_curves_val_surv = []\n",
    "loss_curves_val_values = []    \n",
    "print(f\"Training model with {sum(p.numel() for p in model.parameters())/1e6} M parameters\")\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.optim.learning_rate)\n",
    "\n",
    "best_val, epochs_since_best = np.inf, 0\n",
    "for epoch in range(10):\n",
    "    \n",
    "    epoch_loss, epoch_surv_loss, epoch_values_loss = 0, 0, 0\n",
    "    model.train()\n",
    "    for i, batch in tqdm(enumerate(dm.train_dataloader()), desc=f\"Training epoch {epoch}\", total=len(dm.train_dataloader())):\n",
    "        \n",
    "            \n",
    "        # evaluate the loss\n",
    "        _, loss_dict, _ = model(tokens=batch['tokens'].to(device),\n",
    "                                ages=batch['ages'].to(device),\n",
    "                                values=batch['values'].to(device),\n",
    "                                covariates=batch[\"static_covariates\"].to(device),\n",
    "                                attention_mask=batch['attention_mask'].to(device)\n",
    "                               )\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss_dict[\"loss\"].backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # record\n",
    "        epoch_loss += loss_dict[\"loss\"].item()            \n",
    "        epoch_surv_loss += loss_dict[\"loss_desurv\"].item()\n",
    "        epoch_values_loss += loss_dict[\"loss_values\"].item()\n",
    "        \n",
    "        if i > 1000:\n",
    "            break\n",
    "    \n",
    "    epoch_loss /= i\n",
    "    epoch_surv_loss /= i\n",
    "    epoch_values_loss /= i\n",
    "    loss_curves_train.append(epoch_loss)\n",
    "    loss_curves_train_surv.append(epoch_surv_loss)\n",
    "    loss_curves_train_values.append(epoch_values_loss)\n",
    "\n",
    "    # evaluate the loss on val set\n",
    "    with torch.no_grad(): \n",
    "        model.eval()\n",
    "        if epoch % 1 == 0 or epoch == cfg.optim.num_epochs - 1:\n",
    "            val_loss, val_surv_loss, val_values_loss = 0, 0, 0\n",
    "            for j, batch in tqdm(enumerate(dm.val_dataloader()), desc=f\"Validation epoch {epoch}\", total=len(dm.val_dataloader())):\n",
    "                if j > 100:\n",
    "                    break\n",
    "                _, loss_dict, _ = model(tokens=batch['tokens'].to(device),\n",
    "                                        ages=batch['ages'].to(device),\n",
    "                                        values=batch['values'].to(device),\n",
    "                                        covariates=batch[\"static_covariates\"].to(device),\n",
    "                                        attention_mask=batch['attention_mask'].to(device)\n",
    "                                       )\n",
    "                # record\n",
    "                val_loss += loss_dict[\"loss\"].item()                    \n",
    "                val_surv_loss += loss_dict[\"loss_desurv\"].item()\n",
    "                val_values_loss += loss_dict[\"loss_values\"].item()\n",
    "                \n",
    "            val_loss /= j\n",
    "            val_surv_loss /= j\n",
    "            val_values_loss /= j\n",
    "            loss_curves_val.append(val_loss)\n",
    "            loss_curves_val_surv.append(val_surv_loss)\n",
    "            loss_curves_val_values.append(val_values_loss)\n",
    "\n",
    "            print(f\"Epoch {epoch}:\\tTrain loss {epoch_loss:.2f}: ({epoch_surv_loss:.2f}, {epoch_values_loss:.2f}). Val loss {val_loss:.2f}: ({val_surv_loss:.2f}, {val_values_loss:.2f})\")          \n",
    "            # TODO: Note not fully accurate as last batch is likely not the same size, will be fixed with lightning\n",
    "\n",
    "        if val_loss >= best_val:\n",
    "            epochs_since_best += 1\n",
    "            if epochs_since_best >= 5:\n",
    "                break\n",
    "        else:\n",
    "            best_val = val_loss\n",
    "            epochs_since_best = 0\n",
    "\n",
    "            # Save best seen model\n",
    "            # torch.save(model.state_dict(), path_to_db + \"polars/CR.pt\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss\n",
    "plt.figure()\n",
    "# Training\n",
    "iterations = np.linspace(0, len(loss_curves_train), len(loss_curves_train)) * opt.eval_interval\n",
    "plt.plot(iterations, loss_curves_train, label=\"train\")\n",
    "# Validation\n",
    "iterations = np.linspace(0, len(loss_curves_val), len(loss_curves_val)) * opt.eval_interval\n",
    "plt.plot(iterations, loss_curves_val, label=\"val\", linestyle='dashed')\n",
    "plt.legend()\n",
    "plt.savefig(f\"figs/loss.png\")\n",
    "\n",
    "# Plot DeSurv loss\n",
    "plt.figure()\n",
    "# Training\n",
    "iterations = np.linspace(0, len(loss_curves_train_surv), len(loss_curves_train_surv)) * opt.eval_interval\n",
    "plt.plot(iterations, loss_curves_train_surv, label=\"train\")\n",
    "# Validation\n",
    "iterations = np.linspace(0, len(loss_curves_val_surv), len(loss_curves_val_surv)) * opt.eval_interval\n",
    "plt.plot(iterations, loss_curves_val_surv, label=\"val\", linestyle='dashed')\n",
    "plt.legend()\n",
    "plt.savefig(f\"figs/loss_desurv.png\")\n",
    "\n",
    "# Plot value loss\n",
    "plt.figure()\n",
    "# Training\n",
    "iterations = np.linspace(0, len(loss_curves_train_values), len(loss_curves_train_values)) * opt.eval_interval\n",
    "plt.plot(iterations, loss_curves_train_values, label=\"train\", )\n",
    "# Validation\n",
    "iterations = np.linspace(0, len(loss_curves_val_values), len(loss_curves_val_values)) * opt.eval_interval\n",
    "plt.plot(iterations, loss_curves_val_values, label=\"val\", linestyle='dashed')\n",
    "plt.legend()\n",
    "plt.savefig(f\"figs/loss_val.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: model architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SurvStreamGPTForCausalModelling(\n",
       "  (transformer): TTETransformer(\n",
       "    (wpe): TemporalPositionalEncoding()\n",
       "    (wte): DataEmbeddingLayer(\n",
       "      (static_proj): Linear(in_features=16, out_features=384, bias=True)\n",
       "      (dynamic_embedding_layer): SplitDynamicEmbeddingLayer(\n",
       "        (cat_event_embed_layer): Embedding(184, 384, padding_idx=0)\n",
       "        (cat_event_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (num_value_embed_layer): EmbeddingBag(184, 384, mode='sum', padding_idx=0)\n",
       "        (num_value_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (blocks): ModuleList(\n",
       "      (0-5): 6 x Block(\n",
       "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiHeadedSelfAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (acti): ReLU()\n",
       "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (surv_layer): ODESurvSingleLayer()\n",
       "  (value_layer): GaussianRegressionLayer(\n",
       "    (regression_layers): ModuleDict(\n",
       "      (Token 15): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 17): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 24): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 26): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 41): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 46): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 49): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 50): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 52): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 53): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 57): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 59): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 61): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 62): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 64): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 67): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 68): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 71): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 74): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 78): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 79): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 80): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 86): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 88): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 89): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 91): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 92): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 93): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 95): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 97): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 98): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 99): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 100): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 101): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 102): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 103): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 105): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 106): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 107): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 109): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 111): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 113): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 114): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 116): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 119): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 121): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 122): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 123): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 124): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 125): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 126): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 127): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 128): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 129): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 130): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 131): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 132): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 133): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 134): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 135): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 136): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 137): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 138): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 139): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 140): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 141): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 142): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 143): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 144): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 145): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 146): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 147): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 148): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 149): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 150): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 151): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 152): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 153): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 154): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 155): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 156): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 157): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 158): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 159): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 160): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 161): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 162): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 163): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 164): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 165): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 166): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 167): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 168): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 169): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 170): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 171): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 172): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 173): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 174): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 175): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 176): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 177): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 178): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 179): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 180): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 181): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 182): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 183): Linear(in_features=384, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook single_risk.ipynb to html\n",
      "[NbConvertApp] Writing 603283 bytes to single_risk.html\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to html --no-input single_risk.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "sys_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
