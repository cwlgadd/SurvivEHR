{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJpXpmjEYC_T"
   },
   "source": [
    "# Demo Notebook:\n",
    "## Survival Transformer For Causal Sequence Modelling \n",
    "\n",
    "Including time, and excluding tabular values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added path '/rds/homes/g/gaddcz/Projects/CPRD/virtual-env-icelake/lib/python3.10/site-packages' at start of search paths.\n",
      "/rds/homes/g/gaddcz/Projects/CPRD/examples/modelling/SurvStreamGPT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "node_type = os.getenv('BB_CPU')\n",
    "venv_dir = f'/rds/homes/g/gaddcz/Projects/CPRD/virtual-env-{node_type}'\n",
    "venv_site_pkgs = Path(venv_dir) / 'lib' / f'python{sys.version_info.major}.{sys.version_info.minor}' / 'site-packages'\n",
    "if venv_site_pkgs.exists():\n",
    "    sys.path.insert(0, str(venv_site_pkgs))\n",
    "    print(f\"Added path '{venv_site_pkgs}' at start of search paths.\")\n",
    "else:\n",
    "    print(f\"Path '{venv_site_pkgs}' not found. Check that it exists and/or that it exists for node-type '{node_type}'.\")\n",
    "\n",
    "!pwd\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda.\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sqlite3\n",
    "from dataclasses import dataclass\n",
    "import logging\n",
    "from CPRD.data.foundational_loader import FoundationalDataModule\n",
    "from CPRD.src.models.survival.task_heads.causal import SurvStreamGPTForCausalModelling\n",
    "from pycox.evaluation import EvalSurv\n",
    "from tqdm import tqdm \n",
    "\n",
    "# TODO:\n",
    "# replace experiment boilerplate with pytorch lightning\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = \"cpu\"    # if more informative debugging statements are needed\n",
    "print(f\"Using device: {device}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set config to be equivalent architecture of kaparthy benchmark, however they are not comparable tasks.\n",
    "@dataclass\n",
    "class DemoConfig:\n",
    "    block_size: int = 128        # what is the maximum context length for predictions?\n",
    "    n_layer: int = 6\n",
    "    n_head: int = 6\n",
    "    n_embd: int = 384\n",
    "    bias: bool = True\n",
    "    attention_type: str = \"global\"    \n",
    "    dropout: float = 0.0\n",
    "    unk_freq_threshold: float = 0.0\n",
    "    SurvLayer = \"Single-Risk\"                                  # \"Competing-Risk\"\n",
    "    tokens_for_univariate_regression = None\n",
    "\n",
    "config = DemoConfig()\n",
    "\n",
    "@dataclass\n",
    "class OptConfig:\n",
    "    batch_size: int = 16\n",
    "    eval_interval: int = 1\n",
    "    learning_rate: float = 3e-4\n",
    "    epochs: int = 30\n",
    "    \n",
    "opt = OptConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data loader on a reduced cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from CPRD.data.database import queries\n",
    "\n",
    "# # Get a list of patients which fit a reduced set of criterion\n",
    "# PATH_TO_DB = \"/rds/projects/s/subramaa-mum-predict/CharlesGadd_Oxford/FoundationModel/preprocessing/processed/cprd.db\"\n",
    "# conn = sqlite3.connect(PATH_TO_DB)\n",
    "# cursor = conn.cursor()\n",
    "# # identifiers1 = queries.query_measurement([\"bmi\", \"diastolic_blood_pressure\"], cursor)        \n",
    "# identifiers2 = queries.query_diagnosis([\"DEPRESSION\", \"TYPE1DM\", \"TYPE2DIABETES\"], cursor)    #  \"DEPRESSION\"  ,  \"ANXIETY\"\n",
    "# # all_identifiers = list(set(identifiers1).intersection(identifiers2))    # Turn smaller list into the set\n",
    "# all_identifiers = identifiers2\n",
    "\n",
    "# if True:\n",
    "#     # Lets take only the first N for faster run-time\n",
    "#     N = np.min((len(all_identifiers), 20000))\n",
    "#     print(f\"Using N={N} random samples, from the available {len(all_identifiers)}\")\n",
    "#     identifiers = random.choices(all_identifiers, k=N)\n",
    "# else:\n",
    "#     print(f\"Using all available {len(all_identifiers)} samples\")\n",
    "#     identifiers = all_identifiers\n",
    "\n",
    "# # Build \n",
    "# dm = FoundationalDataModule(identifiers=identifiers,\n",
    "#                             tokenizer=\"tabular\",\n",
    "#                             batch_size=opt.batch_size,\n",
    "#                             max_seq_length=config.block_size,\n",
    "#                             unk_freq_threshold=config.unk_freq_threshold,\n",
    "#                             include_measurements=True,\n",
    "#                             include_diagnoses=True,\n",
    "#                             preprocess_measurements=True\n",
    "#                            )\n",
    "\n",
    "\n",
    "# vocab_size = dm.train_set.tokenizer.vocab_size\n",
    "\n",
    "# print(f\"{len(dm.train_set)} training samples\")\n",
    "# print(f\"{len(dm.val_set)} validation samples\")\n",
    "# print(f\"{len(dm.test_set)} test samples\")\n",
    "# print(f\"{vocab_size} vocab elements\")\n",
    "# # print(dm.train_set.tokenizer._itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading Polars dataset from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/archive/Version2/polars/\n",
      "INFO:root:Using tokenizer tabular\n",
      "INFO:root:Tokenzier created based on 74.82M tokens\n",
      "INFO:root:Creating dataset\n",
      "INFO:root:Creating hash map\n",
      "Calculating chunk index splits : 27it [00:05,  4.56it/s]\n",
      "INFO:root:Creating dataset\n",
      "INFO:root:Creating hash map\n",
      "Calculating chunk index splits : 2it [00:00,  8.99it/s]\n",
      "INFO:root:Creating dataset\n",
      "INFO:root:Creating hash map\n",
      "Calculating chunk index splits : 2it [00:00, 13.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466364 training patients\n",
      "17841 validation patients\n",
      "22034 test patients\n",
      "184 vocab elements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Get a list of patients which fit a reduced set of criterion\n",
    "path_to_db = \"/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/archive/Version2/\"\n",
    "\n",
    "# Build \n",
    "dm = FoundationalDataModule(path_to_db=path_to_db,\n",
    "                            load=True,\n",
    "                            # include_measurements=True,\n",
    "                            # drop_missing_data=True,\n",
    "                            # include_diagnoses=True,\n",
    "                            # drop_empty_dynamic=True,\n",
    "                            tokenizer=\"tabular\",\n",
    "                            batch_size=opt.batch_size,\n",
    "                            max_seq_length=config.block_size,\n",
    "                            unk_freq_threshold=config.unk_freq_threshold,\n",
    "                            min_workers=1\n",
    "                           )\n",
    "\n",
    "vocab_size = dm.train_set.tokenizer.vocab_size\n",
    "\n",
    "print(f\"{len(dm.train_set)} training patients\")\n",
    "print(f\"{len(dm.val_set)} validation patients\")\n",
    "print(f\"{len(dm.test_set)} test patients\")\n",
    "print(f\"{vocab_size} vocab elements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch loaded in 5.673753976821899 seconds\n",
      "tokens              torch.Size([16, 128])\n",
      "ages                torch.Size([16, 128])\n",
      "values              torch.Size([16, 128])\n",
      "attention_mask      torch.Size([16, 128])\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()   # starting time\n",
    "for batch in dm.train_dataloader():\n",
    "    break\n",
    "print(f\"batch loaded in {time.time()-start} seconds\")    \n",
    "    \n",
    "for key in batch.keys():\n",
    "    print(f\"{key}\".ljust(20) + f\"{batch[key].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardisation\n",
    "\n",
    "This was performed automatically across measurements and tests in the dataloader. The standardisation statistics (bias and scale respectively) are given in the dictionary object. \n",
    "\n",
    "We define two mappings to simplify notation later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(dm.standardisation_dict)\n",
    "\n",
    "# standardise = lambda key, v: (v - dm.standardisation_dict[key][0]) / dm.standardisation_dict[key][1]\n",
    "# unstandardise = lambda key, v: (v * dm.standardisation_dict[key][1]) + dm.standardisation_dict[key][0]\n",
    "\n",
    "# print(standardise(\"bmi\", 30))\n",
    "# print(unstandardise(\"bmi\", standardise(\"bmi\", 20)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the frequency of tokens in the extracted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>EVENT</th><th>COUNT</th><th>FREQUENCY</th></tr><tr><td>str</td><td>u32</td><td>f64</td></tr></thead><tbody><tr><td>&quot;UNK&quot;</td><td>0</td><td>0.0</td></tr><tr><td>&quot;Plasma_N-termi…</td><td>23</td><td>3.0742e-7</td></tr><tr><td>&quot;SICKLE_CELL_DI…</td><td>123</td><td>0.000002</td></tr><tr><td>&quot;CYSTICFIBROSIS…</td><td>127</td><td>0.000002</td></tr><tr><td>&quot;SYSTEMIC_SCLER…</td><td>199</td><td>0.000003</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 3)\n",
       "┌───────────────────────────────────┬───────┬───────────┐\n",
       "│ EVENT                             ┆ COUNT ┆ FREQUENCY │\n",
       "│ ---                               ┆ ---   ┆ ---       │\n",
       "│ str                               ┆ u32   ┆ f64       │\n",
       "╞═══════════════════════════════════╪═══════╪═══════════╡\n",
       "│ UNK                               ┆ 0     ┆ 0.0       │\n",
       "│ Plasma_N-terminal_pro_B-type_nat… ┆ 23    ┆ 3.0742e-7 │\n",
       "│ SICKLE_CELL_DISEASE_V2            ┆ 123   ┆ 0.000002  │\n",
       "│ CYSTICFIBROSIS                    ┆ 127   ┆ 0.000002  │\n",
       "│ SYSTEMIC_SCLEROSIS                ┆ 199   ┆ 0.000003  │\n",
       "└───────────────────────────────────┴───────┴───────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import polars as pl\n",
    "pl.Config.set_tbl_rows(vocab_size + 1)\n",
    "display(dm.tokenizer._event_counts.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the measurements, using the fact that the diagnoses are all up upper case. This is needed for automatically setting the configuration below\n",
    "measurements_for_univariate_regression = [record for record in dm.tokenizer._event_counts[\"EVENT\"] if record.upper() != record]\n",
    "\n",
    "# print(measurements_for_univariate_regression)\n",
    "# print(dm.encode(measurements_for_univariate_regression))\n",
    "# print(dm.decode([7,4,3,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create models and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using Temporal Positional Encoding. This module uses the patient's age at an event within their time series.\n",
      "INFO:root:Using Single-Risk DeSurvival head. This module predicts a separate survival curve for each possible future event\n",
      "INFO:root:Internally scaling time in survival head by 1825 days\n",
      "INFO:root:In generation forwarding DeSurv on the grid between [0.0, 1825.0], with delta=1/300\n",
      "INFO:root:ModuleDict(\n",
      "  (Token 2): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 12): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 13): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 15): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 19): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 33): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 36): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 44): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 45): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 49): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 54): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 55): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 58): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 59): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 61): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 63): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 69): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 71): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 72): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 75): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 80): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 82): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 84): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 87): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 90): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 92): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 94): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 95): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 96): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 97): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 98): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 100): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 102): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 104): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 105): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 110): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 111): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 113): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 114): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 115): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 116): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 117): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 118): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 119): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 120): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 121): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 122): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 123): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 124): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 125): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 126): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 127): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 128): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 129): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 130): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 131): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 132): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 133): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 134): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 135): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 136): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 137): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 138): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 139): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 140): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 141): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 142): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 143): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 144): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 145): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 146): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 147): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 148): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 149): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 150): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 151): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 152): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 153): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 154): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 155): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 156): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 157): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 158): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 159): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 160): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 161): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 162): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 163): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 164): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 165): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 166): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 167): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 168): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 169): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 170): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 171): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 172): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 173): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 174): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 175): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 176): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 177): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 178): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 179): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 180): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 181): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 182): Linear(in_features=384, out_features=2, bias=True)\n",
      "  (Token 183): Linear(in_features=384, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "models, m_names = [], []\n",
    "\n",
    "# My development model\n",
    "for surv_layer in [\"Single-Risk\"]: #, \"Competing-Risk\"]:\n",
    "    \n",
    "    ## Create configuration\n",
    "    config = DemoConfig()\n",
    "    # Specify which survival head layer to use\n",
    "    config.SurvLayer = surv_layer   \n",
    "    # list of univariate measurements to model with Normal distribution\n",
    "    config.tokens_for_univariate_regression = dm.encode(measurements_for_univariate_regression) \n",
    "    \n",
    "    models.append(SurvStreamGPTForCausalModelling(config, vocab_size).to(device))\n",
    "    m_names.append(f\"SurvStreamGPTForCausalModelling: {surv_layer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_curves_train = [[] for _ in models]\n",
    "loss_curves_train_clf = [[] for _ in models]\n",
    "loss_curves_train_surv = [[] for _ in models]\n",
    "loss_curves_train_values = [[] for _ in models]\n",
    "\n",
    "loss_curves_val = [[] for _ in models]\n",
    "loss_curves_val_clf = [[] for _ in models]\n",
    "loss_curves_val_surv = [[] for _ in models]\n",
    "loss_curves_val_values = [[] for _ in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hoelkOrFY8bN",
    "outputId": "961304cd-e379-40d4-dd56-8de0b91d2861"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model `SurvStreamGPTForCausalModelling: Single-Risk`, with 10.865304 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   0%|          | 51/29148 [02:18<21:56:50,  2.72s/it]\n",
      "Validation epoch 0:   0%|          | 21/29148 [00:24<9:28:13,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\tTrain loss 237.65: (4.42, 470.88). Val loss 1091136543880417.75: (3.02, 2182273087760832.75)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   0%|          | 51/29148 [02:19<22:10:05,  2.74s/it]\n",
      "Validation epoch 1:   0%|          | 21/29148 [00:24<9:28:15,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\tTrain loss 60.28: (4.29, 116.28). Val loss 869623810089096.38: (2.99, 1739247620178189.75)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 2:   0%|          | 51/29148 [02:21<22:22:53,  2.77s/it]\n",
      "Validation epoch 2:   0%|          | 21/29148 [00:24<9:27:52,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:\tTrain loss 681.18: (4.27, 1358.09). Val loss 702718261969965.38: (3.04, 1405436523939927.50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 3:   0%|          | 41/29148 [01:49<21:34:49,  2.67s/it]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for m_idx, (model, m_name) in enumerate(zip(models, m_names)):\n",
    "    \n",
    "    print(f\"Training model `{m_name}`, with {sum(p.numel() for p in model.parameters())/1e6} M parameters\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # create a PyTorch optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=opt.learning_rate)\n",
    "\n",
    "    best_val, epochs_since_best = np.inf, 0\n",
    "    for epoch in range(opt.epochs):\n",
    "        \n",
    "        epoch_loss, epoch_surv_loss, epoch_values_loss = 0, 0, 0\n",
    "        model.train()\n",
    "        for i, batch in tqdm(enumerate(dm.train_dataloader()), desc=f\"Training epoch {epoch}\", total=len(dm.train_dataloader())):\n",
    "            if i > 50:\n",
    "                break\n",
    "\n",
    "            # evaluate the loss\n",
    "            _, (losses_desurv, loss_values), loss = model(batch['tokens'].to(device), \n",
    "                                                        ages=batch['ages'].to(device), \n",
    "                                                        values=batch['values'].to(device),\n",
    "                                                        attention_mask=batch['attention_mask'].to(device)   \n",
    "                                                        )\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # record\n",
    "            epoch_loss += loss.item()            \n",
    "            epoch_surv_loss += torch.sum(losses_desurv).item()\n",
    "            epoch_values_loss += loss_values.item()\n",
    "        \n",
    "        epoch_loss /= i\n",
    "        epoch_surv_loss /= i\n",
    "        epoch_values_loss /= i\n",
    "        loss_curves_train[m_idx].append(epoch_loss)\n",
    "        loss_curves_train_surv[m_idx].append(epoch_surv_loss)\n",
    "        loss_curves_train_values[m_idx].append(epoch_values_loss)\n",
    "\n",
    "        # evaluate the loss on val set\n",
    "        with torch.no_grad(): \n",
    "            model.eval()\n",
    "            if epoch % opt.eval_interval == 0 or epoch == opt.epochs - 1:\n",
    "                val_loss, val_surv_loss, val_values_loss = 0, 0, 0\n",
    "                for j, batch in tqdm(enumerate(dm.val_dataloader()), desc=f\"Validation epoch {epoch}\", total=len(dm.train_dataloader())):\n",
    "                    if j > 20:\n",
    "                        break\n",
    "                    _, (losses_desurv, loss_values), loss = model(batch['tokens'].to(device), \n",
    "                                                                  ages=batch['ages'].to(device),\n",
    "                                                                  values=batch['values'].to(device),\n",
    "                                                                  attention_mask=batch['attention_mask'].to(device)   \n",
    "                                                                  )\n",
    "                    # record\n",
    "                    val_loss += loss.item()                    \n",
    "                    val_surv_loss += torch.sum(losses_desurv).item()\n",
    "                    val_values_loss += loss_values.item()\n",
    "                    \n",
    "                val_loss /= j\n",
    "                val_surv_loss /= j\n",
    "                val_values_loss /= j\n",
    "                loss_curves_val[m_idx].append(val_loss)\n",
    "                loss_curves_val_surv[m_idx].append(val_surv_loss)\n",
    "                loss_curves_val_values[m_idx].append(val_values_loss)\n",
    "\n",
    "                print(f\"Epoch {epoch}:\\tTrain loss {epoch_loss:.2f}: ({epoch_surv_loss:.2f}, {epoch_values_loss:.2f}). Val loss {val_loss:.2f}: ({val_surv_loss:.2f}, {val_values_loss:.2f})\")          \n",
    "                # TODO: Note not fully accurate as last batch is likely not the same size, will be fixed with lightning\n",
    "        \n",
    "            if val_loss >= best_val:\n",
    "                epochs_since_best += 1\n",
    "                if epochs_since_best >= 5:\n",
    "                    break\n",
    "            else:\n",
    "                best_val = val_loss\n",
    "                epochs_since_best = 0\n",
    "\n",
    "    # Test trained model with a prompt\n",
    "    # ----------------    \n",
    "    # set context: diagnosis of depression at 20 years old\n",
    "    tokens = torch.from_numpy(np.array(dm.encode([\"DEPRESSION\"])).reshape((1,-1))).to(device)\n",
    "    ages = torch.tensor([[20*365]], device=device)\n",
    "    values = torch.tensor([[torch.nan]], device=device)\n",
    "    \n",
    "    # generate: sample the next 10 tokens\n",
    "    new_tokens, new_ages, new_values = model.generate(tokens, ages, values, max_new_tokens=10)\n",
    "    generated = dm.decode(new_tokens[0].tolist())\n",
    "    # report:\n",
    "    for _cat, _age, _value in zip(generated.split(\" \"), new_ages[0, :], new_values[0, :]):\n",
    "        try:\n",
    "            _value = unstandardise(_cat, _value)\n",
    "        except:\n",
    "            pass\n",
    "        print(f\"{_cat}\".ljust(50) + f\"{_value:.02f}\".ljust(15) + f\"at age {_age/365:.0f} ({_age:.1f} days)\")    # with value {_value}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing output to real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dm.train_dataloader():\n",
    "    break\n",
    "    \n",
    "conditions = batch[\"tokens\"].numpy().tolist()\n",
    "# delta_ages = batch[\"ages\"][:, 1:] - batch[\"ages\"][:, :-1]\n",
    "for idx, (token, _age, _value) in enumerate(zip(conditions[0], batch[\"ages\"][0,:],  batch[\"values\"][0,:])):\n",
    "    if token == 0 or idx >= 10:\n",
    "        break\n",
    "    _cat = dm.decode([token])\n",
    "    try:\n",
    "        _value = unstandardise(_cat, _value)\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    print(f\"{_cat}\".ljust(50) + f\"{_value:.02f}\".ljust(15) + f\"at age {_age/365:.0f} ({_age:.1f} days)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"k\", \"r\", \"b\", \"y\"]\n",
    "\n",
    "# Plot loss\n",
    "plt.figure()\n",
    "for m_idx, _ in enumerate(models):\n",
    "    # Training\n",
    "    iterations = np.linspace(0, len(loss_curves_train[m_idx]), len(loss_curves_train[m_idx])) * opt.eval_interval\n",
    "    plt.plot(iterations, loss_curves_train[m_idx], label=f\"{m_names[m_idx]}-train\", c=cols[m_idx], linestyle='dashed')\n",
    "    # Validation\n",
    "    iterations = np.linspace(0, len(loss_curves_val[m_idx]), len(loss_curves_val[m_idx])) * opt.eval_interval\n",
    "    plt.plot(iterations, loss_curves_val[m_idx], label=f\"{m_names[m_idx]}-val\", c=cols[m_idx])\n",
    "plt.legend()\n",
    "plt.savefig(f\"figs/single_risk/loss.png\")\n",
    "\n",
    "# Plot DeSurv loss\n",
    "plt.figure()\n",
    "for m_idx, _ in enumerate(models):\n",
    "    # Training\n",
    "    iterations = np.linspace(0, len(loss_curves_train_surv[m_idx]), len(loss_curves_train_surv[m_idx])) * opt.eval_interval\n",
    "    plt.plot(iterations, loss_curves_train_surv[m_idx], label=f\"{m_names[m_idx]}-train\", c=cols[m_idx], linestyle='dashed')\n",
    "    # Validation\n",
    "    iterations = np.linspace(0, len(loss_curves_val_surv[m_idx]), len(loss_curves_val_surv[m_idx])) * opt.eval_interval\n",
    "    plt.plot(iterations, loss_curves_val_surv[m_idx], label=f\"{m_names[m_idx]}-val\", c=cols[m_idx])\n",
    "plt.legend()\n",
    "plt.savefig(f\"figs/single_risk/loss_desurv.png\")\n",
    "\n",
    "plt.figure()\n",
    "for m_idx, _ in enumerate(models):\n",
    "    # Training\n",
    "    iterations = np.linspace(0, len(loss_curves_train_values[m_idx]), len(loss_curves_train_values[m_idx])) * opt.eval_interval\n",
    "    plt.plot(iterations, loss_curves_train_values[m_idx], label=f\"{m_names[m_idx]}-train\", c=cols[m_idx], linestyle='dashed')\n",
    "    # Validation\n",
    "    iterations = np.linspace(0, len(loss_curves_val_values[m_idx]), len(loss_curves_val_values[m_idx])) * opt.eval_interval\n",
    "    plt.plot(iterations, loss_curves_val_values[m_idx], label=f\"{m_names[m_idx]}-val\", c=cols[m_idx])\n",
    "plt.legend()\n",
    "plt.savefig(f\"figs/single_risk/loss_val.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diabetes: How related conditions are impacted by each other\n",
    "Probability of type II diabetes before and after a type I diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_token = dm.tokenizer._stoi[\"TYPE1DM\"]\n",
    "t2_token = dm.tokenizer._stoi[\"TYPE2DIABETES\"]\n",
    "\n",
    "\n",
    "base_prompt = [\"DEPRESSION\"]\n",
    "ages_in_years = [20]\n",
    "base_values = [torch.tensor([torch.nan])]\n",
    "\n",
    "to_days = lambda a_list: torch.FloatTensor([365 * _a for _a in a_list]).reshape((1,-1)).to(device)\n",
    "\n",
    "# Create a set of prompts\n",
    "prompts, ages, values, desc = [], [], [], []\n",
    "# control prompt\n",
    "desc.append(\"Depression\")\n",
    "prompts.append(base_prompt)\n",
    "ages.append(ages_in_years)\n",
    "values.append(base_values)\n",
    "# prompt with type 1 diabetes\n",
    "desc.append(\"Depression -> Type 1\")\n",
    "prompts.append(base_prompt + [\"TYPE1DM\"])\n",
    "ages.append(ages_in_years + [21])\n",
    "values.append(base_values + [torch.tensor([torch.nan])])\n",
    "\n",
    "desc.append(\"Depression - > Type 2\")\n",
    "prompts.append(base_prompt + [\"TYPE2DIABETES\"])\n",
    "ages.append(ages_in_years + [21])\n",
    "values.append(base_values + [torch.tensor([torch.nan])])\n",
    "\n",
    "for model_idx, model in enumerate(models):\n",
    "    print(f\"\\n\\n{m_names[model_idx]}\\n--------------------------------------\")\n",
    "    with torch.no_grad(): \n",
    "        model.eval()\n",
    "\n",
    "        prompt_survs = []\n",
    "        for p_idx, (prompt, age, value) in enumerate(zip(prompts, ages, values)):\n",
    "            print(f\"\\n{desc[p_idx]}: \\t ({','.join(prompt)}): \")\n",
    "            encoded_prompt = torch.from_numpy(np.array(dm.encode(prompt)).reshape((1,-1))).to(device)\n",
    "            (surv, val_dist), _, _ = model(encoded_prompt,\n",
    "                                           values=torch.tensor(value).to(device),\n",
    "                                           ages=to_days(age),\n",
    "                                           is_generation=True)\n",
    "            prompt_survs.append(surv)\n",
    "\n",
    "        for si, _ in enumerate(surv):\n",
    "            plt.close()\n",
    "            event_name = dm.decode([si + 1])\n",
    "            for p_idx in range(len(prompts)):\n",
    "                plt.plot(model.surv_layer.t_eval / 365, prompt_survs[p_idx][si][0, :], label=f\"{desc[p_idx]}\")\n",
    "            plt.legend()\n",
    "            plt.savefig(f\"figs/single_risk/diabetes/{event_name}.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Values: How increasing BMI affects diagnosis risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_of_interest = [\"bmi\", \"diastolic_blood_pressure\", \n",
    "                      \"TYPE1DM\", \"TYPE2DIABETES\",\n",
    "                      \"HYPERTENSION\", \"OSTEOARTHRITIS\",\n",
    "                      \"CKDSTAGE3TO5\",\n",
    "                      \"HF\", \"ISCHAEMICSTROKE\"\n",
    "                     ]\n",
    "prompt = [\"bmi\"]\n",
    "values = [torch.tensor([standardise(_cat, v) for _cat in prompt], device=device) for v in [12.,15.,18.,21.,24.,30.,40.]]\n",
    "age = [40]\n",
    "\n",
    "for model_idx, model in enumerate(models):\n",
    "    print(f\"\\n\\n{m_names[model_idx]}\\n--------------------------------------\")\n",
    "    with torch.no_grad(): \n",
    "        model.eval()\n",
    "\n",
    "        prompt_survs = []\n",
    "        for p_idx, value in enumerate(values):\n",
    "            print(f\"Value {value}\\n======\")\n",
    "            encoded_prompt = torch.from_numpy(np.array(dm.encode(prompt)).reshape((1,-1))).to(device)\n",
    "            (surv, val_dist), _, _ = model(encoded_prompt,\n",
    "                                           values=value,\n",
    "                                           ages=to_days(age),\n",
    "                                           is_generation=True)\n",
    "            prompt_survs.append(surv)\n",
    "\n",
    "        for si, _ in enumerate(surv):\n",
    "            plt.close()\n",
    "            event_name = dm.decode([si + 1])\n",
    "            \n",
    "            if event_name in events_of_interest:\n",
    "                \n",
    "                for p_idx in range(len(prompt_survs)):\n",
    "                    bmi_value = unstandardise(\"bmi\", values[p_idx])\n",
    "                    plt.plot(model.surv_layer.t_eval / 365, prompt_survs[p_idx][si][0, :], label=f\"BMI {bmi_value.item():.2f}\")\n",
    "                plt.xlabel(\"t (years)\")\n",
    "                plt.ylabel(\"P(T>t) ()\")\n",
    "                plt.legend()\n",
    "                plt.savefig(f\"figs/single_risk/bmi/{event_name}.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Values: How increasing diastolic_blood_pressure affects likelihood of diagnoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_of_interest = [\"bmi\", \"diastolic_blood_pressure\", \n",
    "                      \"TYPE1DM\", \"TYPE2DIABETES\",\n",
    "                      \"HYPERTENSION\", \"OSTEOARTHRITIS\",\n",
    "                      \"CKDSTAGE3TO5\",\n",
    "                      \"HF\", \"ISCHAEMICSTROKE\"\n",
    "                     ]\n",
    "\n",
    "prompt = [\"diastolic_blood_pressure\"]\n",
    "values = [torch.tensor([standardise(_cat, _value) for _cat in prompt], device=device) for _value in [60.,70.,80.,90.,100.,120.]]\n",
    "age = [40]\n",
    "\n",
    "\n",
    "for model_idx, model in enumerate(models):\n",
    "    print(f\"\\n\\n{m_names[model_idx]}\\n--------------------------------------\")\n",
    "    with torch.no_grad(): \n",
    "        model.eval()\n",
    "\n",
    "        prompt_survs = []\n",
    "        for p_idx, value in enumerate(values):\n",
    "            print(f\"Value {value}\\n======\")\n",
    "            encoded_prompt = torch.from_numpy(np.array(dm.encode(prompt)).reshape((1,-1))).to(device)\n",
    "            (surv, val_dist), _, _ = model(encoded_prompt,\n",
    "                                           values=value,\n",
    "                                           ages=to_days(age),\n",
    "                                           is_generation=True)\n",
    "            prompt_survs.append(surv)\n",
    "\n",
    "        for si, _ in enumerate(surv):\n",
    "            plt.close()\n",
    "            event_name = dm.decode([si + 1])\n",
    "            \n",
    "            if event_name in events_of_interest:\n",
    "                \n",
    "                for p_idx in range(len(prompt_survs)):\n",
    "                    dbp_value = unstandardise(\"diastolic_blood_pressure\", values[p_idx])\n",
    "                    plt.plot(model.surv_layer.t_eval / 365, prompt_survs[p_idx][si][0, :], label=f\"DBP {dbp_value.item():.2f}\")\n",
    "                plt.xlabel(\"t (years)\")\n",
    "                plt.ylabel(\"P(T>t) ()\")\n",
    "                plt.legend()\n",
    "                plt.savefig(f\"figs/single_risk/diastolic_blood_pressure/{event_name}.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Values: How varying diagnosis affects value of diastolic_blood_pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements_of_interest = [\"diastolic_blood_pressure\"]\n",
    "t1_token = dm.tokenizer._stoi[\"diastolic_blood_pressure\"]\n",
    "\n",
    "diagnoses = [[\"DEPRESSION\"],[\"TYPE2DIABETES\"], [\"HF\"], [\"HYPERTENSION\"]]\n",
    "values = torch.tensor([torch.nan], device=device)\n",
    "age = [40]\n",
    "\n",
    "for model_idx, model in enumerate(models):\n",
    "    print(f\"\\n\\n{m_names[model_idx]}\\n--------------------------------------\")\n",
    "    with torch.no_grad(): \n",
    "        model.eval()\n",
    "\n",
    "    for p_idx, diagnosis in enumerate(diagnoses):\n",
    "        print(f\"\\nDiagnosis {diagnosis}\\n======\")\n",
    "        encoded_prompt = torch.from_numpy(np.array(dm.encode(diagnosis)).reshape((1,-1))).to(device)\n",
    "        (surv, val_dist), _, _ = model(encoded_prompt,\n",
    "                                       values=values,\n",
    "                                       ages=to_days(age),\n",
    "                                       is_generation=True)\n",
    "        dist = val_dist[model.value_layer.token_key(t1_token)]\n",
    "        print(f\"standardised diastolic_blood_pressure ~ N({dist.loc.item():.1f}, {dist.scale.item():.1f})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Values: How increasing bmi affects value of diastolic_blood_pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_token = dm.tokenizer._stoi[\"diastolic_blood_pressure\"]\n",
    "\n",
    "prompt = [\"bmi\"]\n",
    "values = [torch.tensor([standardise(_cat, _value) for _cat in prompt], device=device) for _value in [12.,15.,18.,21.,24.,30.,40.,50.]]\n",
    "age = [40]\n",
    "\n",
    "for model_idx, model in enumerate(models):\n",
    "    print(f\"\\n\\n{m_names[model_idx]}\\n--------------------------------------\")\n",
    "\n",
    "    for p_idx, value in enumerate(values):\n",
    "        print(f\"\\nValues {value.tolist()}\\n======\")\n",
    "        encoded_prompt = torch.from_numpy(np.array(dm.encode(prompt)).reshape((1,-1))).to(device)\n",
    "        (surv, val_dist), _, _ = model(encoded_prompt,\n",
    "                                       values=value,\n",
    "                                       ages=to_days(age),\n",
    "                                       is_generation=True)\n",
    "        \n",
    "        dist = val_dist[model.value_layer.token_key(t1_token)]\n",
    "        print(f\"standardised diastolic_blood_pressure ~ N({dist.loc.item():.1f}, {dist.scale.item():.1f})\")\n",
    "        # print(f\"\\tprobability of type II diabetes: {100*float(probs[0, 0, t2_token].cpu().detach().numpy()):.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: model architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_idx, model in enumerate(models):\n",
    "    print(f\"\\n\\n{m_names[model_idx]}\\n\" + \"=\"*len(m_names[model_idx]))\n",
    "    print(f\"\\n\\n{model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to html --no-input single_risk.ipynb"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "sys_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
