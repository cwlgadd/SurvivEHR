{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJpXpmjEYC_T"
   },
   "source": [
    "# Demo Notebook:\n",
    "## Survival Transformer For Causal Sequence Modelling \n",
    "\n",
    "Including time, and excluding tabular values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added path '/rds/homes/g/gaddcz/Projects/CPRD/virtual-env-icelake/lib/python3.10/site-packages' at start of search paths.\n",
      "/rds/homes/g/gaddcz/Projects/CPRD/examples/modelling/SurvStreamGPT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "node_type = os.getenv('BB_CPU')\n",
    "venv_dir = f'/rds/homes/g/gaddcz/Projects/CPRD/virtual-env-{node_type}'\n",
    "venv_site_pkgs = Path(venv_dir) / 'lib' / f'python{sys.version_info.major}.{sys.version_info.minor}' / 'site-packages'\n",
    "if venv_site_pkgs.exists():\n",
    "    sys.path.insert(0, str(venv_site_pkgs))\n",
    "    print(f\"Added path '{venv_site_pkgs}' at start of search paths.\")\n",
    "else:\n",
    "    print(f\"Path '{venv_site_pkgs}' not found. Check that it exists and/or that it exists for node-type '{node_type}'.\")\n",
    "\n",
    "!pwd\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda.\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sqlite3\n",
    "from dataclasses import dataclass\n",
    "import logging\n",
    "from CPRD.data.foundational_loader import FoundationalDataModule\n",
    "from CPRD.src.models.survival.task_heads.causal import SurvStreamGPTForCausalModelling\n",
    "from pycox.evaluation import EvalSurv\n",
    "from tqdm import tqdm \n",
    "\n",
    "# TODO:\n",
    "# replace experiment boilerplate with pytorch lightning\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = \"cpu\"    # if more informative debugging statements are needed\n",
    "print(f\"Using device: {device}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set config to be equivalent architecture of kaparthy benchmark, however they are not comparable tasks.\n",
    "@dataclass\n",
    "class DemoConfig:\n",
    "    block_size: int = 128        # what is the maximum context length for predictions?\n",
    "    n_layer: int = 6\n",
    "    n_head: int = 6\n",
    "    n_embd: int = 384\n",
    "    bias: bool = True\n",
    "    attention_type: str = \"global\"    \n",
    "    dropout: float = 0.0\n",
    "    unk_freq_threshold: float = 0.0\n",
    "    SurvLayer = \"sr\"                                  # \"Competing-Risk\"\n",
    "    tokens_for_univariate_regression = None\n",
    "\n",
    "config = DemoConfig()\n",
    "\n",
    "@dataclass\n",
    "class OptConfig:\n",
    "    batch_size: int = 64\n",
    "    eval_interval: int = 1\n",
    "    learning_rate: float = 3e-4\n",
    "    epochs: int = 30\n",
    "    \n",
    "opt = OptConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading Polars dataset from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/polars/\n",
      "INFO:root:Using tokenizer tabular\n",
      "INFO:root:Tokenzier created based on 3584.43M tokens\n",
      "INFO:root:Creating split=train/ dataset\n",
      "INFO:root:\t Loading split=train/ hash map for parquet\n",
      "INFO:root:\t Hash map created for split=train/ with 22,912,046 samples\n",
      "INFO:root:Creating split=test/ dataset\n",
      "INFO:root:\t Loading split=test/ hash map for parquet\n",
      "INFO:root:\t Hash map created for split=test/ with 1,207,449 samples\n",
      "INFO:root:Creating split=val/ dataset\n",
      "INFO:root:\t Loading split=val/ hash map for parquet\n",
      "INFO:root:\t Hash map created for split=val/ with 1,226,576 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184 vocab elements\n"
     ]
    }
   ],
   "source": [
    "# Get a list of patients which fit a reduced set of criterion\n",
    "# path_to_db = \"/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/archive/Version2/\"\n",
    "path_to_db = \"/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/\"\n",
    "\n",
    "# Build \n",
    "dm = FoundationalDataModule(path_to_db=path_to_db,\n",
    "                            load=True,\n",
    "                            tokenizer=\"tabular\",\n",
    "                            batch_size=opt.batch_size,\n",
    "                            max_seq_length=config.block_size,\n",
    "                            unk_freq_threshold=config.unk_freq_threshold,\n",
    "                            min_workers=20,\n",
    "                           )\n",
    "\n",
    "vocab_size = dm.train_set.tokenizer.vocab_size\n",
    "print(f\"{vocab_size} vocab elements\")\n",
    "\n",
    "# Extract the measurements, using the fact that the diagnoses are all up upper case. This is needed for automatically setting the configuration below\n",
    "measurements_for_univariate_regression = [record for record in dm.tokenizer._event_counts[\"EVENT\"] if record.upper() != record]\n",
    "# display(measurements_for_univariate_regression)\n",
    "# list of univariate measurements to model with Normal distribution\n",
    "config.tokens_for_univariate_regression = dm.encode(measurements_for_univariate_regression) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to retrieve sample index 6546 was 0.05623459815979004 seconds\n",
      "\n",
      "SEX                 | M\n",
      "IMD                 | 4.0\n",
      "ETHNICITY           | ASIAN\n",
      "birth_year          | 1988.0\n",
      "\n",
      "Token                                                                      | Age               | Standardised value\n",
      "===================================================================================================================\n",
      "Body_mass_index_3                                                          | 8805              | -0.14             \n",
      "O_E___height_1                                                             | 8805              | 0.25              \n",
      "O_E___weight_2                                                             | 8805              | 0.02              \n",
      "Diastolic_blood_pressure_5                                                 | 11070             | -0.06             \n",
      "Systolic_blood_pressure_4                                                  | 11070             | -0.22             \n",
      "Diastolic_blood_pressure_5                                                 | 11081             | -0.13             \n",
      "Systolic_blood_pressure_4                                                  | 11081             | -0.16             \n",
      "Basophil_count_22                                                          | 11085             | 0.03              \n",
      "Eosinophil_count_21                                                        | 11085             | -0.21             \n",
      "Erythrocyte_sedimentation_rate_61                                          | 11085             | -0.08             \n",
      "GFR_calculated_abbreviated_MDRD_34                                         | 11085             | 0.25              \n",
      "Haematocrit_15                                                             | 11085             | 0.08              \n"
     ]
    }
   ],
   "source": [
    "dm.train_set.view_sample(6546, max_dynamic_events=12, report_time=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create models and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using Temporal Positional Encoding. This module uses the patient's age at an event within their time series.\n",
      "INFO:root:Using Single-Risk DeSurvival head. This module predicts a separate survival curve for each possible future event\n",
      "INFO:root:Internally scaling time in survival head by 1825 days\n",
      "INFO:root:In generation forwarding DeSurv on the grid between [0.0, 1825.0], with delta=1/300\n"
     ]
    }
   ],
   "source": [
    "model = SurvStreamGPTForCausalModelling(config, vocab_size).to(device)\n",
    "\n",
    "loss_curves_train = []\n",
    "loss_curves_train_surv = []\n",
    "loss_curves_train_values = []\n",
    "\n",
    "loss_curves_val = []\n",
    "loss_curves_val_surv = []\n",
    "loss_curves_val_values = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hoelkOrFY8bN",
    "outputId": "961304cd-e379-40d4-dd56-8de0b91d2861"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with 11.167512 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   0%|          | 1001/358001 [10:11<60:32:47,  1.64it/s]\n",
      "Validation epoch 0:   1%|          | 101/19166 [00:39<2:04:25,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\tTrain loss -2.85: (2.93, -8.63). Val loss -5.16: (2.59, -12.91)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   0%|          | 394/358001 [04:00<60:37:38,  1.64it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_243354/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">308677829.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">19</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_243354/308677829.py'</span>                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/rds/bear-apps/2022a/EL8-ice/software/PyTorch/1.12.1-foss-2022a-CUDA-11.7.0/lib/python3.10/site-</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1130</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1127 │   │   # this function, and just call forward.</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1128 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">o</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1129 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1130 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1131 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1132 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1133 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/rds/homes/g/gaddcz/Projects/CPRD/src/models/survival/task_heads/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">causal.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">101</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 98 │   │   │   │   │   │   │   │   │   │   │   │   │     </span>is_generation=is_generation)         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 99 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">100 │   │   # regression head (values of next token if applicable)</span>                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>101 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>values_dist, loss_values = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.value_layer.predict(hidden_states,                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">102 │   │   │   │   │   │   │   │   │   │   │   │   │   │   │   </span>target_tokens=tokens,          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">103 │   │   │   │   │   │   │   │   │   │   │   │   │   │   │   </span>target_values=values,          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">104 │   │   │   │   │   │   │   │   │   │   │   │   │   │   │   </span>attention_mask=attention_mas   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/rds/homes/g/gaddcz/Projects/CPRD/src/modules/head_layers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">value_layers.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">109</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">predict</span>         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">106 │   │   │   │   # print(token_ll_per_patient.shape)</span>                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">107 │   │   │   │   </span>                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">108 │   │   │   │   # average across batch</span>                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>109 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>loss += -token_ll_per_patient.mean()                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">110 │   │   │   </span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">111 │   │   │   # loss /= len(self.measurement_tokens)</span>                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">112 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_243354/\u001b[0m\u001b[1;33m308677829.py\u001b[0m:\u001b[94m19\u001b[0m in \u001b[92m<module>\u001b[0m                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_243354/308677829.py'\u001b[0m                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/rds/bear-apps/2022a/EL8-ice/software/PyTorch/1.12.1-foss-2022a-CUDA-11.7.0/lib/python3.10/site-\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33mpackages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1130\u001b[0m in \u001b[92m_call_impl\u001b[0m                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1127 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# this function, and just call forward.\u001b[0m                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1128 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_pre_hooks \u001b[95mo\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1129 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1130 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*\u001b[96minput\u001b[0m, **kwargs)                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1131 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1132 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1133 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m _global_backward_hooks:                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/rds/homes/g/gaddcz/Projects/CPRD/src/models/survival/task_heads/\u001b[0m\u001b[1;33mcausal.py\u001b[0m:\u001b[94m101\u001b[0m in \u001b[92mforward\u001b[0m        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 98 \u001b[0m\u001b[2m│   │   │   │   │   │   │   │   │   │   │   │   │     \u001b[0mis_generation=is_generation)         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 99 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m100 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# regression head (values of next token if applicable)\u001b[0m                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m101 \u001b[2m│   │   \u001b[0mvalues_dist, loss_values = \u001b[96mself\u001b[0m.value_layer.predict(hidden_states,                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m102 \u001b[0m\u001b[2m│   │   │   │   │   │   │   │   │   │   │   │   │   │   │   \u001b[0mtarget_tokens=tokens,          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m103 \u001b[0m\u001b[2m│   │   │   │   │   │   │   │   │   │   │   │   │   │   │   \u001b[0mtarget_values=values,          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m104 \u001b[0m\u001b[2m│   │   │   │   │   │   │   │   │   │   │   │   │   │   │   \u001b[0mattention_mask=attention_mas   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/rds/homes/g/gaddcz/Projects/CPRD/src/modules/head_layers/\u001b[0m\u001b[1;33mvalue_layers.py\u001b[0m:\u001b[94m109\u001b[0m in \u001b[92mpredict\u001b[0m         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m106 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[2m# print(token_ll_per_patient.shape)\u001b[0m                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m107 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m108 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[2m# average across batch\u001b[0m                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m109 \u001b[2m│   │   │   │   \u001b[0mloss += -token_ll_per_patient.mean()                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m110 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m111 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# loss /= len(self.measurement_tokens)\u001b[0m                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m112 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for m_idx, (model, m_name) in enumerate(zip(models, m_names)):\n",
    "    \n",
    "print(f\"Training model with {sum(p.numel() for p in model.parameters())/1e6} M parameters\")\n",
    "model = model.to(device)\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=opt.learning_rate)\n",
    "\n",
    "best_val, epochs_since_best = np.inf, 0\n",
    "for epoch in range(opt.epochs):\n",
    "    \n",
    "    epoch_loss, epoch_surv_loss, epoch_values_loss = 0, 0, 0\n",
    "    model.train()\n",
    "    for i, batch in tqdm(enumerate(dm.train_dataloader()), desc=f\"Training epoch {epoch}\", total=len(dm.train_dataloader())):\n",
    "        if i > 1000:\n",
    "            break\n",
    "            \n",
    "        # evaluate the loss\n",
    "        _, (losses_desurv, loss_values), loss = model(tokens=batch['tokens'].to(device), \n",
    "                                                      ages=batch['ages'].to(device), \n",
    "                                                      values=batch['values'].to(device),\n",
    "                                                      covariates=batch[\"static_covariates\"].to(device),\n",
    "                                                      attention_mask=batch['attention_mask'].to(device)   \n",
    "                                                      )\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # record\n",
    "        epoch_loss += loss.item()            \n",
    "        epoch_surv_loss += torch.sum(losses_desurv).item()\n",
    "        epoch_values_loss += loss_values.item()\n",
    "    \n",
    "    epoch_loss /= i\n",
    "    epoch_surv_loss /= i\n",
    "    epoch_values_loss /= i\n",
    "    loss_curves_train.append(epoch_loss)\n",
    "    loss_curves_train_surv.append(epoch_surv_loss)\n",
    "    loss_curves_train_values.append(epoch_values_loss)\n",
    "\n",
    "    # evaluate the loss on val set\n",
    "    with torch.no_grad(): \n",
    "        model.eval()\n",
    "        if epoch % opt.eval_interval == 0 or epoch == opt.epochs - 1:\n",
    "            val_loss, val_surv_loss, val_values_loss = 0, 0, 0\n",
    "            for j, batch in tqdm(enumerate(dm.val_dataloader()), desc=f\"Validation epoch {epoch}\", total=len(dm.val_dataloader())):\n",
    "                if j > 100:\n",
    "                    break\n",
    "                _, (losses_desurv, loss_values), loss = model(tokens=batch['tokens'].to(device), \n",
    "                                                              ages=batch['ages'].to(device),\n",
    "                                                              values=batch['values'].to(device),\n",
    "                                                              covariates=batch[\"static_covariates\"].to(device),\n",
    "                                                              attention_mask=batch['attention_mask'].to(device)   \n",
    "                                                              )\n",
    "                # record\n",
    "                val_loss += loss.item()                    \n",
    "                val_surv_loss += torch.sum(losses_desurv).item()\n",
    "                val_values_loss += loss_values.item()\n",
    "                \n",
    "            val_loss /= j\n",
    "            val_surv_loss /= j\n",
    "            val_values_loss /= j\n",
    "            loss_curves_val.append(val_loss)\n",
    "            loss_curves_val_surv.append(val_surv_loss)\n",
    "            loss_curves_val_values.append(val_values_loss)\n",
    "\n",
    "            print(f\"Epoch {epoch}:\\tTrain loss {epoch_loss:.2f}: ({epoch_surv_loss:.2f}, {epoch_values_loss:.2f}). Val loss {val_loss:.2f}: ({val_surv_loss:.2f}, {val_values_loss:.2f})\")          \n",
    "            # TODO: Note not fully accurate as last batch is likely not the same size, will be fixed with lightning\n",
    "\n",
    "        if val_loss >= best_val:\n",
    "            epochs_since_best += 1\n",
    "            if epochs_since_best >= 5:\n",
    "                break\n",
    "        else:\n",
    "            best_val = val_loss\n",
    "            epochs_since_best = 0\n",
    "\n",
    "            # Save best seen model\n",
    "            torch.save(model.state_dict(), path_to_db + \"polars/SR.pt\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:   0%|          | 1001/358001 [02:54<17:16:02,  5.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# quick check to see if bottlenecked by cpu or gpu\n",
    "for i, batch in tqdm(enumerate(dm.train_dataloader()), desc=f\"Training epoch {epoch}\", total=len(dm.train_dataloader())):\n",
    "    if i > 1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default context start\n",
    "baseline_covariates = {\"sex\": \"F\", \"deprivation\": 4.0, \"ethnicity\": \"WHITE\", \"year_of_birth\": 1997}\n",
    "prompt = [\"O_E___height_1\", \"O_E___weight_2\"]\n",
    "values = [163, 90]\n",
    "ages_in_years = [18.2, 18.2]\n",
    "\n",
    "# define encoding functions (TODO: add this wrap to datamodule\n",
    "encode_prompt = lambda prompt_list: torch.from_numpy(np.array(dm.encode(prompt_list)).reshape((1,-1))).to(device)\n",
    "encode_value = lambda prompt_list, value_list: torch.tensor(np.array([dm.standardise(_cat, _val) for _cat, _val in zip(prompt_list, value_list) ]).reshape((1,-1)), dtype=torch.float32).to(device)\n",
    "encode_age = lambda age_list: torch.tensor([365 * _age for _age in age_list], dtype=torch.int64).reshape((1,-1)).to(device)\n",
    "\n",
    "# Convert for model\n",
    "covariates = dm.train_set._encode_covariates(**baseline_covariates).reshape(1,-1).to(device)\n",
    "tokens = encode_prompt(prompt)\n",
    "values_scaled = encode_value(prompt, values)\n",
    "ages_in_days = encode_age(ages_in_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline covariates: \n",
      "{'sex': 'F', 'deprivation': 4.0, 'ethnicity': 'WHITE', 'year_of_birth': 1997}\n",
      "==========================================================================================\n",
      "PROMPT:\n",
      "O_E___height_1                                    -0.04          at age 18 (6643.0 days)\n",
      "O_E___weight_2                                    0.16           at age 18 (6643.0 days)\n",
      "==========================================================================================\n",
      "GENERATION\n",
      "Free_T4_level_76                                  -0.11          at age 22 (7912.6 days)\n",
      "TSH_level_74                                      0.05           at age 22 (7961.4 days)\n",
      "Non_HDL_cholesterol_level_108                     0.19           at age 24 (8761.0 days)\n",
      "Serum_urea_level_29                               -0.17          at age 24 (8846.4 days)\n",
      "25_Hydroxyvitamin_D2_level_92                     0.24           at age 27 (9694.8 days)\n",
      "COPD                                              nan            at age 27 (9823.0 days)\n",
      "Diastolic_blood_pressure_5                        0.05           at age 30 (10848.4 days)\n",
      "Plasma_urea_level_30                              -0.13          at age 32 (11672.4 days)\n",
      "Diastolic_blood_pressure_5                        0.12           at age 34 (12514.7 days)\n",
      "ADDISON_DISEASE                                   nan            at age 37 (13643.9 days)\n"
     ]
    }
   ],
   "source": [
    "# generate: sample the next 10 tokens\n",
    "new_tokens, new_ages, new_values = model.generate(tokens, ages_in_days, values_scaled, covariates, max_new_tokens=10)\n",
    "\n",
    "# report:\n",
    "print(f\"Baseline covariates: \\n{baseline_covariates}\\n\" + \"=\"*90)\n",
    "print(f\"PROMPT:\")\n",
    "for _idx, (_cat, _age, _value) in enumerate(zip(dm.decode(new_tokens[0].tolist()).split(\" \"), \n",
    "                                                new_ages[0, :], \n",
    "                                                new_values[0, :]\n",
    "                                               )\n",
    "                                           ):\n",
    "    # _value = dm.unstandardise(_cat, _value)\n",
    "    print(f\"{_cat}\".ljust(50) + f\"{_value:.02f}\".ljust(15) + f\"at age {_age/365:.0f} ({_age:.1f} days)\")    # with value {_value}\n",
    "    if _idx == tokens.shape[-1] - 1:\n",
    "        print(\"=\"*90)\n",
    "        print(f\"GENERATION\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing generation to real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to retrieve sample index 1 was 0.11182141304016113 seconds\n",
      "\n",
      "SEX                 | F\n",
      "IMD                 | 4.0\n",
      "ETHNICITY           | WHITE\n",
      "birth_year          | 1997.0\n",
      "\n",
      "Token                                                                      | Age               | Standardised value\n",
      "===================================================================================================================\n",
      "Serum_total_25_hydroxy_vitamin_D_level_87                                  | 8032              | 0.34              \n",
      "Body_mass_index_3                                                          | 8168              | 0.31              \n",
      "Diastolic_blood_pressure_5                                                 | 8168              | 0.22              \n",
      "O_E___height_1                                                             | 8168              | -0.03             \n",
      "O_E___weight_2                                                             | 8168              | 0.25              \n",
      "Systolic_blood_pressure_4                                                  | 8168              | 0.07              \n",
      "Basophil_count_22                                                          | 8172              | 0.06              \n",
      "Eosinophil_count_21                                                        | 8172              | -0.17             \n",
      "GFR_calculated_abbreviated_MDRD_34                                         | 8172              | 0.04              \n",
      "Haematocrit_15                                                             | 8172              | -0.21             \n"
     ]
    }
   ],
   "source": [
    "dm.train_set.view_sample(1, max_dynamic_events=10, report_time=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot loss\n",
    "plt.figure()\n",
    "# Training\n",
    "iterations = np.linspace(0, len(loss_curves_train), len(loss_curves_train)) * opt.eval_interval\n",
    "plt.plot(iterations, loss_curves_train, label=\"train\")\n",
    "# Validation\n",
    "iterations = np.linspace(0, len(loss_curves_val), len(loss_curves_val)) * opt.eval_interval\n",
    "plt.plot(iterations, loss_curves_val, label=\"val\", linestyle='dashed')\n",
    "plt.legend()\n",
    "plt.savefig(f\"figs/single_risk/loss.png\")\n",
    "\n",
    "# Plot DeSurv loss\n",
    "plt.figure()\n",
    "# Training\n",
    "iterations = np.linspace(0, len(loss_curves_train_surv), len(loss_curves_train_surv)) * opt.eval_interval\n",
    "plt.plot(iterations, loss_curves_train_surv, label=\"train\")\n",
    "# Validation\n",
    "iterations = np.linspace(0, len(loss_curves_val_surv), len(loss_curves_val_surv)) * opt.eval_interval\n",
    "plt.plot(iterations, loss_curves_val_surv, label=\"val\", linestyle='dashed')\n",
    "plt.legend()\n",
    "plt.savefig(f\"figs/single_risk/loss_desurv.png\")\n",
    "\n",
    "# Plot value loss\n",
    "plt.figure()\n",
    "# Training\n",
    "iterations = np.linspace(0, len(loss_curves_train_values), len(loss_curves_train_values)) * opt.eval_interval\n",
    "plt.plot(iterations, loss_curves_train_values, label=\"train\", )\n",
    "# Validation\n",
    "iterations = np.linspace(0, len(loss_curves_val_values), len(loss_curves_val_values)) * opt.eval_interval\n",
    "plt.plot(iterations, loss_curves_val_values, label=\"val\", linestyle='dashed')\n",
    "plt.legend()\n",
    "plt.savefig(f\"figs/single_risk/loss_val.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnoses: How related conditions are impacted by each other\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_prompts = [[\"DEPRESSION\"], [\"TYPE1DM\"], [\"TYPE2DIABETES\"], [\"TYPE2DIABETES\"], [\"Never_smoked_tobacco_85\"]]\n",
    "exp_ages = [[20] for _ in range(len(exp_prompts))]\n",
    "exp_values = [[np.nan] for _ in range(len(exp_prompts))]\n",
    "\n",
    "with torch.no_grad(): \n",
    "    model.eval()\n",
    "\n",
    "    _exp_survs = []\n",
    "    for p_idx, (_exp_prompt, _exp_age, _exp_value) in enumerate(zip(exp_prompts, \n",
    "                                                                    exp_ages, \n",
    "                                                                    exp_values)):\n",
    "\n",
    "        _tokens = encode_prompt(_exp_prompt)\n",
    "        _values_scaled = encode_value(_exp_prompt, _exp_value)\n",
    "        _ages_in_days = encode_age(_exp_age)\n",
    "        \n",
    "        (surv, val_dist), _, _ = model(_tokens,\n",
    "                                       values=_values_scaled,\n",
    "                                       ages=_ages_in_days,\n",
    "                                       covariates=covariates,\n",
    "                                       is_generation=True)\n",
    "        _exp_survs.append(surv)\n",
    "\n",
    "    for si, _ in enumerate(surv):\n",
    "        plt.close()\n",
    "        event_name = dm.decode([si + 1])\n",
    "        for p_idx in range(len(exp_prompts)):\n",
    "            plt.plot(model.surv_layer.t_eval / 365, _exp_survs[p_idx][si][0, :], label=f\"{'->'.join(exp_prompts[p_idx]).lower()}\")\n",
    "        plt.legend()\n",
    "        plt.savefig(f\"figs/single_risk/diabetes/{event_name}.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Values: How increasing BMI affects diagnosis risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_prompts = [[\"DEPRESSION\"], [\"TYPE1DM\"], [\"TYPE2DIABETES\"], [\"TYPE2DIABETES\"], [\"Never_smoked_tobacco_85\"]]\n",
    "exp_ages = [[20] for _ in range(len(exp_prompts))]\n",
    "exp_values = [[np.nan] for _ in range(len(exp_prompts))]\n",
    "\n",
    "with torch.no_grad(): \n",
    "    model.eval()\n",
    "\n",
    "    _exp_survs = []\n",
    "    for p_idx, (_exp_prompt, _exp_age, _exp_value) in enumerate(zip(exp_prompts, \n",
    "                                                                    exp_ages, \n",
    "                                                                    exp_values)):\n",
    "\n",
    "        _tokens = encode_prompt(_exp_prompt)\n",
    "        _values_scaled = encode_value(_exp_prompt, _exp_value)\n",
    "        _ages_in_days = encode_age(_exp_age)\n",
    "        \n",
    "        (surv, val_dist), _, _ = model(_tokens,\n",
    "                                       values=_values_scaled,\n",
    "                                       ages=_ages_in_days,\n",
    "                                       covariates=covariates,\n",
    "                                       is_generation=True)\n",
    "        _exp_survs.append(surv)\n",
    "\n",
    "    for si, _ in enumerate(surv):\n",
    "        plt.close()\n",
    "        event_name = dm.decode([si + 1])\n",
    "        for p_idx in range(len(exp_prompts)):\n",
    "            plt.plot(model.surv_layer.t_eval / 365, _exp_survs[p_idx][si][0, :], label=f\"{'->'.join(exp_prompts[p_idx]).lower()}\")\n",
    "        plt.legend()\n",
    "        plt.savefig(f\"figs/single_risk/diabetes/{event_name}.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Values: How increasing DBP affects diagnosis risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_of_interest = [\"Body_mass_index_3\", \"Diastolic_blood_pressure_5\", \n",
    "                      \"TYPE1DM\", \"TYPE2DIABETES\",\n",
    "                      \"HYPERTENSION\", \"OSTEOARTHRITIS\",\n",
    "                      \"CKDSTAGE3TO5\",\n",
    "                      \"HF_V3\", \"ISCHAEMICSTROKE_V2\"\n",
    "                     ]\n",
    "\n",
    "\n",
    "_exp_prompt = [\"Diastolic_blood_pressure_5\"]\n",
    "_exp_age = [40]\n",
    "_exp_values = [[60.], [70.], [80.], [90.], [100.], [110.]]\n",
    "\n",
    "with torch.no_grad(): \n",
    "    model.eval()\n",
    "\n",
    "    _exp_survs = []\n",
    "    for p_idx, _exp_value in enumerate(_exp_values):\n",
    "\n",
    "        _tokens = encode_prompt(_exp_prompt)\n",
    "        _values_scaled = encode_value(_exp_prompt, _exp_value)\n",
    "        _ages_in_days = encode_age(_exp_age)\n",
    "\n",
    "        (surv, val_dist), _, _ = model(_tokens,\n",
    "                                       values=_values_scaled,\n",
    "                                       ages=_ages_in_days,\n",
    "                                       covariates=covariates,\n",
    "                                       is_generation=True)\n",
    "        _exp_survs.append(surv)\n",
    "\n",
    "    for si, _ in enumerate(surv):\n",
    "        plt.close()\n",
    "        event_name = dm.decode([si + 1])\n",
    "        if event_name in events_of_interest:\n",
    "            for p_idx in range(len(_exp_values)):\n",
    "                plt.plot(model.surv_layer.t_eval / 365, _exp_survs[p_idx][si][0, :], label=f\"{_exp_values[p_idx][0]:.2f}\")\n",
    "            plt.xlabel(\"t (years)\")\n",
    "            plt.ylabel(\"P(T>t) ()\")\n",
    "            plt.legend()\n",
    "            plt.savefig(f\"figs/single_risk/diastolic_blood_pressure/{event_name}.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Values: How varying diagnosis affects value of DBP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEPRESSION                    leads to            standardised Diastolic_blood_pressure_5 ~ N(-0.1, 0.2)\n",
      "TYPE2DIABETES                 leads to            standardised Diastolic_blood_pressure_5 ~ N(-0.1, 0.2)\n",
      "HF_V3                         leads to            standardised Diastolic_blood_pressure_5 ~ N(-0.1, 0.2)\n",
      "HYPERTENSION                  leads to            standardised Diastolic_blood_pressure_5 ~ N(-0.1, 0.2)\n"
     ]
    }
   ],
   "source": [
    "measurements_of_interest = \"Diastolic_blood_pressure_5\"\n",
    "\n",
    "\n",
    "_exp_prompts = [[\"DEPRESSION\"], [\"TYPE2DIABETES\"], [\"HF_V3\"], [\"HYPERTENSION\"]]\n",
    "_exp_age = [20]\n",
    "_exp_value = [np.nan]\n",
    "\n",
    "\n",
    "with torch.no_grad(): \n",
    "    model.eval()\n",
    "\n",
    "    for p_idx, _exp_prompt in enumerate(_exp_prompts):\n",
    "\n",
    "        _tokens = encode_prompt(_exp_prompt)\n",
    "        _values_scaled = encode_value(_exp_prompt, _exp_value)\n",
    "        _ages_in_days = encode_age(_exp_age)\n",
    "        \n",
    "        (surv, val_dist), _, _ = model(_tokens,\n",
    "                                       values=_values_scaled,\n",
    "                                       ages=_ages_in_days,\n",
    "                                       covariates=covariates,\n",
    "                                       is_generation=True)\n",
    "\n",
    "        dist = val_dist[model.value_layer.token_key(dm.tokenizer._stoi[measurements_of_interest])]\n",
    "        print(f\"{'->'.join(_exp_prompt)}\".ljust(30) + \"leads to\".ljust(20) + f\"standardised {measurements_of_interest} ~ N({dist.loc.item():.1f}, {dist.scale.item():.1f})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Values: How increasing bmi affects value of diastolic_blood_pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Body_mass_index_3 of 12.0     leads to            standardised Diastolic_blood_pressure_5 ~ N(-0.2, 0.2)\n",
      "Body_mass_index_3 of 15.0     leads to            standardised Diastolic_blood_pressure_5 ~ N(-0.2, 0.2)\n",
      "Body_mass_index_3 of 18.0     leads to            standardised Diastolic_blood_pressure_5 ~ N(-0.2, 0.2)\n",
      "Body_mass_index_3 of 21.0     leads to            standardised Diastolic_blood_pressure_5 ~ N(-0.1, 0.2)\n",
      "Body_mass_index_3 of 24.0     leads to            standardised Diastolic_blood_pressure_5 ~ N(-0.1, 0.2)\n",
      "Body_mass_index_3 of 30.0     leads to            standardised Diastolic_blood_pressure_5 ~ N(-0.1, 0.2)\n",
      "Body_mass_index_3 of 40.0     leads to            standardised Diastolic_blood_pressure_5 ~ N(-0.0, 0.2)\n"
     ]
    }
   ],
   "source": [
    "measurements_of_interest = \"Diastolic_blood_pressure_5\"\n",
    "\n",
    "\n",
    "_exp_prompt = [\"Body_mass_index_3\"]\n",
    "_exp_values = [[12.], [15.], [18.], [21.], [24.], [30.], [40.]]\n",
    "_exp_value = [np.nan]\n",
    "\n",
    "\n",
    "with torch.no_grad(): \n",
    "    model.eval()\n",
    "\n",
    "    for p_idx, _exp_value in enumerate(_exp_values):\n",
    "\n",
    "        _tokens = encode_prompt(_exp_prompt)\n",
    "        _values_scaled = encode_value(_exp_prompt, _exp_value)\n",
    "        _ages_in_days = encode_age(_exp_age)\n",
    "        \n",
    "        (surv, val_dist), _, _ = model(_tokens,\n",
    "                                       values=_values_scaled,\n",
    "                                       ages=_ages_in_days,\n",
    "                                       covariates=covariates,\n",
    "                                       is_generation=True)\n",
    "\n",
    "        dist = val_dist[model.value_layer.token_key(dm.tokenizer._stoi[measurements_of_interest])]\n",
    "        print(f\"{'->'.join(_exp_prompt)} of {_exp_value[0]}\".ljust(30) + \"leads to\".ljust(20) + f\"standardised {measurements_of_interest} ~ N({dist.loc.item():.1f}, {dist.scale.item():.1f})\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline, impact of gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "events_of_interest = [\"Body_mass_index_3\", \"Diastolic_blood_pressure_5\", \n",
    "                      \"TYPE1DM\", \"TYPE2DIABETES\",\n",
    "                      \"HYPERTENSION\", \"OSTEOARTHRITIS\",\n",
    "                      \"CKDSTAGE3TO5\",\n",
    "                      \"HF_V3\", \"ISCHAEMICSTROKE_V2\"\n",
    "                     ]\n",
    "\n",
    "_genders = [\"M\", \"F\", \"I\"]\n",
    "_exp_prompt = [\"Diastolic_blood_pressure_5\"]\n",
    "_exp_age = [20]\n",
    "_exp_value = [90.]\n",
    "\n",
    "with torch.no_grad(): \n",
    "    model.eval()\n",
    "\n",
    "    _exp_survs = []\n",
    "    for p_idx, _gender in enumerate(_genders):\n",
    "\n",
    "        _baseline_covariate = {\"sex\": _gender, \"deprivation\": 4.0, \"ethnicity\": \"WHITE\", \"year_of_birth\": 1997}\n",
    "        _covariates = dm.train_set._encode_covariates(**_baseline_covariate).reshape(1,-1).to(device)\n",
    "        _tokens = encode_prompt(_exp_prompt)\n",
    "        _values_scaled = encode_value(_exp_prompt, _exp_value)\n",
    "        _ages_in_days = encode_age(_exp_age)\n",
    "\n",
    "        (surv, val_dist), _, _ = model(_tokens,\n",
    "                                       values=_values_scaled,\n",
    "                                       ages=_ages_in_days,\n",
    "                                       covariates=_covariates,\n",
    "                                       is_generation=True)\n",
    "        _exp_survs.append(surv)\n",
    "\n",
    "    for si, _ in enumerate(surv):\n",
    "        plt.close()\n",
    "        event_name = dm.decode([si + 1])\n",
    "        if event_name in events_of_interest:\n",
    "            for p_idx in range(len(_genders)):\n",
    "                plt.plot(model.surv_layer.t_eval / 365, _exp_survs[p_idx][si][0, :], label=f\"{_genders[p_idx]}\")\n",
    "            plt.xlabel(\"t (years)\")\n",
    "            plt.ylabel(\"P(T>t) ()\")\n",
    "            plt.legend()\n",
    "            plt.savefig(f\"figs/single_risk/gender/{event_name}.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: model architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SurvStreamGPTForCausalModelling(\n",
       "  (transformer): TTETransformer(\n",
       "    (wpe): TemporalPositionalEncoding()\n",
       "    (wte): DataEmbeddingLayer(\n",
       "      (static_proj): Linear(in_features=16, out_features=384, bias=True)\n",
       "      (dynamic_embedding_layer): SplitDynamicEmbeddingLayer(\n",
       "        (cat_event_embed_layer): Embedding(184, 384, padding_idx=0)\n",
       "        (cat_event_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (num_value_embed_layer): EmbeddingBag(184, 384, mode=sum, padding_idx=0)\n",
       "        (num_value_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (blocks): ModuleList(\n",
       "      (0): Block(\n",
       "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiHeadedSelfAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (acti): ReLU()\n",
       "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiHeadedSelfAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (acti): ReLU()\n",
       "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiHeadedSelfAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (acti): ReLU()\n",
       "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiHeadedSelfAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (acti): ReLU()\n",
       "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiHeadedSelfAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (acti): ReLU()\n",
       "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiHeadedSelfAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (acti): ReLU()\n",
       "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (surv_layer): ODESurvSingleLayer()\n",
       "  (value_layer): GaussianRegressionLayer(\n",
       "    (regression_layers): ModuleDict(\n",
       "      (Token 15): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 17): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 24): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 26): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 41): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 46): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 49): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 50): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 52): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 53): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 57): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 59): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 61): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 62): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 64): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 67): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 68): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 71): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 74): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 78): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 79): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 80): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 86): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 88): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 89): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 91): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 92): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 93): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 95): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 97): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 98): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 99): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 100): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 101): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 102): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 103): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 105): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 106): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 107): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 109): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 111): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 113): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 114): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 116): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 119): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 121): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 122): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 123): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 124): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 125): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 126): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 127): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 128): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 129): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 130): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 131): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 132): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 133): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 134): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 135): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 136): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 137): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 138): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 139): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 140): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 141): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 142): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 143): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 144): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 145): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 146): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 147): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 148): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 149): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 150): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 151): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 152): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 153): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 154): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 155): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 156): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 157): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 158): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 159): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 160): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 161): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 162): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 163): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 164): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 165): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 166): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 167): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 168): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 169): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 170): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 171): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 172): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 173): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 174): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 175): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 176): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 177): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 178): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 179): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 180): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 181): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 182): Linear(in_features=384, out_features=2, bias=True)\n",
       "      (Token 183): Linear(in_features=384, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook single_risk.ipynb to html\n",
      "[NbConvertApp] Writing 608669 bytes to single_risk.html\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to html --no-input single_risk.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "sys_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
