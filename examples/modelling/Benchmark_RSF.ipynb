{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4921342e-8ff8-45d4-b1f6-9fe480c2ee1d",
   "metadata": {},
   "source": [
    "# CPRD Notebook:\n",
    "## Evaluation of fine-tuning the pre-trained SurvivEHR-CR model on a supervised cohort study.\n",
    "\n",
    "Cohort study: predicting Cardiovascular Disease in a Type 2 Diabetes Mellitus population.\n",
    "\n",
    "This notebook quantifies the performance obtained when fine-tuning the pre-trained model to a sub-population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a671c59b-4428-4e63-a138-7244418a87c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added path '/rds/homes/g/gaddcz/Projects/CPRD/virtual-envTorch2.0-icelake/lib/python3.10/site-packages' at start of search paths.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "node_type = os.getenv('BB_CPU')\n",
    "venv_dir = f'/rds/homes/g/gaddcz/Projects/CPRD/virtual-envTorch2.0-{node_type}'\n",
    "venv_site_pkgs = Path(venv_dir) / 'lib' / f'python{sys.version_info.major}.{sys.version_info.minor}' / 'site-packages'\n",
    "if venv_site_pkgs.exists():\n",
    "    sys.path.insert(0, str(venv_site_pkgs))\n",
    "    print(f\"Added path '{venv_site_pkgs}' at start of search paths.\")\n",
    "else:\n",
    "    print(f\"Path '{venv_site_pkgs}' not found. Check that it exists and/or that it exists for node-type '{node_type}'.\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d866c7f0-eaa8-4129-b3b1-d4d6b504d0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda.\n",
      "env: SLURM_NTASKS_PER_NODE=28\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "from CPRD.examples.modelling.SurvStreamGPT.run_experiment import run\n",
    "from CPRD.data.foundational_loader import FoundationalDataModule\n",
    "# from CPRD.src.models.survival.task_heads.causal import SurvStreamGPTForCausalModelling\n",
    "\n",
    "from sklearn import set_config\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "from sksurv.datasets import load_gbsg2\n",
    "from sksurv.preprocessing import OneHotEncoder\n",
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "\n",
    "import time\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "import polars as pl\n",
    "pl.Config.set_tbl_rows(10000)\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 10000\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = \"cpu\"    # if more informative debugging statements are needed\n",
    "print(f\"Using device: {device}.\")\n",
    "\n",
    " # TODO: define an env variable to fix for a local hpc environment issue, this shouldn't be needed\n",
    "%env SLURM_NTASKS_PER_NODE=28   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63e00688-1426-4046-910c-45bde94db899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_decoder: true\n",
      "data:\n",
      "  batch_size: 64\n",
      "  unk_freq_threshold: 0.0\n",
      "  min_workers: 12\n",
      "  global_diagnoses: false\n",
      "  path_to_db: /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/\n",
      "  path_to_ds: /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/\n",
      "  meta_information_path: /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/meta_information_QuantJenny.pickle\n",
      "experiment:\n",
      "  type: clinical prediction model\n",
      "  project_name: SurvEHR_${head.SurvLayer}\n",
      "  run_id: CR_11M_new\n",
      "  train: true\n",
      "  test: true\n",
      "  verbose: true\n",
      "  seed: 1337\n",
      "  log: true\n",
      "  log_dir: /rds/projects/s/subramaa-mum-predict/CharlesGadd_Oxford/FoundationModelOutput/\n",
      "  ckpt_dir: /rds/projects/s/subramaa-mum-predict/CharlesGadd_Oxford/FoundationModelOutput/checkpoints/\n",
      "  fine_tune_outcomes:\n",
      "  - IHDINCLUDINGMI_OPTIMALV2\n",
      "  - ISCHAEMICSTROKE_V2\n",
      "  - MINFARCTION\n",
      "  - STROKEUNSPECIFIED_V2\n",
      "  - STROKE_HAEMRGIC\n",
      "optim:\n",
      "  num_epochs: 10\n",
      "  learning_rate: 0.0003\n",
      "  scheduler: ReduceOnPlateau\n",
      "  scheduler_periods: 5000\n",
      "  scheduler_warmup: false\n",
      "  lr_cosine_decay_period: 10000000.0\n",
      "  val_check_interval: 50\n",
      "  early_stop: true\n",
      "  early_stop_patience: 5\n",
      "  log_every_n_steps: 20\n",
      "  limit_val_batches: 0.05\n",
      "  limit_test_batches: null\n",
      "transformer:\n",
      "  block_type: Neo\n",
      "  block_size: 128\n",
      "  n_layer: 6\n",
      "  n_head: 6\n",
      "  n_embd: 384\n",
      "  layer_norm_bias: false\n",
      "  attention_type: global\n",
      "  bias: true\n",
      "  dropout: 0.0\n",
      "  attention_dropout: 0.0\n",
      "  resid_dropout: 0.0\n",
      "head:\n",
      "  SurvLayer: cr\n",
      "  surv_weight: 1\n",
      "  tokens_for_univariate_regression: None\n",
      "  value_weight: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load the configuration file, override any settings \n",
    "with initialize(version_base=None, config_path=\"../../confs\", job_name=\"testing_notebook\"):\n",
    "    cfg = compose(config_name=\"config_CompetingRisk11M\", \n",
    "                  overrides=[# Experiment setup\n",
    "                             \"experiment.type='clinical prediction model'\",\n",
    "                             \"experiment.run_id='CR_11M_new'\",\n",
    "                             \"experiment.train=True\",\n",
    "                             \"experiment.test=True\",\n",
    "                             'experiment.fine_tune_outcomes=[\"IHDINCLUDINGMI_OPTIMALV2\", \"ISCHAEMICSTROKE_V2\", \"MINFARCTION\", \"STROKEUNSPECIFIED_V2\", \"STROKE_HAEMRGIC\"]',\n",
    "                             # Dataloader\n",
    "                             \"data.path_to_ds=/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/\",\n",
    "                             # \"data.batch_size=512\",\n",
    "                             \"data.min_workers=12\",\n",
    "                             # Optimiser\n",
    "                             \"optim.num_epochs=10\",\n",
    "                             \"optim.limit_test_batches=null\",\n",
    "                             \"optim.scheduler=ReduceOnPlateau\",\n",
    "                             \"optim.scheduler_warmup=False\",\n",
    "                             \"optim.val_check_interval=50\",\n",
    "                             \"optim.early_stop=True\",\n",
    "                             # Head\n",
    "                             \"head.surv_weight=1\",\n",
    "                             \"head.value_weight=0\",\n",
    "                            ]\n",
    "                 )     \n",
    "\n",
    "print(OmegaConf.to_yaml(cfg))\n",
    "\n",
    "save_path = f\"/rds/projects/s/subramaa-mum-predict/CharlesGadd_Oxford/FoundationModelOutput/checkpoints/{cfg.experiment.run_id}/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e606ced-beae-4b0f-87f7-462adfc4e671",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:====================================================================================================\n",
      "INFO:root:# Loading DataModule for dataset /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/. This will be loaded in supervised form.\n",
      "INFO:root:====================================================================================================\n",
      "INFO:root:Creating supervised collator for DataModule\n",
      "INFO:root:Using meta information from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/PreTrain/meta_information_QuantJenny.pickle\n",
      "INFO:root:Using train file-row count dictionary from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/file_row_count_dict_train.pickle\n",
      "INFO:root:Using test file-row count dictionary from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/file_row_count_dict_test.pickle\n",
      "INFO:root:Using val file-row count dictionary from /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/file_row_count_dict_val.pickle\n",
      "INFO:root:Tokenzier created based on 7,555,415,275 tokens\n",
      "INFO:root:Using tabular tokenizer, created from meta information and containing 265 tokens\n",
      "INFO:root:Loaded /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/split=train/ dataset, with 572,096 samples\n",
      "INFO:root:Loaded /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/split=test/ dataset, with 35,758 samples\n",
      "INFO:root:Loaded /rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/split=val/ dataset, with 33,280 samples\n"
     ]
    }
   ],
   "source": [
    "supervised = True if cfg.experiment.fine_tune_outcomes is not None else False\n",
    "logging.info(\"=\"*100)\n",
    "logging.info(f\"# Loading DataModule for dataset {cfg.data.path_to_ds}. This will be loaded in {'supervised' if supervised else 'causal'} form.\")\n",
    "logging.info(\"=\"*100)\n",
    "dm = FoundationalDataModule(path_to_db=cfg.data.path_to_db,\n",
    "                            path_to_ds=cfg.data.path_to_ds,\n",
    "                            load=True,\n",
    "                            tokenizer=\"tabular\",\n",
    "                            batch_size=cfg.data.batch_size,\n",
    "                            max_seq_length=cfg.transformer.block_size,\n",
    "                            global_diagnoses=cfg.data.global_diagnoses,\n",
    "                            freq_threshold=cfg.data.unk_freq_threshold,\n",
    "                            min_workers=cfg.data.min_workers,\n",
    "                            overwrite_meta_information=cfg.data.meta_information_path,\n",
    "                            supervised=supervised\n",
    "                           )\n",
    "# Get required information from initialised dataloader\n",
    "# ... vocab size\n",
    "vocab_size = dm.train_set.tokenizer.vocab_size\n",
    "# ... Extract the measurements, using the fact that the diagnoses are all up upper case. This is needed for automatically setting the configuration below\n",
    "#     encode into the list of univariate measurements to model with Normal distribution\n",
    "# measurements_for_univariate_regression = [record for record in dm.tokenizer._event_counts[\"EVENT\"] if record.upper() != record]\n",
    "# cfg.head.tokens_for_univariate_regression = dm.encode(measurements_for_univariate_regression) #\n",
    "measurements_for_univariate_regression = dm.train_set.meta_information[\"measurement_tables\"][dm.train_set.meta_information[\"measurement_tables\"][\"count_obs\"] > 0][\"event\"].to_list()\n",
    "cfg.head.tokens_for_univariate_regression = dm.encode(measurements_for_univariate_regression)\n",
    "logging.debug(OmegaConf.to_yaml(cfg))\n",
    "\n",
    "target_tokens = dm.encode(['IHDINCLUDINGMI_OPTIMALV2', 'ISCHAEMICSTROKE_V2', 'MINFARCTION', 'STROKEUNSPECIFIED_V2', 'STROKE_HAEMRGIC'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2e0a69-cc78-43d6-8e6c-bac2194a3f1b",
   "metadata": {},
   "source": [
    "# Run experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bacadcc-f857-45e2-94e8-0ca43ccbd196",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# X, y = load_gbsg2()\n",
    "# grade_str = X.loc[:, \"tgrade\"].astype(object).values[:, np.newaxis]\n",
    "# grade_num = OrdinalEncoder(categories=[[\"I\", \"II\", \"III\"]]).fit_transform(grade_str)\n",
    "\n",
    "# X_no_grade = X.drop(\"tgrade\", axis=1)\n",
    "# Xt = OneHotEncoder().fit_transform(X_no_grade)\n",
    "# Xt.loc[:, \"tgrade\"] = grade_num\n",
    "# print(X.head())\n",
    "# print(Xt.head())\n",
    "\n",
    "# print(type(y))\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f61fa0f-4217-40d7-8996-73301eaf736a",
   "metadata": {},
   "source": [
    "# Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfbfbc95-0553-4f07-aca4-c25ef905acf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_xsectional_dataset(dataset, n=None):\n",
    "\n",
    "    X = pd.DataFrame(columns=[f'static_{_idx}' for _idx in range(16)] + [f'{dataset.tokenizer._itos[_idx]}' for _idx in range(2,vocab_size)])\n",
    "    Y = []\n",
    "    \n",
    "    for s_idx, sample in tqdm(enumerate(dataset), total=n):\n",
    "    \n",
    "        # Input\n",
    "        ########\n",
    "        # Static variables are already processed into categories where required\n",
    "        static = sample[\"static_covariates\"]\n",
    "    \n",
    "        # Get a binary vector of vocab_size elements, which indicate if patient has any history of a condition (at any time, as long as it fits study criteria)\n",
    "        # Note, 0 and 1 are PAD and UNK tokens which arent required\n",
    "        input_tokens = sample[\"tokens\"][:-1]\n",
    "        token_binary = torch.zeros(vocab_size-2)\n",
    "        for tkn_idx in range(2, vocab_size):\n",
    "            if tkn_idx in input_tokens:\n",
    "                token_binary[tkn_idx-2] = 1\n",
    "    \n",
    "        sample_input = torch.hstack((static, token_binary)).tolist()\n",
    "        X.loc[s_idx] = sample_input\n",
    "    \n",
    "        # Target\n",
    "        ########\n",
    "        target = sample[\"tokens\"][-1]\n",
    "        if target in target_tokens:\n",
    "            target = True\n",
    "        else:\n",
    "            target = False\n",
    "        delta_age = sample[\"ages\"][-1] - sample[\"ages\"][-2]\n",
    "        Y.append((target, int(delta_age)))\n",
    "    \n",
    "        if n is not None and s_idx >= n:\n",
    "            break\n",
    "\n",
    "    y = np.array(Y, dtype=[('Status', '?'), ('Survival_in_days', '<f8')])\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def make_xsectional_dataset2(datamodule, target_tokens, split='train', n=None):\n",
    "\n",
    "    X = pd.DataFrame(columns=[f'static_{_idx}' for _idx in range(16)] + [f'{datamodule.train_set.tokenizer._itos[_idx]}' for _idx in range(2,vocab_size)])\n",
    "    Y = []\n",
    "\n",
    "    match split:\n",
    "        case 'train':\n",
    "            dataloader = datamodule.train_dataloader()\n",
    "        case 'val':\n",
    "            dataloader = datamodule.val_dataloader()\n",
    "        case 'test':\n",
    "            dataloader = datamodule.test_dataloader()\n",
    "    \n",
    "    for b_idx, batch in tqdm(enumerate(dataloader), total=n):\n",
    "\n",
    "        # Input\n",
    "        ########\n",
    "        # Static variables are already processed into categories where required\n",
    "        static = batch[\"static_covariates\"].numpy()\n",
    "    \n",
    "        # Get a binary vector of vocab_size elements, which indicate if patient has any history of a condition (at any time, as long as it fits study criteria)\n",
    "        # Note, 0 and 1 are PAD and UNK tokens which arent required\n",
    "        input_tokens = batch[\"tokens\"]\n",
    "        token_binary = np.zeros((static.shape[0], vocab_size-2))\n",
    "        for s_idx in range(static.shape[0]):\n",
    "            for tkn_idx in range(2, vocab_size):\n",
    "                if tkn_idx in input_tokens[s_idx, :]:\n",
    "                    token_binary[s_idx, tkn_idx-2] = 1\n",
    "    \n",
    "        batch_input = np.hstack((static, token_binary))\n",
    "        batch_df = pd.DataFrame(batch_input, columns=X.columns)\n",
    "        X = pd.concat([X, batch_df])\n",
    "        \n",
    "        # Target\n",
    "        ########\n",
    "        targets = batch[\"target_token\"].numpy()\n",
    "        for s_idx in range(static.shape[0]):\n",
    "            if targets[s_idx] in target_tokens:\n",
    "                target = True\n",
    "            else:\n",
    "                target = False\n",
    "            \n",
    "            Y.append((target, int(batch[\"target_age_delta\"][s_idx] )))\n",
    "    \n",
    "        if n is not None and b_idx >= n:\n",
    "            break\n",
    "\n",
    "    y = np.array(Y, dtype=[('Status', '?'), ('Survival_in_days', '<f8')])\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "589fa98a-4455-42ec-8871-ef22fef8adc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for batch in dm.train_dataloader():\n",
    "#     break\n",
    "# print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "032cb2f5-10f5-4b90-a8df-d4ea7d05a513",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# X_train, y_train = make_static_dataset(dm.train_set, n=1e6)\n",
    "# X_test, y_test = make_static_dataset(dm.test_set, n=None)\n",
    "# # print(Y)\n",
    "# # print(X.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a4b2ba2-22c3-48fa-8df3-ffd201700636",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8939/8939 [3:30:51<00:00,  1.42s/it]  \n",
      "100%|██████████| 559/559 [03:17<00:00,  2.82it/s]\n"
     ]
    }
   ],
   "source": [
    "n_train =  len(dm.train_dataloader()) \n",
    "X_train, y_train = make_xsectional_dataset2(dm, target_tokens, split='train', n=n_train)\n",
    "\n",
    "n_test = len(dm.test_dataloader())  \n",
    "X_test, y_test = make_xsectional_dataset2(dm, target_tokens, split='test', n=n_test)\n",
    "# print(Y)\n",
    "# print(X.head())\n",
    "\n",
    "import pickle \n",
    "\n",
    "data = {\"X_train\": X_train,\n",
    "        \"y_train\": y_train,\n",
    "        \"X_test\": X_test,\n",
    "        \"y_test\": y_test}\n",
    "with open('/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/xsectional_data.pickle', 'wb') as handle:\n",
    "    pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c07623f-a10e-4a75-8086-d237766b40a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m \n\u001b[0;32m----> 3\u001b[0m data \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_train\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mX_train\u001b[49m,\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_train\u001b[39m\u001b[38;5;124m\"\u001b[39m: y_train,\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_test\u001b[39m\u001b[38;5;124m\"\u001b[39m: X_test,\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_test\u001b[39m\u001b[38;5;124m\"\u001b[39m: y_test}\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/rds/projects/g/gokhalkm-optimal/OPTIMAL_MASTER_DATASET/data/FoundationalModel/FineTune_CVD/xsectional_data.pickle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[1;32m      8\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(data, handle, protocol\u001b[38;5;241m=\u001b[39mpickle\u001b[38;5;241m.\u001b[39mHIGHEST_PROTOCOL)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "print(X_train.shape)\n",
    "print(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee5f222d-0065-4794-a98a-2cc6807a37cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[95, 41, 67, 65, 28]\n"
     ]
    }
   ],
   "source": [
    "# dm.train_set.tokenizer._stoi\n",
    "print(target_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76fe9707-3774-4472-b12a-b1aa06507126",
   "metadata": {},
   "outputs": [],
   "source": [
    "rsf = RandomSurvivalForest(\n",
    "    n_estimators=1000, min_samples_split=50, min_samples_leaf=15, n_jobs=-1, random_state=1337,\n",
    "    bootstrap=True, max_samples=1000, low_memory=False\n",
    ")\n",
    "est = rsf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00ee6b2-59b6-4484-a6be-7cc8550755e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rsf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d35471-825a-4f1e-ad27-45711ec1572d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.metrics import integrated_brier_score\n",
    "\n",
    "survs = est.predict_survival_function(X_test)\n",
    "times = np.arange(1, 365*5)[::20]\n",
    "preds = np.asarray([[fn(t) for t in times] for fn in survs])\n",
    "score = integrated_brier_score(y_train, y_test, preds, times)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51a0cad-6fed-49d4-b550-cf46589e953a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "sys_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
