{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJpXpmjEYC_T"
   },
   "source": [
    "# Demo Notebook:\n",
    "## Transformer For Causal Language Modelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "/rds/homes/g/gaddcz/Projects/CPRD/examples/modelling/NLP\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sqlite3\n",
    "from dataclasses import dataclass\n",
    "import logging\n",
    "from CPRD.data.foundational_loader import FoundationalDataModule\n",
    "from CPRD.src.models.benchmarks.karpathy_gpt.transformer import GPTLanguageModel\n",
    "from CPRD.src.models.transformer.task_heads.causal_lm import TransformerForCausalLM\n",
    "\n",
    "# TODO:\n",
    "# replace experiment boilerplate with pytorch lightning\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "# device = \"cpu\"    # just for if i need more informative debugging statements\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path '/rds/homes/g/gaddcz/Projects/CPRD/my-virtual-env-icelake/lib/python3.10/site-packages' not found. Check that it exists and/or that it exists for node-type 'icelake'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "node_type = os.getenv('BB_CPU')\n",
    "venv_dir = f'/rds/homes/g/gaddcz/Projects/CPRD/my-virtual-env-{node_type}'\n",
    "venv_site_pkgs = Path(venv_dir) / 'lib' / f'python{sys.version_info.major}.{sys.version_info.minor}' / 'site-packages'\n",
    "if venv_site_pkgs.exists():\n",
    "    sys.path.insert(0, str(venv_site_pkgs))\n",
    "    print(f\"Loaded environment from {venv_site_pkgs} for node-type '{node_type}.\")\n",
    "else:\n",
    "    print(f\"Path '{venv_site_pkgs}' not found. Check that it exists and/or that it exists for node-type '{node_type}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set config to be equivalent architecture of kaparthy benchmark,\n",
    "#      note: there will be fewer paramaters due to weight tying in the causal language modelling head\n",
    "@dataclass\n",
    "class DemoConfig:\n",
    "    learn_positional_embedding: bool = True\n",
    "    block_size: int = 256        # what is the maximum context length for predictions?\n",
    "    n_layer: int = 6\n",
    "    n_head: int = 6\n",
    "    n_embd: int = 384\n",
    "    bias: bool = True\n",
    "    attention_type: str = \"global\"    \n",
    "    dropout: float = 0.0\n",
    "    unk_freq_threshold: float = 0.0\n",
    "\n",
    "config = DemoConfig()\n",
    "\n",
    "@dataclass\n",
    "class OptConfig:\n",
    "    batch_size: int = 64\n",
    "    eval_interval: int = 1\n",
    "    learning_rate: float = 3e-4\n",
    "    epochs: int = 20\n",
    "    \n",
    "opt = OptConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data loader on a reduced cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Building polars dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using N=10000 random samples, from the available 117102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using measurements\n",
      "INFO:root:Using diagnoses\n",
      "INFO:root:Dropping samples with no dynamic events\n",
      "INFO:root:Using non-tabular tokenizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8619 training, 479 validation, and 479 test samples\n",
      "101 vocab elements\n",
      "{0: 'PAD', 1: 'UNK', 2: '0', 3: '1', 4: '2', 5: '3', 6: '4', 7: '5', 8: '6', 9: '7', 10: '8', 11: '9', 12: '.', 13: 'diastolic_blood_pressure', 14: 'bmi', 15: 'eosinophil_count', 16: 'basophil_count', 17: 'corrected_serum_calcium_level', 18: 'DEPRESSION', 19: 'serum_level', 20: 'calculated_LDL_cholesterol_level', 21: 'ANXIETY', 22: 'HYPERTENSION', 23: 'TYPE2DIABETES', 24: 'ASTHMA_PUSHASTHMA', 25: 'OSTEOARTHRITIS', 26: 'ATOPICECZEMA', 27: 'ALLERGICRHINITISCONJ', 28: 'ANY_DEAFNESS_HEARING_LOSS', 29: 'aspartate_transam', 30: 'ALLCA_NOBCC_VFINAL', 31: 'PREVALENT_IBS', 32: 'IHD_NOMI', 33: 'ALCOHOLMISUSE', 34: 'CKDSTAGE3TO5', 35: 'blood_urea', 36: 'calcium_adjusted_level', 37: 'HYPOTHYROIDISM_DRAFT_V1', 38: 'PERIPHERAL_NEUROPATHY', 39: 'COPD', 40: 'AF', 41: 'combined_total_vitamin_D2_and_D3_level', 42: 'OSTEOPOROSIS', 43: 'PSORIASIS', 44: 'HF', 45: 'MINFARCTION', 46: 'GOUT', 47: 'SUBSTANCEMISUSE', 48: 'STROKEUNSPECIFIED', 49: 'ALL_DEMENTIA', 50: 'hydroxyvitamin3', 51: 'hydroxyvitamin2', 52: 'OTHER_CHRONIC_LIVER_DISEASE_OPTIMAL', 53: 'PAD_STRICT', 54: 'VALVULARDISEASES', 55: 'TYPE1DM', 56: 'EPILEPSY', 57: 'FIBROMYALGIA', 58: 'OSA', 59: 'RHEUMATOIDARTHRITIS', 60: 'EATINGDISORDERS', 61: 'POLYCYSTIC_OVARIAN_SYNDROME_PCOS', 62: 'HYPERTHYROIDISM', 63: 'PMRANDGCA', 64: 'NAFLD', 65: 'PTSDDIAGNOSIS', 66: 'ENDOMETRIOSIS_ADENOMYOSIS_V2', 67: 'VISUAL_IMPAIRMENT', 68: 'ISCHAEMICSTROKE', 69: 'SCHIZOPHRENIAMM', 70: 'BIPOLAR', 71: 'creatinine_ratio', 72: 'BRONCHIECTASIS', 73: 'CHRONIC_LIVER_DISEASE_ALCOHOL', 74: 'CHRONICFATIGUESYNDROMEMM', 75: 'ULCERATIVE_COLITIS', 76: 'AORTICANEURYSM', 77: 'STROKE_HAEMRGIC', 78: 'LEARNINGDISABILITY', 79: 'MENIERESDISEASE', 80: 'LYMPHOMA_PREVALENCE', 81: 'CROHNS_DISEASE', 82: 'PERNICIOUSANAEMIA', 83: 'PARKINSONS', 84: 'ILD_SH', 85: 'brain_natriuretic_peptide_level', 86: 'MS', 87: 'AUTISM', 88: 'PSORIATICARTHRITIS2021', 89: 'LEUKAEMIA_PREVALENCE', 90: 'HIVAIDS', 91: 'SYSTEMIC_LUPUS_ERYTHEMATOSUS', 92: 'SJOGRENSSYNDROME', 93: 'PLASMACELL_NEOPLASM', 94: 'blood_calcium', 95: 'HAEMOCHROMATOSIS', 96: 'SYSTEMIC_SCLEROSIS', 97: 'ADDISON_DISEASE', 98: 'DOWNSSYNDROME', 99: 'CYSTICFIBROSIS', 100: 'SICKLE_CELL_DISEASE'}\n"
     ]
    }
   ],
   "source": [
    "from CPRD.data.database import queries\n",
    "\n",
    "# Get a list of patients which fit a reduced set of criterion\n",
    "PATH_TO_DB = \"/rds/projects/s/subramaa-mum-predict/CharlesGadd_Oxford/FoundationModel/preprocessing/processed/cprd.db\"\n",
    "conn = sqlite3.connect(PATH_TO_DB)\n",
    "cursor = conn.cursor()\n",
    "identifiers1 = queries.query_measurement([\"bmi\", \"diastolic_blood_pressure\"], cursor)        \n",
    "identifiers2 = queries.query_diagnosis([\"DEPRESSION\", \"TYPE1DM\", \"TYPE2DIABETES\"], cursor)    #  \"DEPRESSION\"  ,  \"ANXIETY\"\n",
    "all_identifiers = list(set(identifiers1).intersection(identifiers2))    # Turn smaller list into the set\n",
    "\n",
    "# Lets take only the first N for faster run-time\n",
    "N = np.min((len(all_identifiers), 10000))\n",
    "print(f\"Using N={N} random samples, from the available {len(all_identifiers)}\")\n",
    "identifiers = random.choices(all_identifiers, k=N)\n",
    "\n",
    "# Build \n",
    "dm = FoundationalDataModule(identifiers=identifiers,\n",
    "                            tokenizer=\"non-tabular\",\n",
    "                            batch_size=opt.batch_size,\n",
    "                            max_seq_length=config.block_size,\n",
    "                            unk_freq_threshold=config.unk_freq_threshold,\n",
    "                            include_measurements=True,\n",
    "                            include_diagnoses=True,\n",
    "                            preprocess_measurements=False)\n",
    "vocab_size = dm.train_set.tokenizer.vocab_size\n",
    "\n",
    "print(f\"{len(dm.train_set)} training, {len(dm.val_set)} validation, and {len(dm.test_set)} test samples\")\n",
    "print(f\"{vocab_size} vocab elements\")\n",
    "print(dm.train_set.tokenizer._itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (89, 3)\n",
      "┌───────────────────────────────────┬────────┬──────────┐\n",
      "│ EVENT                             ┆ counts ┆ freq     │\n",
      "│ ---                               ┆ ---    ┆ ---      │\n",
      "│ str                               ┆ u32    ┆ f64      │\n",
      "╞═══════════════════════════════════╪════════╪══════════╡\n",
      "│ UNK                               ┆ 0      ┆ 0.0      │\n",
      "│ diastolic_blood_pressure          ┆ 181291 ┆ 0.423281 │\n",
      "│ bmi                               ┆ 70051  ┆ 0.163556 │\n",
      "│ eosinophil_count                  ┆ 65777  ┆ 0.153577 │\n",
      "│ basophil_count                    ┆ 43672  ┆ 0.101966 │\n",
      "│ corrected_serum_calcium_level     ┆ 12271  ┆ 0.028651 │\n",
      "│ DEPRESSION                        ┆ 6949   ┆ 0.016225 │\n",
      "│ serum_level                       ┆ 6073   ┆ 0.014179 │\n",
      "│ calculated_LDL_cholesterol_level  ┆ 5767   ┆ 0.013465 │\n",
      "│ ANXIETY                           ┆ 3867   ┆ 0.009029 │\n",
      "│ HYPERTENSION                      ┆ 2686   ┆ 0.006271 │\n",
      "│ TYPE2DIABETES                     ┆ 2194   ┆ 0.005123 │\n",
      "│ OSTEOARTHRITIS                    ┆ 2079   ┆ 0.004854 │\n",
      "│ ASTHMA_PUSHASTHMA                 ┆ 2071   ┆ 0.004835 │\n",
      "│ ATOPICECZEMA                      ┆ 1872   ┆ 0.004371 │\n",
      "│ ALLERGICRHINITISCONJ              ┆ 1527   ┆ 0.003565 │\n",
      "│ ANY_DEAFNESS_HEARING_LOSS         ┆ 1395   ┆ 0.003257 │\n",
      "│ aspartate_transam                 ┆ 1237   ┆ 0.002888 │\n",
      "│ PREVALENT_IBS                     ┆ 965    ┆ 0.002253 │\n",
      "│ ALLCA_NOBCC_VFINAL                ┆ 917    ┆ 0.002141 │\n",
      "│ ALCOHOLMISUSE                     ┆ 915    ┆ 0.002136 │\n",
      "│ IHD_NOMI                          ┆ 882    ┆ 0.002059 │\n",
      "│ blood_urea                        ┆ 866    ┆ 0.002022 │\n",
      "│ CKDSTAGE3TO5                      ┆ 813    ┆ 0.001898 │\n",
      "│ calcium_adjusted_level            ┆ 735    ┆ 0.001716 │\n",
      "│ PERIPHERAL_NEUROPATHY             ┆ 728    ┆ 0.0017   │\n",
      "│ HYPOTHYROIDISM_DRAFT_V1           ┆ 645    ┆ 0.001506 │\n",
      "│ COPD                              ┆ 625    ┆ 0.001459 │\n",
      "│ AF                                ┆ 500    ┆ 0.001167 │\n",
      "│ SUBSTANCEMISUSE                   ┆ 470    ┆ 0.001097 │\n",
      "│ PSORIASIS                         ┆ 470    ┆ 0.001097 │\n",
      "│ HF                                ┆ 457    ┆ 0.001067 │\n",
      "│ OSTEOPOROSIS                      ┆ 444    ┆ 0.001037 │\n",
      "│ GOUT                              ┆ 438    ┆ 0.001023 │\n",
      "│ MINFARCTION                       ┆ 412    ┆ 0.000962 │\n",
      "│ combined_total_vitamin_D2_and_D3… ┆ 402    ┆ 0.000939 │\n",
      "│ STROKEUNSPECIFIED                 ┆ 391    ┆ 0.000913 │\n",
      "│ ALL_DEMENTIA                      ┆ 346    ┆ 0.000808 │\n",
      "│ hydroxyvitamin3                   ┆ 318    ┆ 0.000742 │\n",
      "│ hydroxyvitamin2                   ┆ 302    ┆ 0.000705 │\n",
      "│ VALVULARDISEASES                  ┆ 274    ┆ 0.00064  │\n",
      "│ PAD_STRICT                        ┆ 249    ┆ 0.000581 │\n",
      "│ TYPE1DM                           ┆ 246    ┆ 0.000574 │\n",
      "│ OTHER_CHRONIC_LIVER_DISEASE_OPTI… ┆ 213    ┆ 0.000497 │\n",
      "│ EPILEPSY                          ┆ 207    ┆ 0.000483 │\n",
      "│ FIBROMYALGIA                      ┆ 199    ┆ 0.000465 │\n",
      "│ OSA                               ┆ 182    ┆ 0.000425 │\n",
      "│ POLYCYSTIC_OVARIAN_SYNDROME_PCOS  ┆ 178    ┆ 0.000416 │\n",
      "│ NAFLD                             ┆ 168    ┆ 0.000392 │\n",
      "│ HYPERTHYROIDISM                   ┆ 167    ┆ 0.00039  │\n",
      "│ EATINGDISORDERS                   ┆ 163    ┆ 0.000381 │\n",
      "│ ENDOMETRIOSIS_ADENOMYOSIS_V2      ┆ 157    ┆ 0.000367 │\n",
      "│ RHEUMATOIDARTHRITIS               ┆ 153    ┆ 0.000357 │\n",
      "│ PMRANDGCA                         ┆ 142    ┆ 0.000332 │\n",
      "│ PTSDDIAGNOSIS                     ┆ 135    ┆ 0.000315 │\n",
      "│ BIPOLAR                           ┆ 125    ┆ 0.000292 │\n",
      "│ ISCHAEMICSTROKE                   ┆ 123    ┆ 0.000287 │\n",
      "│ creatinine_ratio                  ┆ 106    ┆ 0.000247 │\n",
      "│ SCHIZOPHRENIAMM                   ┆ 100    ┆ 0.000233 │\n",
      "│ VISUAL_IMPAIRMENT                 ┆ 98     ┆ 0.000229 │\n",
      "│ BRONCHIECTASIS                    ┆ 95     ┆ 0.000222 │\n",
      "│ AORTICANEURYSM                    ┆ 82     ┆ 0.000191 │\n",
      "│ CHRONIC_LIVER_DISEASE_ALCOHOL     ┆ 76     ┆ 0.000177 │\n",
      "│ ULCERATIVE_COLITIS                ┆ 72     ┆ 0.000168 │\n",
      "│ CHRONICFATIGUESYNDROMEMM          ┆ 64     ┆ 0.000149 │\n",
      "│ PARKINSONS                        ┆ 63     ┆ 0.000147 │\n",
      "│ STROKE_HAEMRGIC                   ┆ 62     ┆ 0.000145 │\n",
      "│ PERNICIOUSANAEMIA                 ┆ 58     ┆ 0.000135 │\n",
      "│ brain_natriuretic_peptide_level   ┆ 57     ┆ 0.000133 │\n",
      "│ LEARNINGDISABILITY                ┆ 56     ┆ 0.000131 │\n",
      "│ LYMPHOMA_PREVALENCE               ┆ 49     ┆ 0.000114 │\n",
      "│ MENIERESDISEASE                   ┆ 46     ┆ 0.000107 │\n",
      "│ CROHNS_DISEASE                    ┆ 45     ┆ 0.000105 │\n",
      "│ AUTISM                            ┆ 40     ┆ 0.000093 │\n",
      "│ PSORIATICARTHRITIS2021            ┆ 35     ┆ 0.000082 │\n",
      "│ ILD_SH                            ┆ 34     ┆ 0.000079 │\n",
      "│ LEUKAEMIA_PREVALENCE              ┆ 29     ┆ 0.000068 │\n",
      "│ MS                                ┆ 24     ┆ 0.000056 │\n",
      "│ HAEMOCHROMATOSIS                  ┆ 19     ┆ 0.000044 │\n",
      "│ HIVAIDS                           ┆ 17     ┆ 0.00004  │\n",
      "│ SJOGRENSSYNDROME                  ┆ 14     ┆ 0.000033 │\n",
      "│ SYSTEMIC_LUPUS_ERYTHEMATOSUS      ┆ 14     ┆ 0.000033 │\n",
      "│ SYSTEMIC_SCLEROSIS                ┆ 9      ┆ 0.000021 │\n",
      "│ PLASMACELL_NEOPLASM               ┆ 8      ┆ 0.000019 │\n",
      "│ blood_calcium                     ┆ 7      ┆ 0.000016 │\n",
      "│ CYSTICFIBROSIS                    ┆ 7      ┆ 0.000016 │\n",
      "│ ADDISON_DISEASE                   ┆ 6      ┆ 0.000014 │\n",
      "│ DOWNSSYNDROME                     ┆ 5      ┆ 0.000012 │\n",
      "│ SICKLE_CELL_DISEASE               ┆ 1      ┆ 0.000002 │\n",
      "└───────────────────────────────────┴────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "pl.Config.set_tbl_rows(vocab_size + 1)\n",
    "print(dm.tokenizer._event_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create models and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using Positional Embedding. This module uses the index position of an event within the block of events.\n",
      "INFO:root:Using Positional Encoding. This module uses the index position of an event within the block of events.\n"
     ]
    }
   ],
   "source": [
    "models, m_names = [], []\n",
    "\n",
    "# Baseline model to test my changes against\n",
    "# models.append(GPTLanguageModel(config, vocab_size).to(device))\n",
    "# m_names.append(\"kaparthy benchmark\")\n",
    "\n",
    "# My development model\n",
    "for pe in [True, False]:\n",
    "    config = DemoConfig()\n",
    "    config.learn_positional_embedding = pe\n",
    "    models.append(TransformerForCausalLM(config, vocab_size).to(device))\n",
    "    m_names.append(f\"{'pos_embedding' if pe else 'pos_encoding'}\")\n",
    "\n",
    "loss_curves_train = [[] for _ in models]\n",
    "loss_curves_val = [[] for _ in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hoelkOrFY8bN",
    "outputId": "961304cd-e379-40d4-dd56-8de0b91d2861"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model `pos_embedding`, with 10.777728 M parameters\n",
      "\n",
      "Epoch 0:\tTrain loss 1.88. Val loss 1.57\n",
      "Epoch 1:\tTrain loss 1.23. Val loss 1.30\n",
      "Epoch 2:\tTrain loss 1.10. Val loss 1.20\n",
      "Epoch 3:\tTrain loss 1.04. Val loss 1.17\n",
      "Epoch 4:\tTrain loss 1.03. Val loss 1.16\n",
      "Epoch 5:\tTrain loss 1.02. Val loss 1.16\n",
      "Epoch 6:\tTrain loss 1.01. Val loss 1.15\n",
      "Epoch 7:\tTrain loss 1.01. Val loss 1.15\n",
      "Epoch 8:\tTrain loss 1.00. Val loss 1.15\n",
      "Epoch 9:\tTrain loss 1.00. Val loss 1.15\n",
      "Epoch 10:\tTrain loss 0.99. Val loss 1.14\n",
      "Epoch 11:\tTrain loss 0.99. Val loss 1.14\n",
      "Epoch 12:\tTrain loss 0.99. Val loss 1.14\n",
      "Epoch 13:\tTrain loss 0.99. Val loss 1.14\n",
      "\t DEPRESSION BIPOLAR diastolic_blood_pressure 1 0 1 . 0 bmi 2 5 . 1 diastolic_blood_pressure 7 6 . 0 diastolic_blood_pressure 7 8 . 0 diastolic_blood_pressure 7 9 . 0 eosinophil_count 0 .\n",
      "\n",
      "Training model `pos_encoding`, with 10.679424 M parameters\n",
      "\n",
      "Epoch 0:\tTrain loss 2.60. Val loss 2.72\n",
      "Epoch 1:\tTrain loss 2.13. Val loss 2.49\n",
      "Epoch 2:\tTrain loss 2.01. Val loss 2.60\n",
      "Epoch 3:\tTrain loss 1.96. Val loss 2.06\n",
      "Epoch 4:\tTrain loss 1.77. Val loss 1.86\n",
      "Epoch 5:\tTrain loss 1.48. Val loss 1.54\n",
      "Epoch 6:\tTrain loss 1.28. Val loss 1.39\n",
      "Epoch 7:\tTrain loss 1.21. Val loss 1.36\n",
      "Epoch 8:\tTrain loss 1.19. Val loss 1.34\n",
      "Epoch 9:\tTrain loss 1.17. Val loss 1.33\n",
      "Epoch 10:\tTrain loss 1.16. Val loss 1.31\n",
      "Epoch 11:\tTrain loss 1.14. Val loss 1.29\n",
      "Epoch 12:\tTrain loss 1.13. Val loss 1.28\n",
      "Epoch 13:\tTrain loss 1.11. Val loss 1.23\n",
      "Epoch 14:\tTrain loss 1.07. Val loss 1.21\n",
      "Epoch 15:\tTrain loss 1.06. Val loss 1.20\n",
      "Epoch 16:\tTrain loss 1.05. Val loss 1.19\n",
      "Epoch 17:\tTrain loss 1.04. Val loss 1.18\n",
      "Epoch 18:\tTrain loss 1.26. Val loss 1.33\n",
      "Epoch 19:\tTrain loss 1.11. Val loss 1.20\n",
      "\t DEPRESSION ANXIETY bmi 2 0 . 9 diastolic_blood_pressure 8 0 . 0 diastolic_blood_pressure 7 0 . 0 eosinophil_count 0 . 2 bmi 2 5 . 7 2 diastolic_blood_pressure 8 0 .\n"
     ]
    }
   ],
   "source": [
    "for m_idx, (model, m_name) in enumerate(zip(models, m_names)):\n",
    "    \n",
    "    print(f\"\\nTraining model `{m_name}`, with {sum(p.numel() for p in model.parameters())/1e6} M parameters\\n\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # create a PyTorch optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=opt.learning_rate)\n",
    "\n",
    "    best_val, epochs_since_best = np.inf, 0\n",
    "    for epoch in range(opt.epochs):\n",
    "        epoch_loss = 0\n",
    "        model.train()\n",
    "        for i, batch in enumerate(dm.train_dataloader()):\n",
    "            # evaluate the loss\n",
    "            _, loss = model(batch['tokens'].to(device),\n",
    "                            attention_mask=batch['attention_mask'].to(device)\n",
    "                            )\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        epoch_loss /= i\n",
    "        loss_curves_train[m_idx].append(epoch_loss)\n",
    "\n",
    "        # evaluate the loss on val set\n",
    "        with torch.no_grad(): \n",
    "            model.eval()\n",
    "            if epoch % opt.eval_interval == 0 or epoch == opt.epochs - 1:\n",
    "                val_loss = 0\n",
    "                for j, batch in enumerate(dm.val_dataloader()):\n",
    "                    _, loss = model(batch['tokens'].to(device), \n",
    "                                    attention_mask=batch['attention_mask'].to(device)   \n",
    "                                   )\n",
    "                    val_loss += loss.item()\n",
    "                val_loss /= j\n",
    "                loss_curves_val[m_idx].append(val_loss)\n",
    "                print(f\"Epoch {epoch}:\\tTrain loss {epoch_loss:.2f}. Val loss {val_loss:.2f}\")          \n",
    "                # TODO: Note not fully accurate as last batch is likely not the same size, will be fixed with lightning\n",
    "        \n",
    "            if val_loss >= best_val:\n",
    "                epochs_since_best += 1\n",
    "                if epochs_since_best >= 2:\n",
    "                    break\n",
    "            else:\n",
    "                best_val = val_loss\n",
    "                epochs_since_best = 0\n",
    "\n",
    "    # Test trained model with a prompt\n",
    "    # ----------------    \n",
    "    # set context: an initial diagnosis of depression\n",
    "    tokens = torch.from_numpy(np.array(dm.encode([\"DEPRESSION\"])).reshape((1,-1))).to(device)\n",
    "    # generate: then sample the next 10 tokens\n",
    "    new_tokens = model.generate(tokens, max_new_tokens=30)[0].tolist()\n",
    "    generated = dm.decode(new_tokens)\n",
    "    print(f\"\\t {generated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"k\", \"r\", \"b\", \"y\"]\n",
    "\n",
    "# Plot loss\n",
    "plt.figure()\n",
    "for m_idx, _ in enumerate(models):\n",
    "    # Training\n",
    "    iterations = np.linspace(0, len(loss_curves_train[m_idx]), len(loss_curves_train[m_idx])) * opt.eval_interval\n",
    "    plt.plot(iterations, loss_curves_train[m_idx], label=f\"{m_names[m_idx]}-train\", c=cols[m_idx], linestyle='dashed')\n",
    "    # Validation\n",
    "    iterations = np.linspace(0, len(loss_curves_val[m_idx]), len(loss_curves_val[m_idx])) * opt.eval_interval\n",
    "    plt.plot(iterations, loss_curves_val[m_idx], label=f\"{m_names[m_idx]}-val\", c=cols[m_idx])\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(f\"figs/transformer/loss.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diabetes\n",
    "\n",
    "Probability of type II diabetes before and after a type I diagnosis\n",
    "\n",
    "keys: \n",
    "\n",
    "    70: 'TYPE1DM'\n",
    "    31: 'TYPE2DIABETES'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_token1 = dm.tokenizer._stoi[\"TYPE1DM\"]\n",
    "target_token2 = dm.tokenizer._stoi[\"TYPE2DIABETES\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small context comparison, high bmi and blood pressure vs low for diabetes risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_risk_prompt = [\"bmi\", \"2\", \"2\", \".\", \"5\", \"diastolic_blood_pressure\", \"7\", \"9\", \".\", \"0\"]\n",
    "high_risk_prompt = [\"bmi\", \"3\", \"7\", \".\", \"5\", \"diastolic_blood_pressure\", \"9\", \"9\", \".\", \"0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "pos_embedding\n",
      "=============\n",
      "\n",
      "Control: Low risk: \t (bmi,2,2,.,5,diastolic_blood_pressure,7,9,.,0): \n",
      "torch.Size([9, 101])\n",
      "probability of type I diabetes 0.0010%\n",
      "probability of type II diabetes 0.0009%\n",
      "\n",
      "Control: High risk: \t (bmi,3,7,.,5,diastolic_blood_pressure,9,9,.,0): \n",
      "torch.Size([9, 101])\n",
      "probability of type I diabetes 0.0010%\n",
      "probability of type II diabetes 0.0009%\n",
      "\n",
      "Control: Low risk + depression: \t (DEPRESSION,bmi,2,2,.,5,diastolic_blood_pressure,7,9,.,0): \n",
      "torch.Size([10, 101])\n",
      "probability of type I diabetes 0.0391%\n",
      "probability of type II diabetes 0.2069%\n",
      "\n",
      "Low risk context: Type 1 diagnosis in prompt: \t (TYPE1DM,bmi,2,2,.,5,diastolic_blood_pressure,7,9,.,0): \n",
      "torch.Size([10, 101])\n",
      "probability of type I diabetes 0.1947%\n",
      "probability of type II diabetes 0.7335%\n",
      "\n",
      "Low risk context: Type 1I diagnosis in prompt: \t (TYPE2DIABETES,bmi,2,2,.,5,diastolic_blood_pressure,7,9,.,0): \n",
      "torch.Size([10, 101])\n",
      "probability of type I diabetes 0.2901%\n",
      "probability of type II diabetes 0.2662%\n",
      "\n",
      "\n",
      "pos_encoding\n",
      "============\n",
      "\n",
      "Control: Low risk: \t (bmi,2,2,.,5,diastolic_blood_pressure,7,9,.,0): \n",
      "torch.Size([9, 101])\n",
      "probability of type I diabetes 0.0100%\n",
      "probability of type II diabetes 0.0063%\n",
      "\n",
      "Control: High risk: \t (bmi,3,7,.,5,diastolic_blood_pressure,9,9,.,0): \n",
      "torch.Size([9, 101])\n",
      "probability of type I diabetes 0.0100%\n",
      "probability of type II diabetes 0.0063%\n",
      "\n",
      "Control: Low risk + depression: \t (DEPRESSION,bmi,2,2,.,5,diastolic_blood_pressure,7,9,.,0): \n",
      "torch.Size([10, 101])\n",
      "probability of type I diabetes 0.0579%\n",
      "probability of type II diabetes 0.1237%\n",
      "\n",
      "Low risk context: Type 1 diagnosis in prompt: \t (TYPE1DM,bmi,2,2,.,5,diastolic_blood_pressure,7,9,.,0): \n",
      "torch.Size([10, 101])\n",
      "probability of type I diabetes 0.1613%\n",
      "probability of type II diabetes 0.7481%\n",
      "\n",
      "Low risk context: Type 1I diagnosis in prompt: \t (TYPE2DIABETES,bmi,2,2,.,5,diastolic_blood_pressure,7,9,.,0): \n",
      "torch.Size([10, 101])\n",
      "probability of type I diabetes 0.1916%\n",
      "probability of type II diabetes 1.0659%\n"
     ]
    }
   ],
   "source": [
    "prompts, desc = [], []\n",
    "\n",
    "desc.append(\"Control: Low risk\")\n",
    "prompts.append(low_risk_prompt)\n",
    "\n",
    "desc.append(\"Control: High risk\")\n",
    "prompts.append(high_risk_prompt)\n",
    "\n",
    "desc.append(\"Control: Low risk + depression\")\n",
    "prompts.append([\"DEPRESSION\"] + low_risk_prompt)\n",
    "\n",
    "desc.append(\"Low risk context: Type 1 diagnosis in prompt\")\n",
    "prompts.append([\"TYPE1DM\"] + low_risk_prompt)\n",
    "\n",
    "desc.append(\"Low risk context: Type 1I diagnosis in prompt\")\n",
    "prompts.append([\"TYPE2DIABETES\"] + low_risk_prompt)\n",
    "\n",
    "for model_idx, model in enumerate(models):\n",
    "    print(f\"\\n\\n{m_names[model_idx]}\\n\" + \"=\"*len(m_names[model_idx]))\n",
    "    \n",
    "    for p_idx, prompt in enumerate(prompts):\n",
    "        print(f\"\\n{desc[p_idx]}: \\t ({','.join(prompt)}): \")\n",
    "        encoded_prompt = torch.from_numpy(np.array(dm.encode(prompt)).reshape((1,-1))).to(device)\n",
    "        lgts, _ = model(encoded_prompt)\n",
    "        print(lgts.shape)\n",
    "        probs = torch.nn.functional.softmax(lgts, dim=1)\n",
    "        print(f\"probability of type I diabetes {100*float(probs[0, target_token1].cpu().detach().numpy()):.4f}%\")\n",
    "        print(f\"probability of type II diabetes {100*float(probs[0, target_token2].cpu().detach().numpy()):.4f}%\")\n",
    "\n",
    "# Note: adding a diagnosis (even if potentially orthogonal) at the beginning of the prompt increases probability of either type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: model architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "pos_embedding\n",
      "=============\n",
      "\n",
      "\n",
      "TransformerForCausalLM(\n",
      "  (transformer): Transformer(\n",
      "    (wpe): PositionalEmbedding(\n",
      "      (wpe): Embedding(256, 384)\n",
      "    )\n",
      "    (wte): Embedding(101, 384, padding_idx=0)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (blocks): ModuleList(\n",
      "      (0): Block(\n",
      "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (acti): ReLU()\n",
      "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): Block(\n",
      "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (acti): ReLU()\n",
      "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): Block(\n",
      "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (acti): ReLU()\n",
      "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): Block(\n",
      "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (acti): ReLU()\n",
      "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): Block(\n",
      "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (acti): ReLU()\n",
      "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): Block(\n",
      "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (acti): ReLU()\n",
      "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=384, out_features=101, bias=False)\n",
      ")\n",
      "\n",
      "\n",
      "pos_encoding\n",
      "============\n",
      "\n",
      "\n",
      "TransformerForCausalLM(\n",
      "  (transformer): Transformer(\n",
      "    (wpe): PositionalEncoding()\n",
      "    (wte): Embedding(101, 384, padding_idx=0)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (blocks): ModuleList(\n",
      "      (0): Block(\n",
      "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (acti): ReLU()\n",
      "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): Block(\n",
      "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (acti): ReLU()\n",
      "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): Block(\n",
      "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (acti): ReLU()\n",
      "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): Block(\n",
      "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (acti): ReLU()\n",
      "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): Block(\n",
      "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (acti): ReLU()\n",
      "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): Block(\n",
      "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): MultiHeadedSelfAttention(\n",
      "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "          (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (acti): ReLU()\n",
      "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=384, out_features=101, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for model_idx, model in enumerate(models):\n",
    "    print(f\"\\n\\n{m_names[model_idx]}\\n\" + \"=\"*len(m_names[model_idx]))\n",
    "    print(f\"\\n\\n{model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Transformer.ipynb to html\n",
      "[NbConvertApp] Writing 588859 bytes to Transformer.html\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to html --no-input Transformer.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "sys_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
